title,transcript
Are insect brains the secret to great AI?,"Creating intelligence on a computer. This has been the Holy Grail for artificial intelligence for quite some time. But how do we get there? So we view ourselves as highly intelligent beings. So it's logical to study our own brains, the substrate of our cognition, for creating artificial intelligence. Imagine if we could replicate how our own brains work on a computer. But now consider the journey that would be required. The human brain contains 86 billion neurons. Each is constantly communicating with thousands of others, and each has individual characteristics of its own. Capturing the human brain on a computer may simply be too big and too complex a problem to tackle with the technology and the knowledge that we have today. I believe that we can capture a brain on a computer, but we have to start smaller. Much smaller. These insects have three of the most fascinating brains in the world to me. While they do not possess human-level intelligence, each is remarkable at a particular task. Think of them as highly trained specialists. African dung beetles are really good at rolling large balls in straight lines. (Laughter) Now, if you've ever made a snowman, you know that rolling a large ball is not easy. Now picture trying to make that snowman when the ball of snow is as big as you are and you're standing on your head. (Laughter) Sahara desert ants are navigation specialists. They might have to wander a considerable distance to forage for food. But once they do find sustenance, they know how to calculate the straightest path home. And the dragonfly is a hunting specialist. In the wild, dragonflies capture approximately 95 percent of the prey they choose to go after. These insects are so good at their specialties that neuroscientists such as myself study them as model systems to understand how animal nervous systems solve particular problems. And in my own research, I study brains to bring these solutions, the best that biology has to offer, to computers. So consider the dragonfly brain. It has only on the order of one million neurons. Now, it's still not easy to unravel a circuit of even one million neurons. But given the choice between trying to tease apart the one-million-neuron brain versus the 86-billion-neuron brain, which would you choose to try first? (Laughter) When studying these smaller insect brains, the immediate goal is not human intelligence. We study these brains for what the insects do well. And in the case of the dragonfly, that's interception. So when dragonflies are hunting, they do more than just fly straight at the prey. They fly in such a way that they will intercept it. They aim for where the prey is going to be. Much like a soccer player, running to intercept a pass. To do this correctly, dragonflies need to perform what is known as a coordinate transformation, going from the eye’s frame of reference, or what the dragonfly sees, to the body's frame of reference, or how the dragonfly needs to turn its body to intercept. Coordinate transformations are a basic calculation that animals need to perform to interact with the world. We do them instinctively every time we reach for something. When I reach for an object straight in front of me, my arm takes a very different trajectory than if I turn my head, look at that same object when it is off to one side and reach for it there. In both cases, my eyes see the same image of that object, but my brain is sending my arm on a very different trajectory based on the position of my neck. And dragonflies are fast. This means they calculate fast. The latency, or the time it takes for a dragonfly to respond once it sees the prey turn, is about 50 milliseconds. This latency is remarkable. For one thing, it's only half the time of a human eye blink. But for another thing, it suggests that dragonflies capture how to intercept in only relatively or surprisingly few computational steps. So in the brain, a computational step is a single neuron or a layer of neurons working in parallel. It takes a single neuron about 10 milliseconds to add up all its inputs and respond. The 50-millisecond response time means that once the dragonfly sees its prey turn, there's only time for maybe four of these computational steps or four layers of neurons, working in sequence, one after the other, to calculate how the dragonfly needs to turn. In other words, if I want to study how the dragonfly does coordinate transformations, the neural circuit that I need to understand, the neural circuit that I need to study, can have at most four layers of neurons. Each layer may have many neurons, but this is a small neural circuit. Small enough that we can identify it and study it with the tools that are available today. And this is what I'm trying to do. I have built a model of what I believe is the neural circuit that calculates how the dragonfly should turn. And here is the cool result. In the model, dragonflies do coordinate transformations in only one computational step, one layer of neurons. This is something we can test and understand. In a computer simulation, I can predict the activities of individual neurons while the dragonfly is hunting. For example, here I am predicting the action potentials, or the spikes, that are fired by one of these neurons when the dragonfly sees the prey move. To test the model, my collaborators and I are now comparing these predicted neural responses with responses of neurons recorded in living dragonfly brains. These are ongoing experiments in which we put living dragonflies in virtual reality. (Laughter) Now, it's not practical to put VR goggles on a dragonfly. So instead, we show movies of moving targets to the dragonfly, while an electrode records activity patterns of individual neurons in the brain. Yeah, he likes the movies. If the responses that we record in the brain match those predicted by the model, we will have identified which neurons are responsible for coordinate transformations. The next step will be to understand the specifics of how these neurons work together to do the calculation. But this is how we begin to understand how brains do basic or primitive calculations. Calculations that I regard as building blocks for more complex functions, not only for interception but also for cognition. The way that these neurons compute may be different from anything that exists on a computer today. And the goal of this work is to do more than just write code that replicates the activity patterns of neurons. We aim to build a computer chip that not only does the same things as biological brains but does them in the same way as biological brains. This could lead to drones driven by computers the same size of the dragonfly's brain that captures some targets and avoid others. Personally, I'm hoping for a small army of these to defend my backyard from mosquitoes in the summer. (Laughter) The GPS on your phone could be replaced by a new navigation device based on dung beetles or ants that could guide you to the straight or the easy path home. And what would the power requirements of these devices be like? As small as it is -- Or, sorry -- as large as it is, the human brain is estimated to have the same power requirements as a 20-watt light bulb. Imagine if all brain-inspired computers had the same extremely low-power requirements. Your smartphone or your smartwatch probably needs charging every day. Your new brain-inspired device might only need charging every few months, or maybe even every few years. The famous physicist, Richard Feynman, once said, ""What I cannot create, I do not understand."" What I see in insect nervous systems is an opportunity to understand brains through the creation of computers that work as brains do. And creation of these computers will not just be for knowledge. There's potential for real impact on your devices, your vehicles, maybe even artificial intelligences. So next time you see an insect, consider that these tiny brains can lead to remarkable computers. And think of the potential that they offer us for the future. Thank you. (Applause)"
How will AI change the world?,"In the coming years,  artificial intelligence is probably going to change your life,  and likely the entire world. But people have a hard time agreeing on exactly how. The following are excerpts  from a World Economic Forum interview where renowned computer science professor  and AI expert Stuart Russell helps separate the sense  from the nonsense. There’s a big difference between asking a human to do something and giving that as the objective to an AI system. When you ask a human to get you a cup of coffee, you don’t mean this should be  their life’s mission, and nothing else in the universe matters. Even if they have to kill everybody else in Starbucks to get you the coffee before it closes— they should do that. No, that’s not what you mean. All the other things that we mutually care about, they should factor into your behavior as well. And the problem with the way  we build AI systems now is we give them a fixed objective. The algorithms require us to specify everything in the objective. And if you say, can we fix the acidification of the oceans? Yeah, you could have a catalytic reaction that does that extremely efficiently, but it consumes a quarter  of the oxygen in the atmosphere, which would apparently cause us to die fairly slowly and unpleasantly over the course of several hours. So, how do we avoid this problem? You might say, okay, well, just be more careful about specifying the objective— don’t forget the atmospheric oxygen. And then, of course, some side effect of the reaction in the ocean poisons all the fish. Okay, well I meant don’t kill the fish either. And then, well, what about the seaweed? Don’t do anything that’s going to cause all the seaweed to die. And on and on and on. And the reason that we don’t have to do that with humans is that humans often know that they don’t know  all the things that we care about. If you ask a human to get you a cup of coffee, and you happen to be  in the Hotel George Sand in Paris, where the coffee is 13 euros a cup, it’s entirely reasonable to come back and say, well, it’s 13 euros, are you sure you want it,  or I could go next door and get one? And it’s a perfectly normal thing for a person to do. To ask, I’m going to repaint your house— is it okay if I take off the drainpipes and then put them back? We don't think of this as a terribly sophisticated capability, but AI systems don’t have it  because the way we build them now, they have to know the full objective. If we build systems that know that  they don’t know what the objective is, then they start to exhibit these behaviors, like asking permission before getting rid of all the oxygen in the atmosphere. In all these senses,  control over the AI system comes from the machine’s uncertainty  about what the true objective is. And it’s when you build machines that believe with certainty that they have the objective, that’s when you get this sort of psychopathic behavior. And I think we see  the same thing in humans. What happens when general purpose AI hits the real economy? How do things change? Can we adapt? This is a very old point. Amazingly, Aristotle actually has a passage where he says, look, if we had fully automated  weaving machines and plectrums that could pluck the lyre  and produce music without any humans, then we wouldn’t need any workers. That idea, which I think it was Keynes who called it technological unemployment  in 1930, is very obvious to people. They think, yeah, of course,  if the machine does the work, then I'm going to be unemployed. You can think about the warehouses  that companies are currently operating for e-commerce,  they are half automated. The way it works is that an old warehouse— where you’ve got tons of stuff piled up all over the place  and humans go and rummage around and then bring it back and send it off— there’s a robot who goes and gets the shelving unit that contains the thing that you need, but the human has to pick the object  out of the bin or off the shelf, because that’s still too difficult. But, at the same time, would you make a robot that is accurate enough to be able to pick pretty much any object within a very wide  variety of objects that you can buy? That would, at a stroke,  eliminate 3 or 4 million jobs? There's an interesting story that E.M. Forster wrote, where everyone is entirely machine dependent. The story is really about the fact that if you hand over the management of your civilization to machines, you then lose the incentive to understand it yourself or to teach the next generation  how to understand it. You can see “WALL-E” actually as a modern version, where everyone is enfeebled and infantilized by the machine, and that hasn’t been possible up to now. We put a lot of our civilization  into books, but the books can’t run it for us. And so we always have to teach the next generation. If you work it out, it’s about a trillion person years of teaching and learning and an unbroken chain that goes back  tens of thousands of generations. What happens if that chain breaks? I think that’s something we have  to understand as AI moves forward. The actual date of arrival of general purpose AI— you’re not going to be able to pinpoint, it isn’t a single day. It’s also not the case  that it’s all or nothing. The impact is going to be increasing. So with every advance in AI, it significantly expands the range of tasks. So in that sense, I think most experts say by the end of the century, we’re very, very likely to have  general purpose AI. The median is something around 2045. I'm a little more on the conservative side. I think the problem is harder than we think. I like what John McAfee,  he was one of the founders of AI, when he was asked this question, he said,  somewhere between five and 500 years. And we're going to need, I think, several Einsteins to make it happen."
AI-generated creatures that stretch the boundaries of imagination,"I'd like to start by asking you to imagine a color that you've never seen before. Just for a second give this a try. Can you actually visualize a color that you've never been able to perceive? I never seem to get tired of trying this although I know it's not an easy challenge. And the thing is, we can't imagine something without drawing upon our experiences. A color we haven't yet seen outside the spectrum we can perceive is outside our ability to conjure up. It's almost like there's a boundary to our imagination where all the colors we can imagine can only be various shades of other colors we have previously seen. Yet we know for a fact that those color frequencies outside our visible spectrum are there. And scientists believe that there are species that have many more photo receptors than just the three color ones we humans have. Which, by the way, not all humans see the world in the same way. Some of us are colorblind to various degrees, and very often we don't even agree on small things, like if a dress on the internet is blue and black or white and gold. But my favorite creature, one of my favorite creatures, is the peacock mantis shrimp, which is estimated to have 12 to 16 photo receptors. And that indicates the world to them might look so much more colorful. So what about artificial intelligence? Can AI help us see beyond our human capabilities? Well, I've been working with AI for the past five years, and in my experience, it can see within the data it gets fed. But then you might be wondering, OK, if AI can't help imagine anything new, why would an artist see any point in using it? And my answer to that is because I think that it can help augment our creativity as there's value in creating combinations of known elements to form new ones. And this boundary of what we can imagine based on what we have experienced is the place that I have been exploring. For me, it started with jellyfish on a screen at an aquarium and wearing those old 3D glasses, which I hope you remember, the ones with the blue and red lens. And this experience made me want to recreate their textures. But not just that, I also wanted to create new jellyfish that I hadn't seen before, like these. And what started with jellyfish, very quickly escalated to other sea creatures like sea anemone, coral and fish. And then from there came amphibians, birds and insects. And this became a series called “Neural Zoo”. But when you look closely, what do you see? There's no single creature in these images. And AI augments my creative process by allowing me to distill and recombine textures. And that's something that would otherwise take me months to draw by hand. Plus I'm actually terrible at drawing. So you could say, in a way, what I'm doing is a contemporary version of something that humans have already been doing for a long time, even before cameras existed. In medieval times, people went on expeditions, and when they came back they would share about what they saw to an illustrator. And the illustrator, having never seen what was being described, would end up drawing based on the creatures that they had previously seen and in the process creating hybrid animals of some sort. So an explorer might describe a beaver, but having never seen one, the illustrator might give it the head of a rodent, the body of a dog and a fish-like tail. In the series “Artificial Natural History”, I took thousands of illustrations from a natural history archives, and I fed them to a neural network to generate new versions of them. But up until now, all my work was done in 2D. And with the help of my studio partner, Feileacan McCormick, we decided to train a neural network on a data set of 3D scanned beetles. But I must warn you that our first results were extremely blurry, and they looked like the blobs you see here. And this could be due to many reasons, but one of them being that there aren't really a lot of openly available data sets of 3D insects. And also we were repurposing a neural network that normally gets used to generate images to generate 3D. So believe it or not, these are very exciting blobs to us. But with time and some very hacky solutions like data augmentation, where we threw in ants and other beetle-like insects to enhance the data set, we ended up getting this, which we've been told they look like grilled chicken. (Laughter) But hungry for more, we pushed our technique, and eventually they ended up looking like this. We use something called 3D style transfer to map textures onto them, and we also trained a natural language model to generate scientific-like names and anatomical descriptions. And eventually we even found a network architecture that could handle 3D meshes. So they ended up looking like this. And for us, this became a way of creating kind of a speculative study -- (Applause) A speculative study of creatures that never existed, kind of like a speculative biology. But I didn't want to talk about AI and its potential unless it brought me closer to a real species. Which of these do you think is easier to find data about online? (Laughter) Yeah, well, as you guessed correctly, the red panda. And this maybe could be due to many reasons, but one of them being how cute they are, which means we photograph and talk about them a lot, unlike the boreal felt lichen. But both of them are classified as endangered. So I wanted to bring visibility to other endangered species that don't get the same amount of digital representation as a cute, fluffy red panda. And to do this, we trained an AI on millions of images of the natural world, and then we prompted with text to generate some of these creatures. So when prompted with a text, ""an image of a critically endangered spider, the peacock tarantula"" and its scientific name, our model generated this. And here's an image of the real peacock tarantula, which is a wonderful spider endemic to India. But when prompted with a text ""an image of a critically endangered bird, the mangrove finch,"" our model generated this. And here's a photo of the real mangrove finch. Both these creatures exist in the wild, but the accuracy of each generated image is fully dependent on the data available. These chimeras of our everyday data to me are a different way of how the future could be. Not in a literal sense, perhaps, but in the sense that through practicing the expanding of our own imagination about the ecosystems we are a part of, we might just be better equipped to recognize new opportunities and potential. Knowing that there's a boundary to our imagination doesn't have to feel limiting. On the contrary, it can help motivate us to expand that boundary further and to seek out colors and things we haven't yet seen and perhaps enrich our imagination as a result. So thank you. (Applause)"
How AI could empower any business,"When I think about the rise of AI, I'm reminded by the rise of literacy. A few hundred years ago, many people in society thought that maybe not everyone needed to be able to read and write. Back then, many people were tending fields or herding sheep, so maybe there was less need for written communication. And all that was needed was for the high priests and priestesses and monks to be able to read the Holy Book, and the rest of us could just go to the temple or church or the holy building and sit and listen to the high priest and priestesses read to us. Fortunately, it was since figured out that we can build a much richer society if lots of people can read and write. Today, AI is in the hands of the high priests and priestesses. These are the highly skilled AI engineers, many of whom work in the big tech companies. And most people have access only to the AI that they build for them. I think that we can build a much richer society if we can enable everyone to help to write the future. But why is AI largely concentrated in the big tech companies? Because many of these AI projects have been expensive to build. They may require dozens of highly skilled engineers, and they may cost millions or tens of millions of dollars to build an AI system. And the large tech companies, particularly the ones with hundreds of millions or even billions of users, have been better than anyone else at making these investments pay off because, for them, a one-size-fits-all AI system, such as one that improves web search or that recommends better products for online shopping, can be applied to [these] very large numbers of users to generate a massive amount of revenue. But this recipe for AI does not work once you go outside the tech and internet sectors to other places where, for the most part, there are hardly any projects that apply to 100 million people or that generate comparable economics. Let me illustrate an example. Many weekends, I drive a few minutes from my house to a local pizza store to buy a slice of Hawaiian pizza from the gentleman that owns this pizza store. And his pizza is great, but he always has a lot of cold pizzas sitting around, and every weekend some different flavor of pizza is out of stock. But when I watch him operate his store, I get excited, because by selling pizza, he is generating data. And this is data that he can take advantage of if he had access to AI. AI systems are good at spotting patterns when given access to the right data, and perhaps an AI system could spot if Mediterranean pizzas sell really well on a Friday night, maybe it could suggest to him to make more of it on a Friday afternoon. Now you might say to me, ""Hey, Andrew, this is a small pizza store. What's the big deal?"" And I say, to the gentleman that owns this pizza store, something that could help him improve his revenues by a few thousand dollars a year, that will be a huge deal to him. I know that there is a lot of hype about AI's need for massive data sets, and having more data does help. But contrary to the hype, AI can often work just fine even on modest amounts of data, such as the data generated by a single pizza store. So the real problem is not that there isn’t enough data from the pizza store. The real problem is that the small pizza store could never serve enough customers to justify the cost of hiring an AI team. I know that in the United States there are about half a million independent restaurants. And collectively, these restaurants do serve tens of millions of customers. But every restaurant is different with a different menu, different customers, different ways of recording sales that no one-size-fits-all AI would work for all of them. What would it be like if we could enable small businesses and especially local businesses to use AI? Let's take a look at what it might look like at a company that makes and sells T-shirts. I would love if an accountant working for the T-shirt company can use AI for demand forecasting. Say, figure out what funny memes to prints on T-shirts that would drive sales, by looking at what's trending on social media. Or for product placement, why can’t a front-of-store manager take pictures of what the store looks like and show it to an AI and have an AI recommend where to place products to improve sales? Supply chain. Can an AI recommend to a buyer whether or not they should pay 20 dollars per yard for a piece of fabric now, or if they should keep looking because they might be able to find it cheaper elsewhere? Or quality control. A quality inspector should be able to use AI to automatically scan pictures of the fabric they use to make T-shirts to check if there are any tears or discolorations in the cloth. Today, large tech companies routinely use AI to solve problems like these and to great effect. But a typical T-shirt company or a typical auto mechanic or retailer or school or local farm will be using AI for exactly zero of these applications today. Every T-shirt maker is sufficiently different from every other T-shirt maker that there is no one-size-fits-all AI that will work for all of them. And in fact, once you go outside the internet and tech sectors in other industries, even large companies such as the pharmaceutical companies, the car makers, the hospitals, also struggle with this. This is the long-tail problem of AI. If you were to take all current and potential AI projects and sort them in decreasing order of value and plot them, you get a graph that looks like this. Maybe the single most valuable AI system is something that decides what ads to show people on the internet. Maybe the second most valuable is a web search engine, maybe the third most valuable is an online shopping product recommendation system. But when you go to the right of this curve, you then get projects like T-shirt product placement or T-shirt demand forecasting or pizzeria demand forecasting. And each of these is a unique project that needs to be custom-built. Even T-shirt demand forecasting, if it depends on trending memes on social media, is a very different project than pizzeria demand forecasting, if that depends on the pizzeria sales data. So today there are millions of projects sitting on the tail of this distribution that no one is working on, but whose aggregate value is massive. So how can we enable small businesses and individuals to build AI systems that matter to them? For most of the last few decades, if you wanted to build an AI system, this is what you have to do. You have to write pages and pages of code. And while I would love for everyone to learn to code, and in fact, online education and also offline education are helping more people than ever learn to code, unfortunately, not everyone has the time to do this. But there is an emerging new way to build AI systems that will let more people participate. Just as pen and paper, which are a vastly superior technology to stone tablet and chisel, were instrumental to widespread literacy, there are emerging new AI development platforms that shift the focus from asking you to write lots of code to asking you to focus on providing data. And this turns out to be much easier for a lot of people to do. Today, there are multiple companies working on platforms like these. Let me illustrate a few of the concepts using one that my team has been building. Take the example of an inspector wanting AI to help detect defects in fabric. An inspector can take pictures of the fabric and upload it to a platform like this, and they can go in to show the AI what tears in the fabric look like by drawing rectangles. And they can also go in to show the AI what discoloration on the fabric looks like by drawing rectangles. So these pictures, together with the green and pink rectangles that the inspector's drawn, are data created by the inspector to explain to AI how to find tears and discoloration. After the AI examines this data, we may find that it has seen enough pictures of tears, but not yet enough pictures of discolorations. This is akin to if a junior inspector had learned to reliably spot tears, but still needs to further hone their judgment about discolorations. So the inspector can go back and take more pictures of discolorations to show to the AI, to help it deepen this understanding. By adjusting the data you give to the AI, you can help the AI get smarter. So an inspector using an accessible platform like this can, in a few hours to a few days, and with purchasing a suitable camera set up, be able to build a custom AI system to detect defects, tears and discolorations in all the fabric being used to make T-shirts throughout the factory. And once again, you may say, ""Hey, Andrew, this is one factory. Why is this a big deal?"" And I say to you, this is a big deal to that inspector whose life this makes easier and equally, this type of technology can empower a baker to use AI to check for the quality of the cakes they're making, or an organic farmer to check the quality of the vegetables, or a furniture maker to check the quality of the wood they're using. Platforms like these will probably still need a few more years before they're easy enough to use for every pizzeria owner. But many of these platforms are coming along, and some of them are getting to be quite useful to someone that is tech savvy today, with just a bit of training. But what this means is that, rather than relying on the high priests and priestesses to write AI systems for everyone else, we can start to empower every accountant, every store manager, every buyer and every quality inspector to build their own AI systems. I hope that the pizzeria owner and many other small business owners like him will also take advantage of this technology because AI is creating tremendous wealth and will continue to create tremendous wealth. And it's only by democratizing access to AI that we can ensure that this wealth is spread far and wide across society. Hundreds of years ago. I think hardly anyone understood the impact that widespread literacy will have. Today, I think hardly anyone understands the impact that democratizing access to AI will have. Building AI systems has been out of reach for most people, but that does not have to be the case. In the coming era for AI, we’ll empower everyone to build AI systems for themselves, and I think that will be incredibly exciting future. Thank you very much. (Applause)"
What if you could sing in your favorite musician's voice?,"(Overlapping voices sing) (Singing ends) (Applause) Hi, my name is Holly, and I’m an artist. That was my voice, but I didn’t sing that clip. I trained an AI on my own voice, and now she can sing anything in multiple languages. Her name is Holly+, and you just heard her perform “El Cant de la Sibilla,” which is a traditional song arranged by Maria Arnal in Catalan. Not a language that I speak and not a vocal tradition that I've trained in. Those melismatic runs are really difficult to hit. To show you Holly+'s full range, I'll also play an example in German. This is ""Mack the Knife"" by Brecht. (Singing in German) (Music ends) Of course Holly+ can -- thank you. (Applause) Of course, Holly+ can also perform my own music, so here's an excerpt from my own song, ""Frontier."" (Singing ""Frontier"" in overlapping voices) (Singing ends) Holly+ uses a process called timbre transfer, where the timbre, or sound quality of one sound, may be mapped onto the performance of another. Timbre transfer is done by creating a machine-learning model of a sound. In this case, it's my voice. So I recorded a wide variety of phrases in my entire vocal range. To be clear, this version of Holly+ is reading notes from a score. So I'll also play you an example of her singing in Spanish, “Bésame Mucho” by Velázquez. (Music: ""Bésame Mucho"") (Music ends) Thank you. (Applause) I can't speak Spanish, by the way, so. Teaching an AI the sonic properties of one sound in order to generate an entirely new sound is what I like to call ""spawning."" Spawning is what allows Holly+ to create a wide range of vocals that I didn't sing from a set of recorded phrases that I did sing. I like to think of spawning as a kind of 21st-century corollary to the musical tradition of sampling, which had a really big impact on both music and intellectual property. But I think spawning is far more exciting and potentially really weird. (Laughter) So, for example, with sampling, usually you copy and remix a recording by someone else to create something new. But with spawning, you can perform as someone else based on trained information about them. And as an artist, this is making me rethink my own past work as not only my archive but potentially also I myself could become reanimated with AI. This also opens up the question of how we deal with a collective human archive if we can reanimate old media. It opens up really big ethical and intellectual property questions that require entirely new conceptual and legal frameworks. One way that I like to think about intellectual property I call identity play. So rather than limiting the use of my voice, I'm creating instruments to allow as many people as possible to create music with me, and even as me. That's why I've made versions of Holly+ freely available for anyone to use online. If I can allow people to play with my IP, my digital identity, my intellectual property, what might they come up with? Could someone else go on tour as me, with my permission? Could I be in a thousand different bands in multiple languages? And what would that even sound like? To be clear, musicians have been taught to be really protective over our IP, and that's for really good reason. But I'm trying to think of ways to make this new capability mutually beneficial. So to allow people to use my voice but still to maintain the ability to approve certain derivative works. So I invite you to consider, if given the opportunity, who would you like to perform through? And can you imagine someone else performing you? With that in mind, I'd like to invite the incredible musician Pher to the stage. (Applause) So today, Pher will be performing his own song, ""Murky."" And with one microphone you'll hear Pher's beautiful, natural voice. (Pher vocalizing) (Audience cheers) And with this microphone, you'll hear a live version of Holly+ developed with Voctro Labs. (Pher vocalizing in Holly+ voice) (Laughter and applause) Take it away Pher. (Pher singing ""Murky"") It gets so murky Loving what you know But I, I keep on swimming, I'm swimming Further up the road See, the problem is, I don't know what to say When you come around Acting this way Oh oh And, yes, the truth is I show you every day Cause you love to stay (Applause) Living in all the pain And it gets so murky Loving what you know But I'll keep on swimming Further up the road And I've been calling out Calling out your name Deep down in my sleep And I even memorized your face So ain't no leaving' me And I just handle my biz And keep the train moving on And you know what it is Is just you that I'm choosing Cause it gets so murky Loving what you know And I'll I'll keep on swimming Further up the road And it gets so murky Loving what you know And I’ll, I'll keep on swimming Further up the road Swimming I'm swimming Further up the road And I'll keep on swimming Swimming Further up the road (Vocalizing) (Cheers and applause) Holly Herndon: Thank you. Thank you."
