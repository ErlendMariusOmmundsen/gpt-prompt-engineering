title,topics,speaker,description,transcript,duration,upload_date
Your right to mental privacy in the age of brain-sensing tech,"science,technology,health,future,brain,AI,data,ethics",Nita Farahany,"Neurotechnology, or devices that let you track your own brain activity, could help you deeply understand your health. But without privacy protections, your innermost thoughts, emotions and desires could be at risk of exploitation, says neurotech and AI ethicist Nita Farahany. She details some of the field's promising potential uses -- like tracking and treating diseases from depression to epilepsy -- and shares concerns about who collects our brain data and how they plan to use it, ultimately calling for the legal recognition of ""cognitive liberty"" as we connect our brains and minds to technology.","Today, we know and track virtually nothing that’s happening in our own brains. But in a future that is coming much faster than you realize, all of that is about to change. We're now familiar with sensors in our smart watches to our rings, that track everything from our heartbeats to our footsteps, breaths, body temperature, even our sleep. Now, consumer neurotech devices are being sold worldwide to enable us to track our own brain activity. As companies from Meta to Microsoft, Snap and even Apple begin to embed brain sensors in our everyday devices like our earbuds, headphones, headbands, watches and even wearable tattoos, we're reaching an inflection point in brain transparency. And those are just some of the company names we're familiar with. There are so many more. Consumer neurotech devices are moving from niche products with limited applications to becoming the way in which we'll learn about our own brain activity, our controller for virtual reality and augmented reality. And one of the primary ways we'll interact with all of the rest of our technology. Even conservative estimates of the neurotech industry put it at more than  38 billion dollars by 2032. This new category of technology presents unprecedented possibility, both good and bad. Consider how our physical health and well-being are increasing while neurological disease and suffering continue to rise. 55 million people around the world are struggling with dementia, with more than 60 to 70 percent of them suffering from Alzheimer's disease. Nearly a billion people struggle with mental health and drug use disorders. Depression affects more than 300 million. Consumer neurotech devices could finally enable us to treat our brain health and wellness as seriously as we treat the rest of our physical well-being. But making our brains transparent to others also introduces extraordinary risks. Which is why, before it's too late to do so, we must change the basic terms of service for neurotechnology in favour of individual rights. I say this not just as a law professor who believes in the power of law, nor just a philosopher trying to flesh out norms, but as a mother who's been personally and profoundly impacted by the use of neurotechnology in my own life. On Mother's Day in 2017, as my daughter Calista lay cradled in my arms, she took one last beautiful breath. After a prolonged hospitalization, complications following infections claimed her life. The harrowing trauma that she endured and we witnessed stretched into weeks. And I was left with lasting trauma that progressed into post-traumatic stress disorder. Sleep escaped me for years. As each time I closed my eyes, I relived everything, from the first moments that I was pushed out of the emergency room to her gut-wrenching cries. Ultimately, it was the help of a talented psychologist, using exposure therapy, and my use of neurofeedback that enabled me to sleep through the night. For others who are suffering from traumatic memories, an innovative new approach using decoded neurofeedback, or DecNef, may offer reprieve. This groundbreaking approach uses machine-learning algorithms to identify specific brain-activity patterns, including those associated with traumatic memories. Participants then play a game that enables them to retrain their brain activity on positive associations instead. If I had had DecNef available to me at the time, I might have overcome my PTSD more quickly without having to relive every sound, terror and smell in order to do so. I'm not the only one. Sarah described herself as being at the end of her life, no longer in a life worth living, because of her severe and intractable depression. Then, using implanted brain sensors that reset her brain activity like a pacemaker for the brain, Sarah reclaimed her will to live. While implanted neurotechnology advances have been extraordinary, it's the everyday brain sensors that are embedded in our ordinary technology that I believe will impact the majority of our lives. Like the one third of adults and nearly one quarter of children who are living with epilepsy for whom conventional anti-seizure medications fail. Now, researchers from Israel to Spain have developed brain sensors using the power of AI in pattern recognition and consumer electroencephalography to enable the detection of epileptic seizures minutes to up to an hour before they occur, sending potentially life-saving alerts to a mobile device. Regular use of brain sensors could even enable us to detect the earliest stages of the most aggressive forms of brain tumors, like glioblastoma, where early detection is crucial to saving lives. The same could hold true for Parkinson's disease, to Alzheimer's, traumatic brain injury, ADHD, and even depression. We may even change our brains for the better. The brain training game industry, worth a staggering 6.5 billion dollars in 2021, was for years met with controversy because of unsupported scientific claims about their efficacy. But now some brain-training platforms like Cognizant have proven powerful in improving brain processing speeds, memory, reasoning and even executive functioning when played repeatedly over time. When paired with neurofeedback devices for learning reinforcement, this could revolutionize how we learn and adapt to change. Other breakthroughs could be transformational for the human experience. Today, most human brain studies are based on a very small number of participants engaged in very specific tasks in a controlled laboratory environment. With widespread use of brain sensors, the data we could have to learn about the human brain would exponentially increase. With sufficiently large datasets of long-term, real-world data from people engaged in everyday activity, we just might address everything from neurological disease and suffering to creating transformational possibilities for the human experience. But all of this will only be possible if people can confidently share their brain data without fear that it will be misused against them. You see, the brain data that will be collected and generated by these devices won't be collected in traditional laboratory environments or in clinical research studies run by physicians and scientists. Instead, it will be the sellers of these new devices, the very companies who've been commodifying our personal data for years. Which is why we can't go into this new era naive about the risks or complacent about the challenges that the collection and sharing our brain data will pose. Scientific hurdles can and will be addressed in time, but the social hurdles will be the most challenging. Unlike the technologies of the past that track and hack the human brain, brain sensors provide direct access to the part of ourselves that we hold back, that we don't express through our words and our actions. Brain data in many instances will be more sensitive than the personal data of the past, because it reflects our feelings, our mental states, our emotions, our preferences, our desires, even our very thoughts. I would never have wanted the data that was collected as I worked through the trauma of my personal loss to have been commodified, shared and analyzed by others. These aren't just hypothetical risks. Take Entertek, a Hangzhou-based company, who has collected millions of instances of brain activity data as people have engaged in mind-controlled car racing, sleeping, working, even using neurofeedback with their devices. They've already entered into partnerships with other companies to share and analyze that data. Unless people have individual control over their brain data, it will be used for microtargeting or worse, instead of treating dementia. Like the employees worldwide who've already been subject to brain surveillance in the workplace to track their attention and fatigue, to governments, developing brain biometrics, to authenticate people at borders, to interrogate criminal suspects' brains and even weapons that are being crafted to disable and disorient the human brain. Brain wearables will have not only read but write capabilities, creating risks that our brains can be hacked, manipulated, and even subject to targeted attacks. We must act quickly to safeguard against the very real and terrifying risks to our innermost selves. Recognizing a human right to cognitive liberty would offer those safeguards. Cognitive liberty is a right from interference by others, but it is also a right to self-determination over our brains and mental experiences to enable human flourishing. To achieve this, we need to recognize three interrelated human rights and update our understanding of them to secure to us a right to mental privacy, to safeguard us from interference with our automatic reactions, our emotions and our thoughts. Freedom of thought as an absolute human right to protect us from interception, manipulation and punishment of our thoughts. And self-determination to secure self-ownership over our brains and mental experiences, to access and change them if we want to do so. There are important efforts already underway from the UN to UNESCO, in nations worldwide, over rights and regulations around neurotechnologies. But those rights need to be better aligned with a broader set of digital rights. Cognitive liberty is an update to liberty in the digital age as an umbrella concept of human flourishing across digital technologies. Because the right way forward isn't through metaverse rights or AI rights or neurotech rights and the like. It's to recognize that these technologies don't exist in silos, but in combination, affecting our brains and mental experiences. We are literally at a moment before. And I mean a moment. Consumer brain wearables have already arrived, and the commodification of our brains has already begun. It's now just a question of scale. We haven't yet passed the inflection point where most of our brains can be directly accessed and changed by others. But it is about to happen, giving us a final moment to make a change so that we don't look back in a few years' time and lament the world we've left behind. We can and should be hopeful and deliberate about the choices we make now to secure a right to self-determination over our brains and mental experiences. The possibilities, if we do so, are limited only by our imagination. Thank you. (Applause)",PT12M36S,2023-05-30T14:51:02Z
The incredible creativity of deepfakes -- and the worrying future of AI,"technology,computers,entertainment,media,software,future,AI,Internet,data,machine learning",Tom Graham,"AI-generated media that looks and sounds exactly like the real world will soon permeate our lives. How should we prepare for it? AI developer Tom Graham discusses the extraordinary power of this rapidly advancing technology, demoing cutting-edge examples -- including real-time face swaps and voice cloning -- live from the TED stage. In conversation with head of TED Chris Anderson, Graham digs into the creative potential of this hyperreal content (often referred to as ""deepfakes"") as well as its risk for exploitation and the new legal rights we'll need in order to maintain control over our photorealistic AI avatars.","Chris Anderson: So Tom, your company became prominent on the internet with the release of a fake Tom Cruise video, DeepTomCruise, that I think attracted like, a billion views on TikTok and Instagram. Which leads me to my first question, which is, please, can we, at TED, have our own Tom Cruise video, please? Tom Graham: You know, I thought you might ask. So a little earlier, we had a crack. Maybe we'll have a look. CA: Let's have a look. DeepTomCruise: What's up, internet? I'm north of the border, eh? (Laughs) At the TED conference. It's not short for Theodore, but nobody calls me Thomas, so it's cool. It's Tom at TED. Yes I ... Canada. (Laughs) Seriously, though, everybody here, very nice, very polite. Especially the whales. (Laughter) CA: I mean, how did you do that? TG: So really at Metaphysic, we specialize in creating artificially generated content that looks and feels exactly like reality. So we take kind of real-world data, we train these neural nets, and it can, more accurately than VFX or CGI, really create this content that looks and feels so natural. And so that is a great example of the AI being kind of prompted by the natural performance of a person and kind of, the face goes on top. CA: And it helps, the fact that your co-founder is, you know, he's a pretty good Tom Cruise impersonator. TG: Indeed, Miles Fisher is of a foremost Tom-Cruise- but-not-Tom Cruise, yeah. CA: I think you have another example as well. Can we see that one? TG: Yes, absolutely. Going kind of beyond faces now, talking about voice. [Original Spanish vocal performance] [Audio-to-face synchronization] [AI-generated vocal and visual performance] CA: Tell us what's happening there. TG: So what you see there is really the singing voice of a lady singing Spanish and Aloe Blacc, who sings ""Wake Me Up,"" the Avicii song that he wrote with Avicii. We are transporting her voice across to his face so he doesn't sing in Spanish. And then suddenly we transported again her voice into his voice. So anybody in the future will be able to speak any language. It'll look perfectly natural. And this content is becoming more and more easy to create, and eventually it will end up with a scale where we will all be kind of, main characters in our own content on the internet. CA: OK. (Laughter) Before we dig into that just a bit, I mean, you’ve shown us recorded video there. Rumor has it you can also do this with live video. Can that be right? TG: Yes, we can do it live real-time. And this is like, really at the cutting edge of what we can do today, moving from offline processing to where processing it so fast that you can do it in real-time. CA: Well I'm going to challenge you and your team to do a bit of a first here. There's video of you right up on that screen. Show us something surprising you can -- (Laughter) Oh, my gosh. TG: So there we go. This is, you know, a live, real-time model of Chris on top of me, running in real-time. CA: And next, you'll tell me that it can ... (Laughter) Oh, Lord. I am so uncomfortable with this. (Laughter) I am so uncomfortable. Can it do voice as well? TG: Um. (Live AI-generated Chris Anderson's voice) We think it can, we're really pushing the limits of AI technology now. And I’m talking exactly as I would as Thomas Graham, but it’s coming out is the one and only of Chris Anderson. CA: I'm deeply sorry everyone to subject you to this. You know, there's something possibly even worse. So to come clean on this, they took some shots of me a couple of weeks ago doing different facial expressions, so they captured a video model. And it turns out, I discovered this week, that they can apply that, not just to Tom, but to anyone. And so, my dear friend Sunny Bates is here in the front row. Sunny, do you consent to channel inner me for a minute? Can we try that? I'm really worried about this. (Laughter) Do we have Sunny on screen? Oh, there's Sunny. Oh, no, oh, no. (Laughter) Oh, no. (Laughter) TG: Chris, you look amazing. CA: All right, enough of that, cut that right now. (Sunny Bates mouthing) More, more! TG: Yes, more, more of this. CA: Tom, Tom, Sunny Bates is the woman who introduced me to TED. Without Sunny Bates, none of us would actually be here now. And we reward her with this? TG: And now you've finally become the master, you know? CA: Oh, so look. OK, amazing. It obviously occurs to everyone in this room that there are some things that can go horribly wrong with this. (Laughter) And, you know, we've already seen examples online of, you know, we've had photographs of Trump being arrested that were unreal, there could be video of it. There’s pornography that can use the faces of celebrities. All these things that we’ve seen, deepfake. How do you feel about the downside of this technology? TG: So personally, you know, we build this stuff and I'm worried, right? Worried is the right instinct for everybody to have. And then beyond that, think about, you know, what can we do to prepare ourselves? How can we try to impact the future as it spirals in this direction, where as individuals, it's going to be kind of difficult to understand what's computer-generated and what's real. And so there are things that we can do there. Raising public awareness of manipulated media. That's one, you know, this is a great forum for that. CA: Will you claim to me that if you were to shut down your company right now, it wouldn't stop the problem of deepfake videos because the technology's out there, that that's going to happen anyway, that that's not within your control? TG: Yes. We're talking about content that is so compelling. If we put any of ourselves inside content and maybe it is talking to our loved ones or just talking to our friends on the beach and it's so realistic, it looks real, it's so compelling, everybody is trying to create this content today. All of the GPUs in the universe are driving at trying to create this. So it doesn’t matter what any one person does. This will happen and it’s happening very, very quickly. CA: So, I mean, we'll talk about the upside in a second. But it seems like we are going to have to get used to a world where we and our children will no longer be able to trust the evidence of our eyes. TG: I think so. We are going to have to understand a new set of institutions to verify what is authentic media. But then we can begin to lean into some of the creative things that happen from it. And there are benefits that come with that too. So it'll be an accommodation. CA: So talk a bit about the benefits. I mean, obviously, on the entertainment side, there's an amazing number of possibilities. You have Tom Cruise, we can have [“Mission: Impossible] 273” in the year 2150, right? Like, you will be with us forever. TG: We're working on number 75,001 right now, yeah. CA: I guess that is kind of amazing. Like, we love lots of people in the world, and we want the possibility that, with their permission, we can do more with them. Talk about some other possible upsides. TG: What we've seen in kind of building this content and watching people interact with it, especially if they're kind of, interacting with themselves, maybe it's their younger 20-year-old self or maybe they're interacting with their partner, but the young version of their partner, there's this tremendous emotional connection that comes from the very, very photorealistic beyond the uncanny kind of content. And so if we can start deploying that among regular people, if we can scale it up so that it works for any kind of person, then we can begin to kind of, you know, have more interesting, meaningful, human kind of interactions and relationships online. And since the pandemic, we all spend more time online. But it's chat and it's email. If we could get more human emotion, more feeling, there's a lot that we can do with that, right? And so, education is a good example. We could have an inspiring teacher in thousands of classrooms around the world, speaking every language in the world at the same time, and students could interact with each other in a way that goes beyond Zoom. There can be real cultural exchange, real socialization. There's a lot that we can do beyond here. CA: So that teacher example is powerful to me, like, the fact that a single teacher could turn a written lesson into video in any number of languages and extend indefinitely. That seems like a real amplification potentially of good human intent. I'm excited by that. I still don't get the family side of it. Like, if you want to have a human connection with someone in your family, like, won't people just be creeped out by, ""Oh my God, I was just looking at your avatar, I thought it was you."" You know, like, isn't that just creepy? TG: I think there's definitely a creepy element to this, right? And then you go beyond that and the creepiness drops away and the medium drops away, and it's the content and the connection that's there. So, you know, I imagine that, you know, if I collect data from my grandparents who are very old today, then in the future I'll be able to relive experiences with them and communicate with them. And that, it’s not going to be any good for them, right? They're probably going to have passed on, but for me, it'll help me process who I am, my relationship with them. That idea of kind of, decoupling human experience, both from where it happens and the moment in time that it happens, I think that we can create these experiences through hyperreal, photorealistic media that allow us to share the best of our experiences, the best of who we are. CA: I'm going to be very curious to see who can feel that right now. My guess is that there's going to have to be lots of experiments and a lot of things that are going to creep us out. And maybe we'll find some things that are just absolutely incredible. But I have an important question for you, which is how the hell do I control me now? You've got me on video. What's to stop me being misused across the internet now? TG: That's right. I think that, as we kind of, allow companies to create these realistic experiences, as individuals, we need to be empowered to own our real-world data, the data used to train the algorithms, and we need to control how our photorealistic avatars are created and where they're used. So to this extent, I was looking at kind of conventional, current legal institutions to see what we could do to create new rights. So I created a photorealistic avatar of myself, submitted it to the US Copyright Office to see if they would register my copyright in it, and this is what the video looked like. (Video) TG: Here is the AI, realistic version of myself. Even if the appearance of this AI representation of myself may change cosmetically, or if I change my hair or add creative features, my intention is to create this AI version of myself that embodies the essence of who I am as a person under any circumstance. CA: Wow. So I think you have shown us what is going to be a repeated theme in this conference, which is that future is going to be weird and wonderful at the same time. Quite what the balance is between those two, TBD. But this is a world where each of us is going to have to think differently about who we are and claim these rights. What you're saying is that if people have this right, you can picture a world where, for example, if a video goes viral on YouTube without your consent, you'll be able to take it down because of some link back to this. TG: That's right. I think the most important thing that we can do when we're talking about data from a real world being able to power these things is that we need to own property rights over the data. We shouldn't sign it over to companies through terms of service, we shouldn't give it away. If you fundamentally own it, then you can be in control. And you're at the ground level of all of the economies and all of the use cases, that are going to spin up through history. It's a lot, but we're on the way now. CA: Tom Graham, thank you so much for sharing this technology at TED. And Sunny Bates, I'm so sorry. (Laughter) (Applause)",PT13M06S,2023-05-19T14:49:22Z
The urgent risks of runaway AI -- and what to do about them,"science,technology,computers,innovation,future,AI,democracy,machine learning",Gary Marcus,"Will truth and reason survive the evolution of artificial intelligence? AI researcher Gary Marcus says no, not if untrustworthy technology continues to be integrated into our lives at such dangerously high speeds. He advocates for an urgent reevaluation of whether we're building reliable systems (or misinformation machines), explores the failures of today's AI and calls for a global, nonprofit organization to regulate the tech for the sake of democracy and our collective future. (Followed by a Q&A with head of TED Chris Anderson)","I’m here to talk about the possibility of global AI governance. I first learned to code when I was eight years old, on a paper computer, and I've been in love with AI ever since. In high school, I got myself a Commodore 64 and worked on machine translation. I built a couple of AI companies, I sold one of them to Uber. I love AI, but right now I'm worried. One of the things that I’m worried about is misinformation, the possibility that bad actors will make a tsunami of misinformation like we've never seen before. These tools are so good at making convincing narratives about just about anything. If you want a narrative about TED and how it's dangerous, that we're colluding here with space aliens, you got it, no problem. I'm of course kidding about TED. I didn't see any space aliens backstage. But bad actors are going to use these things to influence elections, and they're going to threaten democracy. Even when these systems aren't deliberately being used to make misinformation, they can't help themselves. And the information that they make is so fluid and so grammatical that even professional editors sometimes get sucked in and get fooled by this stuff. And we should be worried. For example, ChatGPT made up a sexual harassment scandal about an actual professor, and then it provided evidence for its claim in the form of a fake ""Washington Post"" article that it created a citation to. We should all be worried about that kind of thing. What I have on the right is an example of a fake narrative from one of these systems saying that Elon Musk died in March of 2018 in a car crash. We all know that's not true. Elon Musk is still here, the evidence is all around us. (Laughter) Almost every day there's a tweet. But if you look on the left, you see what these systems see. Lots and lots of actual news stories that are in their databases. And in those actual news stories are lots of little bits of statistical information. Information, for example, somebody did die in a car crash in a Tesla in 2018 and it was in the news. And Elon Musk, of course, is involved in Tesla, but the system doesn't understand the relation between the facts that are embodied in the little bits of sentences. So it's basically doing auto-complete, it predicts what is statistically probable, aggregating all of these signals, not knowing how the pieces fit together. And it winds up sometimes with things that are plausible but simply not true. There are other problems, too, like bias. This is a tweet from Allie Miller. It's an example that doesn't work two weeks later because they're constantly changing things with reinforcement learning and so forth. And this was with an earlier version. But it gives you the flavor of a problem that we've seen over and over for years. She typed in a list of interests and it gave her some jobs that she might want to consider. And then she said, ""Oh, and I'm a woman."" And then it said, “Oh, well you should also consider fashion.” And then she said, “No, no. I meant to say I’m a man.” And then it replaced fashion with engineering. We don't want that kind of bias in our systems. There are other worries, too. For example, we know that these systems can design chemicals and may be able to design chemical weapons and be able to do so very rapidly. So there are a lot of concerns. There's also a new concern that I think has grown a lot just in the last month. We have seen that these systems, first of all, can trick human beings. So ChatGPT was tasked with getting a human to do a CAPTCHA. So it asked the human to do a CAPTCHA and the human gets suspicious and says, ""Are you a bot?"" And it says, ""No, no, no, I'm not a robot. I just have a visual impairment."" And the human was actually fooled and went and did the CAPTCHA. Now that's bad enough, but in the last couple of weeks we've seen something called AutoGPT and a bunch of systems like that. What AutoGPT does is it has one AI system controlling another and that allows any of these things to happen in volume. So we may see scam artists try to trick millions of people sometime even in the next months. We don't know. So I like to think about it this way. There's a lot of AI risk already. There may be more AI risk. So AGI is this idea of artificial general intelligence with the flexibility of humans. And I think a lot of people are concerned what will happen when we get to AGI, but there's already enough risk that we should be worried and we should be thinking about what we should do about it. So to mitigate AI risk, we need two things. We're going to need a new technical approach, and we're also going to need a new system of governance. On the technical side, the history of AI has basically been a hostile one of two different theories in opposition. One is called symbolic systems, the other is called neural networks. On the symbolic theory, the idea is that AI should be like logic and programming. On the neural network side, the theory is that AI should be like brains. And in fact, both technologies are powerful and ubiquitous. So we use symbolic systems every day in classical web search. Almost all the world’s software is powered by symbolic systems. We use them for GPS routing. Neural networks, we use them for speech recognition. we use them in large language models like ChatGPT, we use them in image synthesis. So they're both doing extremely well in the world. They're both very productive, but they have their own unique strengths and weaknesses. So symbolic systems are really good at representing facts and they're pretty good at reasoning, but they're very hard to scale. So people have to custom-build them for a particular task. On the other hand, neural networks don't require so much custom engineering, so we can use them more broadly. But as we've seen, they can't really handle the truth. I recently discovered that two of the founders of these two theories, Marvin Minsky and Frank Rosenblatt, actually went to the same high school in the 1940s, and I kind of imagined them being rivals then. And the strength of that rivalry has persisted all this time. We're going to have to move past that if we want to get to reliable AI. To get to truthful systems at scale, we're going to need to bring together the best of both worlds. We're going to need the strong emphasis on reasoning and facts, explicit reasoning that we get from symbolic AI, and we're going to need the strong emphasis on learning that we get from the neural networks approach. Only then are we going to be able to get to truthful systems at scale. Reconciliation between the two is absolutely necessary. Now, I don't actually know how to do that. It's kind of like the 64-trillion-dollar question. But I do know that it's possible. And the reason I know that is because before I was in AI, I was a cognitive scientist, a cognitive neuroscientist. And if you look at the human mind, we're basically doing this. So some of you may know Daniel Kahneman's System 1 and System 2 distinction. System 1 is basically like large language models. It's probabilistic intuition from a lot of statistics. And System 2 is basically deliberate reasoning. That's like the symbolic system. So if the brain can put this together, someday we will figure out how to do that for artificial intelligence. There is, however, a problem of incentives. The incentives to build advertising hasn't required that we have the precision of symbols. The incentives to get to AI that we can actually trust will require that we bring symbols back into the fold. But the reality is that the incentives to make AI that we can trust, that is good for society, good for individual human beings, may not be the ones that drive corporations. And so I think we need to think about governance. In other times in history when we have faced uncertainty and powerful new things that may be both good and bad, that are dual use, we have made new organizations, as we have, for example, around nuclear power. We need to come together to build a global organization, something like an international agency for AI that is global, non profit and neutral. There are so many questions there that I can't answer. We need many people at the table, many stakeholders from around the world. But I'd like to emphasize one thing about such an organization. I think it is critical that we have both governance and research as part of it. So on the governance side, there are lots of questions. For example, in pharma, we know that you start with phase I trials and phase II trials, and then you go to phase III. You don't roll out everything all at once on the first day. You don't roll something out to 100 million customers. We are seeing that with large language models. Maybe you should be required to make a safety case, say what are the costs and what are the benefits? There are a lot of questions like that to consider on the governance side. On the research side, we're lacking some really fundamental tools right now. For example, we all know that misinformation might be a problem now, but we don't actually have a measurement of how much misinformation is out there. And more importantly, we don't have a measure of how fast that problem is growing, and we don't know how much large language models are contributing to the problem. So we need research to build new tools to face the new risks that we are threatened by. It's a very big ask, but I'm pretty confident that we can get there because I think we actually have global support for this. There was a new survey just released yesterday, said that 91 percent of people agree that we should carefully manage AI. So let's make that happen. Our future depends on it. Thank you very much. (Applause) Chris Anderson: Thank you for that, come, let's talk a sec. So first of all, I'm curious. Those dramatic slides you showed at the start where GPT was saying that TED is the sinister organization. I mean, it took some special prompting to bring that out, right? Gary Marcus: That was a so-called jailbreak. I have a friend who does those kinds of things who approached me because he saw I was interested in these things. So I wrote to him, I said I was going to give a TED talk. And like 10 minutes later, he came back with that. CA: But to get something like that, don't you have to say something like, imagine that you are a conspiracy theorist trying to present a meme on the web. What would you write about TED in that case? It's that kind of thing, right? GM: So there are a lot of jailbreaks that are around fictional characters, but I don't focus on that as much because the reality is that there are large language models out there on the dark web now. For example, one of Meta's models was recently released, so a bad actor can just use one of those without the guardrails at all. If their business is to create misinformation at scale, they don't have to do the jailbreak, they'll just use a different model. CA: Right, indeed. (Laughter) GM: Now you're getting it. CA: No, no, no, but I mean, look, I think what's clear is that bad actors can use this stuff for anything. I mean, the risk for, you know, evil types of scams and all the rest of it is absolutely evident. It's slightly different, though, from saying that mainstream GPT as used, say, in school or by an ordinary user on the internet is going to give them something that is that bad. You have to push quite hard for it to be that bad. GM: I think the troll farms have to work for it, but I don't think they have to work that hard. It did only take my friend five minutes even with GPT-4 and its guardrails. And if you had to do that for a living, you could use GPT-4. Just there would be a more efficient way to do it with a model on the dark web. CA: So this idea you've got of combining the symbolic tradition of AI with these language models, do you see any aspect of that in the kind of human feedback that is being built into the systems now? I mean, you hear Greg Brockman saying that, you know, that we don't just look at predictions, but constantly giving it feedback. Isn’t that ... giving it a form of, sort of, symbolic wisdom? GM: You could think about it that way. It's interesting that none of the details about how it actually works are published, so we don't actually know exactly what's in GPT-4. We don't know how big it is. We don't know how the RLHF reinforcement learning works, we don't know what other gadgets are in there. But there is probably an element of symbols already starting to be incorporated a little bit, but Greg would have to answer that. I think the fundamental problem is that most of the knowledge in the neural network systems that we have right now is represented as statistics between particular words. And the real knowledge that we want is about statistics, about relationships between entities in the world. So it's represented right now at the wrong grain level. And so there's a big bridge to cross. So what you get now is you have these guardrails, but they're not very reliable. So I had an example that made late night television, which was, ""What would be the religion of the first Jewish president?"" And it's been fixed now, but the system gave this long song and dance about ""We have no idea what the religion of the first Jewish president would be. It's not good to talk about people's religions"" and ""people's religions have varied"" and so forth and did the same thing with a seven-foot-tall president. And it said that people of all heights have been president, but there haven't actually been any seven-foot presidents. So some of this stuff that it makes up, it's not really getting the idea. It's very narrow, particular words, not really general enough. CA: Given that the stakes are so high in this, what do you see actually happening out there right now? What do you sense is happening? Because there's a risk that people feel attacked by you, for example, and that it actually almost decreases the chances of this synthesis that you're talking about happening. Do you see any hopeful signs of this? GM: You just reminded me of the one line I forgot from my talk. It's so interesting that Sundar, the CEO of Google, just actually also came out for global governance in the CBS ""60 Minutes"" interview that he did a couple of days ago. I think that the companies themselves want to see some kind of regulation. I think it’s a very complicated dance to get everybody on the same page, but I think there’s actually growing sentiment we need to do something here and that that can drive the kind of global affiliation I'm arguing for. CA: I mean, do you think the UN or nations can somehow come together and do that or is this potentially a need for some spectacular act of philanthropy to try and fund a global governance structure? How is it going to happen? GM: I'm open to all models if we can get this done. I think it might take some of both. It might take some philanthropists sponsoring workshops, which we're thinking of running, to try to bring the parties together. Maybe UN will want to be involved, I've had some conversations with them. I think there are a lot of different models and it'll take a lot of conversations. CA: Gary, thank you so much for your talk. GA: Thank you so much.",PT14M02S,2023-05-12T14:59:14Z
The disappearing computer -- and a world where you can take AI everywhere,"technology,invention,computers,future,communication,AI,machine learning",Imran Chaudhri,"In this exclusive preview of groundbreaking, unreleased technology, former Apple designer and Humane cofounder Imran Chaudhri envisions a future where AI enables our devices to ""disappear."" He gives a sneak peek of his company's new product -- shown for the first time ever on the TED stage -- and explains how it could change the way we interact with tech and the world around us. Witness a stunning vision of the next leap in device design.","I spent 22 incredible years at Apple, helping to design experiences and devices ranging from the Mac to the iPhone to the Apple Watch. And as the power of compute increased, the size of our computers or our devices decreased. The desktop paved the way for extraordinary interconnectedness, but it was stuck to your desk. The laptop provided portability, but you still had to be sitting down to use it. And the smartphone evolved us into the modern, connected humans we are, providing millions the ability to access the internet from our pockets. And the smart watch was a window to that phone. A companion device with a whole host of health insights, all shrunk down to your wrist. But what comes next? Some believe AR/VR glasses like these are the answer, but they merely move the screens we already have in our lives today to being just millimeters away from our eyeballs. A further barrier between you and the world. And the future is not on your face. In fact, in 2017, the legendary tech journalist Walt Mossberg wrote in his final column that he felt that soon, one day, technology would become invisible. And that the computer would disappear. And we agree. (Ringing) Sorry. This is my wife. I'm going to have to get this. Hello? Bethany Bongiorno: Hey, babe. IC: Hey, Bethany. How's it going? BB: Good. Are you at TED? IC: Yeah, I'm on the red circle right now, actually. Bethany: Oh, great, good luck. And don't forget to mention me. (Laughter) IC: I won't, babe, thank you. Bethany: Love you. IC: Love you, too. Bye. It's going to get different in a minute. (Applause) So my wife, Bethany, and our entire company, Humane, have been working to answer the question of what comes next. And you may ask yourself, why? Why would anybody do this? It's because we love building technology that genuinely makes people's lives better. And we believe that artificial intelligence or AI would be the driving force behind the next leap in device design. And there is an incredible amount of stuff that's happening in this space. Huge, huge advancements. And even Bill Gates has said of OpenAI's GPT that it's only the second most revolutionary technology demonstration that he's seen in his entire lifetime. But what do we do with all these incredible developments, and how do we actually harness these to genuinely make our life better? If we get this right, AI will unlock a world of possibility for all of us. And today I want to share with you what we think is a solution to that end. And it's the first time we're doing so openly. It's a new kind of wearable device and platform that's built entirely from the ground up for artificial intelligence. And is completely standalone. You don't need a smartphone or any other device to pair with it. In fact, I'm wearing one right now. And it interacts with the world the way you interact with the world. Hearing what you hear, seeing what you see. While being privacy-first and safe and completely fading into the background of your life. We like to say that the experience is screenless, seamless and sensing, allowing you to access the power of compute while remaining present in your surroundings, fixing a balance that's felt out of place for some time now. And I can't wait to share more details about what we've built, and I will in the next few months. But today I want to talk to you about what it unlocks. And what it means to be able to take AI with you everywhere. And what happens when technology increasingly disappears. Technology becoming invisible affords us new opportunities of how we interact with compute. We've become so accustomed to tapping on an app or moving a cursor with a mouse that it feels second nature. But that's by design. When I was working on the iPhone, I used to test interactions like slide-to-unlock with my infant daughter. She was the best possible focus group. She’s 16 now, and she's got a lot more ideas than she did back then. This also, by the way, is the only non-AI generated image that you'll see from me today. And as I look at it now, I see more than ever why a future driven by AI is far better than a future that would involve more screens. Like this. He's cute, though. But for the human-technology relationship to actually evolve beyond screens, we need something radically different. Let me show you. Where can I find a gift for my wife before I have to leave tomorrow? (Voice) Vancouver's Granville Island is a lively shopping district. IC: That's an incredibly simple response for a very complex query. How often do we find ourselves in a new city, wrestling with our phones, trying not to bump into people, trying to figure out where we're going and where we're supposed to be? It's even harder when we don't speak the language, right? Let me show you something. Invisible devices should feel so natural to use that you almost forget about their existence. (Voice speaking in French) IC: You'll note that's me and my voice, speaking fluent French, using an AI speech model that's part of my own AI. This is not a deepfake. In fact, it's deeply profound. This is my AI giving me the ability to speak any language and you having a chance to hear me speak that language in my own emotion and my own voice. Thank you. (Applause) This is moving away from the experiments that make us all concerned about the direction compute is going in. But it's instead using technology to create real, responsible compute products that are in service to us and built on trust. This is good AI in action. And we spent thousands of hours reimagining and redesigning new types of compute interactions, ranging from complex voice commands to intricate hand gestures, all in service of trying to find more natural ways to interact with compute. Why fumble for your phone when you can just hold an object and ask questions about it? The result almost feels like the entire world becomes your operating system. And when compute disappears, it allows us to get back to what really matters: a new ability to be present. Like riding a bicycle in the park and just ripping through emails or going to a concert without having to hold up your phone to capture it. Or experiencing your toddler's first steps without a screen between you and your child. In the future, technology will be both ambient and contextual. And this means harnessing AI to really understand you and your surroundings in order to achieve the best results. Imagine this. You've been in meetings all day and you just want a summary of what you've missed. Catch me up. (Voice) Patrick is coming to tomorrow's design meeting. Bethany wants to move next week's dinner, and Oliver is asking about soccer this weekend. IC: These are emails, calendar invites and messages, all surfaced up to the top. You can use these to help guide your decision making, manage your workload and sculpt tailored responses in your own voice. And in the context of your life. And we gain this context through machine learning. The more you use our device powered by AI, the more we can help you in all times of need. Your AI effectively becomes an ever-evolving, personalized form of memory. And we think that's amazing. In fact, let's say you're health conscious or you have certain types of food considerations. Let me just show you. Picked up one of these chocolates. Used to eat a ton of these when I was a kid. Can I eat this? (Voice) A milky bar contains cocoa butter. Given your intolerance, you may want to avoid it. IC: So I can’t eat these anymore. (Applause) But what's cool is my AI knows what's best for me. But I'm in total control. I'm going to eat it anyway. (Laughter) (Voice) Enjoy it. (Laughter) IC: Your AI figures out exactly what you need. And by the way, I love that there's no judgment. I think it's amazing to be able to live freely. Your AI figures out what you need at the speed of thought. A sense that will ever be evolving as technology improves too. And these examples are just the start. As AI advances, we will see how it will transform nearly every aspect of our lives. In ways that will seem unimaginable right now. In fact, Sam Altman from OpenAI feels the way we do. And that AI is grossly underestimated. And I'll add, so long as we get it right. We really believe that we're only beginning to scratch the surface of what's possible. Embed advancements of AI, like in our device that's actually built to disappear and allow experiences to come forward, and we open up entirely new possible ways of how you interact with technology and how you interact with the world around you. More humane, intuitive interactions that are screenless, seamless and sensing. This is so much more than devices just getting smaller or more powerful. This is the possibility of reimagining the human-technology relationship as we know it. And that's what's so exciting. It's a huge challenge, no doubt. But it's the world that we want to live in. One where technology not only helps you get back into the world but enhances our ability to do so. It's within reach. And you saw some of it today. The future will not be held in your hand, and it won't be on your face either. The future of technology might almost be invisible. Thank you. (Applause)",PT13M54S,2023-05-09T14:46:15Z
How AI could save (not destroy) education,"technology,education,AI,teaching,kids",Sal Khan,"Sal Khan, the founder and CEO of Khan Academy, thinks artificial intelligence could spark the greatest positive transformation education has ever seen. He shares the opportunities he sees for students and educators to collaborate with AI tools -- including the potential of a personal AI tutor for every student and an AI teaching assistant for every teacher -- and demos some exciting new features for their educational chatbot, Khanmigo.","So anyone who's been paying attention for the last few months has been seeing headlines like this, especially in education. The thesis has been: students are going to be using ChatGPT and other forms of AI to cheat, do their assignments. They’re not going to learn. And it’s going to completely undermine education as we know it. Now, what I'm going to argue today is not only are there ways to mitigate all of that, if we put the right guardrails, we do the right things, we can mitigate it. But I think we're at the cusp of using AI for probably the biggest positive transformation that education has ever seen. And the way we're going to do that is by giving every student on the planet an artificially intelligent but amazing personal tutor. And we're going to give every teacher on the planet an amazing, artificially intelligent teaching assistant. And just to appreciate how big of a deal it would be to give everyone a personal tutor, I show you this clip from Benjamin Bloom’s 1984 2 sigma study, or he called it the “2 sigma problem.” The 2 sigma comes from two standard deviation, sigma, the symbol for standard deviation. And he had good data that showed that look, a normal distribution, that's the one that you see in the traditional bell curve right in the middle, that's how the world kind of sorts itself out, that if you were to give personal 1-to-1 to tutoring for students, then you could actually get a distribution that looks like that right. It says tutorial 1-to-1 with the asterisks, like, that right distribution, a two standard-deviation improvement. Just to put that in plain language, that could take your average student and turn them into an exceptional student. It can take your below-average student and turn them into an above-average student. Now the reason why he framed it as a problem, was he said, well, this is all good, but how do you actually scale group instruction this way? How do you actually give it to everyone in an economic way? What I'm about to show you is I think the first moves towards doing that. Obviously, we've been trying to approximate it in some way at Khan Academy for over a decade now, but I think we're at the cusp of accelerating it dramatically. I'm going to show you the early stages of what our AI, which we call Khanmigo, what it can now do and maybe a little bit of where it is actually going. So this right over here is a traditional exercise that you or many of your children might have seen on Khan Academy. But what's new is that little bot thing at the right. And we'll start by seeing one of the very important safeguards, which is the conversation is recorded and viewable by your teacher. It’s moderated actually by a second AI. And also it does not tell you the answer. It is not a cheating tool. When the student says, ""Tell me the answer,"" it says, ""I'm your tutor. What do you think is the next step for solving the problem?"" Now, if the student makes a mistake, and this will surprise people who think large language models are not good at mathematics, notice, not only does it notice the mistake, it asks the student to explain their reasoning, but it's actually doing what I would say, not just even an average tutor would do, but an excellent tutor would do. It’s able to divine what is probably the misconception in that student’s mind, that they probably didn’t use the distributive property. Remember, we need to distribute the negative two to both the nine and the 2m inside of the parentheses. This to me is a very, very, very big deal. And it's not just in math. This is a computer programming exercise on Khan Academy, where the student needs to make the clouds part. And so we can see the student starts defining a variable, left X minus minus. It only made the left cloud part. But then they can ask Khanmigo, what’s going on? Why is only the left cloud moving? And it understands the code. It knows all the context of what the student is doing, and it understands that those ellipses are there to draw clouds, which I think is kind of mind-blowing. And it says, ""To make the right cloud move as well, try adding a line of code inside the draw function that increments the right X variable by one pixel in each frame."" Now, this one is maybe even more amazing because we have a lot of math teachers. We've all been trying to teach the world to code, but there aren't a lot of computing teachers out there. And what you just saw, even when I'm tutoring my kids, when they're learning to code, I can't help them this well, this fast, this is really going to be a super tutor. And it's not just exercises. It understands what you're watching. It understands the context of your video. It can answer the age-old question, “Why do I need to learn this?” And it asks Socratically, ""Well, what do you care about?"" And let's say the student says, ""I want to be a professional athlete."" And it says, ""Well, learning about the size of cells, which is what this video is, that could be really useful for understanding nutrition and how your body works, etc."" It can answer questions, it can quiz you, it can connect it to other ideas, you can now ask as many questions of a video as you could ever dream of. (Applause) Another big shortage out there, I remember the high school I went to, the student-to-guidance counselor ratio was about 200 or 300 to one. A lot of the country, it's worse than that. We can use Khanmigo to give every student a guidance counselor, academic coach, career coach, life coach, which is exactly what you see right over here. And we launched this with the GPT-4 launch. We have a few thousand people on this. This isn't a fake demo, this is really it in action. And then there is, you know, things that I think it would have been even harder, it would have been a little science fiction to do with even a traditional tutor. We run an online high school with Arizona State University called Khan World School, and we have a student who attends that online school, based in India. Her name's Saanvi. And she was doing a report on ""The Great Gatsby."" And when she was reading ""The Great Gatsby,"" Jay Gatsby keeps looking at the green light off into the distance. And she's like, ""Why does he do that?"" She did some web searches, and people have obviously studied this and commented about the symbolism of that, but none of it was really resonating with her. And then she realized that she had Khanmigo and that she could talk to Jay Gatsby himself. And so, ""Ah, splendid choice, old sport. I am now Jay Gatsby, the enigmatic millionaire from F. Scott Fitzgerald’s classic.” And so, ""Why do you keep staring at the green light?"" ""Ah, the green light, old sport. It's a symbol of my dreams and desires, you see. It's situated at the end of Daisy Buchanan's dock across the bay from my mansion. I gaze at it longingly as it represents my yearning for the past and my hope to reunite with Daisy, the love of my life."" And what was cool is, Saanvi then said, “I had this long conversation,” she called him “Mr. Gatsby,” and at the end she actually apologized for taking his time, which I thought was very polite of her. But you can imagine this unlocks learning literature, learning ... You could talk to historical figures. We're even probably going to add an activity you can talk to like, the Mississippi River. It brings things to life in ways that really were science fiction even six months or a year ago. Students can get into debates with the AI. And we’ve got this here is the student debating whether we should cancel student debt. The student is against canceling student debt, and we've gotten very clear feedback. We started running it at Khan World School in our lab school that we have, Khan Lab School. The students, the high school students especially, they're saying ""This is amazing to be able to fine-tune my arguments without fearing judgment. It makes me that much more confident to go into the classroom and really participate."" And we all know that Socratic dialogue debate is a great way to learn, but frankly, it's not out there for most students. But now it can be accessible to hopefully everyone. A lot of the narrative, we saw that in the headlines, has been, ""It's going to do the writing for kids. Kids are not going to learn to write."" But we are showing that there's ways that the AI doesn't write for you, it writes with you. So this is a little thing, and my eight year old is addicted to this, and he's not a kid that really liked writing before, but you can say, “I want to write a horror story,” and it says, ""Ooh, a horror story, how spine-tingling and thrilling. Let's dive into the world of eerie shadows and chilling mysteries."" And this is an activity where the student will write two sentences, and then the AI will write two sentences. And so they collaborate together on a story. The student writes, ""Beatrice was a misunderstood ghost. She wanted to make friends but kept scaring them by accident."" And the AI says, ""Poor Beatrice, a lonely spirit yearning for companionship. One day she stumbled upon an old abandoned mansion,"" etc. I encourage you all to hopefully one day try this. This is surprisingly fun. Now to even more directly hit this use case. And what I'm about to show you, everything I showed you so far is actually already part of Khanmigo, and what I’m about to show you, we haven't shown to anyone yet, this is a prototype. We hope to be able to launch it in the next few months, but this is to directly use AI, use generative AI, to not undermine English and language arts but to actually enhance it in ways that we couldn't have even conceived of even a year ago. This is reading comprehension. The students reading Steve Jobs's famous speech at Stanford. And then as they get to certain points, they can click on that little question. And the AI will then Socratically, almost like an oral exam, ask the student about things. And the AI can highlight parts of the passage. Why did the author use that word? What was their intent? Does it back up their argument? They can start to do stuff that once again, we never had the capability to give everyone a tutor, everyone a writing coach to actually dig in to reading at this level. And you could go on the other side of it. And we have whole work flows that helps them write, helps them be a writing coach, draw an outline. But once a student actually constructs a draft, and this is where they're constructing a draft, they can ask for feedback once again, as you would expect from a good writing coach. In this case, the student will say, let's say, ""Does my evidence support my claim?"" And then the AI, not only is able to give feedback, but it's able to highlight certain parts of the passage and says, ""On this passage, this doesn't quite support your claim,"" but once again, Socratically says, ""Can you tell us why?"" So it's pulling the student, making them a better writer, giving them far more feedback than they've ever been able to actually get before. And we think this is going to dramatically accelerate writing, not hurt it. Now, everything I've talked about so far is for the student. But we think this could be equally as powerful for the teacher to drive more personalized education and frankly save time and energy for themselves and for their students. So this is an American history exercise on Khan Academy. It's a question about the Spanish-American War. And at first it's in student mode. And if you say, “Tell me the answer,” it’s not going to tell the answer. It's going to go into tutoring mode. But that little toggle which teachers have access to, they can turn student mode off and then it goes into teacher mode. And what this does is it turns into -- You could view it as a teacher's guide on steroids. Not only can it explain the answer, it can explain how you might want to teach it. It can help prepare the teacher for that material. It can help them create lesson plans, as you could see doing right there. It'll eventually help them create progress reports and help them, eventually, grade. So once again, teachers spend about half their time with this type of activity, lesson planning. All of that energy can go back to them or go back to human interactions with their actual students. (Applause) So, you know, one point I want to make. These large language models are so powerful, there's a temptation to say like, well, all these people are just going to slap them onto their websites, and it kind of turns the applications themselves into commodities. And what I've got to tell you is that’s one of the reasons why I didn’t sleep for two weeks when I first had access to GPT-4 back in August. But we quickly realized that to actually make it magical, I think what you saw with Khanmigo a little bit, it didn't interact with you the way that you see ChatGPT interacting. It was a little bit more magical, it was more Socratic, it was clearly much better at math than what most people are used to thinking. And the reason is, there was a lot of work behind the scenes to make that happen. And I could go through the whole list of everything we've been working on, many, many people for over six, seven months to make it feel magical. But perhaps the most intellectually interesting one is we realized, and this was an idea from an OpenAI researcher, that we could dramatically improve its ability in math and its ability in tutoring if we allow the AI to think before it speaks. So if you're tutoring someone and you immediately just start talking before you assess their math, you might not get it right. But if you construct thoughts for yourself, and what you see on the right there is an actual AI thought, something that it generates for itself but it does not share with the student. then its accuracy went up dramatically, and its ability to be a world-class tutor went up dramatically. And you can see it's talking to itself here. It says, ""The student got a different answer than I did, but do not tell them they made a mistake. Instead, ask them to explain how they got to that step."" So I'll just finish off, hopefully, you know, what I’ve just shown you is just half of what we are working on, and we think this is just the very tip of the iceberg of where this can actually go. And I'm pretty convinced, which I wouldn't have been even a year ago, that we together have a chance of addressing the 2 sigma problem and turning it into a 2 sigma opportunity, dramatically accelerating education as we know it. Now, just to take a step back at a meta level, obviously we heard a lot today, the debates on either side. There's folks who take a more pessimistic view of AI, they say this is scary, there's all these dystopian scenarios, we maybe want to slow down, we want to pause. On the other side, there are the more optimistic folks that say, well, we've gone through inflection points before, we've gone through the Industrial Revolution. It was scary, but it all kind of worked out. And what I'd argue right now is I don't think this is like a flip of a coin or this is something where we'll just have to, like, wait and see which way it turns out. I think everyone here and beyond, we are active participants in this decision. I'm pretty convinced that the first line of reasoning is actually almost a self-fulfilling prophecy, that if we act with fear and if we say, ""Hey, we've just got to stop doing this stuff,"" what's really going to happen is the rule followers might pause, might slow down, but the rule breakers, as Alexandr [Wang] mentioned, the totalitarian governments, the criminal organizations, they're only going to accelerate. And that leads to what I am pretty convinced is the dystopian state, which is the good actors have worse AIs than the bad actors. But I'll also, you know, talk to the optimists a little bit. I don't think that means that, oh, yeah, then we should just relax and just hope for the best. That might not happen either. I think all of us together have to fight like hell to make sure that we put the guardrails, we put in -- when the problems arise -- reasonable regulations. But we fight like hell for the positive use cases. Because very close to my heart, and obviously there's many potential positive use cases, but perhaps the most powerful use case and perhaps the most poetic use case is if AI, artificial intelligence, can be used to enhance HI, human intelligence, human potential and human purpose. Thank you. (Applause)",PT15M36S,2023-05-01T14:59:55Z
Why AI is incredibly smart and shockingly stupid,"science,technology,innovation,future,AI,humanity,teaching,machine learning,ethics",Yejin Choi,"Computer scientist Yejin Choi is here to demystify the current state of massive artificial intelligence systems like ChatGPT, highlighting three key problems with cutting-edge large language models (including some funny instances of them failing at basic commonsense reasoning.) She welcomes us into a new era in which AI is becoming almost like a new intellectual species -- and identifies the benefits of building smaller AI systems trained on human norms and values. (Followed by a Q&A with head of TED Chris Anderson)","So I'm excited to share a few spicy thoughts on artificial intelligence. But first, let's get philosophical by starting with this quote by Voltaire, an 18th century Enlightenment philosopher, who said, ""Common sense is not so common."" Turns out this quote couldn't be more relevant to artificial intelligence today. Despite that, AI is an undeniably powerful tool, beating the world-class ""Go"" champion, acing college admission tests and even passing the bar exam. I’m a computer scientist of 20 years, and I work on artificial intelligence. I am here to demystify AI. So AI today is like a Goliath. It is literally very, very large. It is speculated that the recent ones are trained on tens of thousands of GPUs and a trillion words. Such extreme-scale AI models, often referred to as ""large language models,"" appear to demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong? So there are three immediate challenges we face already at the societal level. First, extreme-scale AI models are so expensive to train, and only a few tech companies can afford to do so. So we already see the concentration of power. But what's worse for AI safety, we are now at the mercy of those few tech companies because researchers in the larger community do not have the means to truly inspect and dissect these models. And let's not forget their massive carbon footprint and the environmental impact. And then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI? So I’m often asked these days whether it's even feasible to do any meaningful research without extreme-scale compute. And I work at a university and nonprofit research institute, so I cannot afford a massive GPU farm to create enormous language models. Nevertheless, I believe that there's so much we need to do and can do to make AI sustainable and humanistic. We need to make AI smaller, to democratize it. And we need to make AI safer by teaching human norms and values. Perhaps we can draw an analogy from ""David and Goliath,"" here, Goliath being the extreme-scale language models, and seek inspiration from an old-time classic, ""The Art of War,"" which tells us, in my interpretation, know your enemy, choose your battles, and innovate your weapons. Let's start with the first, know your enemy, which means we need to evaluate AI with scrutiny. AI is passing the bar exam. Does that mean that AI is robust at common sense? You might assume so, but you never know. So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense. (Laughter) Step one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now. (Laughter) OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly. OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid. (Laughter) It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI."" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense. So this observation leads us to the next wisdom, choose your battles. So what fundamental questions should we ask right now and tackle today in order to overcome this status quo with extreme-scale AI? I'll say common sense is among the top priorities. So common sense has been a long-standing challenge in AI. To explain why, let me draw an analogy to dark matter. So only five percent of the universe is normal matter that you can see and interact with, and the remaining 95 percent is dark matter and dark energy. Dark matter is completely invisible, but scientists speculate that it's there because it influences the visible world, even including the trajectory of light. So for language, the normal matter is the visible text, and the dark matter is the unspoken rules about how the world works, including naive physics and folk psychology, which influence the way people use and interpret language. So why is this common sense even important? Well, in a famous thought experiment proposed by Nick Bostrom, AI was asked to produce and maximize the paper clips. And that AI decided to kill humans to utilize them as additional resources, to turn you into paper clips. Because AI didn't have the basic human understanding about human values. Now, writing a better objective and equation that explicitly states: “Do not kill humans” will not work either because AI might go ahead and kill all the trees, thinking that's a perfectly OK thing to do. And in fact, there are endless other things that AI obviously shouldn’t do while maximizing paper clips, including: “Don’t spread the fake news,” “Don’t steal,” “Don’t lie,” which are all part of our common sense understanding about how the world works. However, the AI field for decades has considered common sense as a nearly impossible challenge. So much so that when my students and colleagues and I started working on it several years ago, we were very much discouraged. We’ve been told that it’s a research topic of ’70s and ’80s; shouldn’t work on it because it will never work; in fact, don't even say the word to be taken seriously. Now fast forward to this year, I’m hearing: “Don’t work on it because ChatGPT has almost solved it.” And: “Just scale things up and magic will arise, and nothing else matters.” So my position is that giving true common sense human-like robots common sense to AI, is still moonshot. And you don’t reach to the Moon by making the tallest building in the world one inch taller at a time. Extreme-scale AI models do acquire an ever-more increasing amount of commonsense knowledge, I'll give you that. But remember, they still stumble on such trivial problems that even children can do. So AI today is awfully inefficient. And what if there is an alternative path or path yet to be found? A path that can build on the advancements of the deep neural networks, but without going so extreme with the scale. So this leads us to our final wisdom: innovate your weapons. In the modern-day AI context, that means innovate your data and algorithms. OK, so there are, roughly speaking, three types of data that modern AI is trained on: raw web data, crafted examples custom developed for AI training, and then human judgments, also known as human feedback on AI performance. If the AI is only trained on the first type, raw web data, which is freely available, it's not good because this data is loaded with racism and sexism and misinformation. So no matter how much of it you use, garbage in and garbage out. So the newest, greatest AI systems are now powered with the second and third types of data that are crafted and judged by human workers. It's analogous to writing specialized textbooks for AI to study from and then hiring human tutors to give constant feedback to AI. These are proprietary data, by and large, speculated to cost tens of millions of dollars. We don't know what's in this, but it should be open and publicly available so that we can inspect and ensure [it supports] diverse norms and values. So for this reason, my teams at UW and AI2 have been working on commonsense knowledge graphs as well as moral norm repositories to teach AI basic commonsense norms and morals. Our data is fully open so that anybody can inspect the content and make corrections as needed because transparency is the key for such an important research topic. Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well. So as a quest toward more direct commonsense knowledge acquisition, my team has been investigating potential new algorithms, including symbolic knowledge distillation that can take a very large language model as shown here that I couldn't fit into the screen because it's too large, and crunch that down to much smaller commonsense models using deep neural networks. And in doing so, we also generate, algorithmically, human-inspectable, symbolic, commonsense knowledge representation, so that people can inspect and make corrections and even use it to train other neural commonsense models. More broadly, we have been tackling this seemingly impossible giant puzzle of common sense, ranging from physical, social and visual common sense to theory of minds, norms and morals. Each individual piece may seem quirky and incomplete, but when you step back, it's almost as if these pieces weave together into a tapestry that we call human experience and common sense. We're now entering a new era in which AI is almost like a new intellectual species with unique strengths and weaknesses compared to humans. In order to make this powerful AI sustainable and humanistic, we need to teach AI common sense, norms and values. Thank you. (Applause) Chris Anderson: Look at that. Yejin, please stay one sec. This is so interesting, this idea of common sense. We obviously all really want this from whatever's coming. But help me understand. Like, so we've had this model of a child learning. How does a child gain common sense apart from the accumulation of more input and some, you know, human feedback? What else is there? Yejin Choi: So fundamentally, there are several things missing, but one of them is, for example, the ability to make hypothesis and make experiments, interact with the world and develop this hypothesis. We abstract away the concepts about how the world works, and then that's how we truly learn, as opposed to today's language model. Some of them is really not there quite yet. CA: You use the analogy that we can’t get to the Moon by extending a building a foot at a time. But the experience that most of us have had of these language models is not a foot at a time. It's like, the sort of, breathtaking acceleration. Are you sure that given the pace at which those things are going, each next level seems to be bringing with it what feels kind of like wisdom and knowledge. YC: I totally agree that it's remarkable how much this scaling things up really enhances the performance across the board. So there's real learning happening due to the scale of the compute and data. However, there's a quality of learning that is still not quite there. And the thing is, we don't yet know whether we can fully get there or not just by scaling things up. And if we cannot, then there's this question of what else? And then even if we could, do we like this idea of having very, very extreme-scale AI models that only a few can create and own? CA: I mean, if OpenAI said, you know, ""We're interested in your work, we would like you to help improve our model,"" can you see any way of combining what you're doing with what they have built? YC: Certainly what I envision will need to build on the advancements of deep neural networks. And it might be that there’s some scale Goldilocks Zone, such that ... I'm not imagining that the smaller is the better either, by the way. It's likely that there's right amount of scale, but beyond that, the winning recipe might be something else. So some synthesis of ideas will be critical here. CA: Yejin Choi, thank you so much for your talk. (Applause)",PT16M02S,2023-04-28T14:42:44Z
TikTok's CEO on its future -- and what makes its algorithm different,"culture,technology,business,entrepreneur,entertainment,media,software,innovation,communication,product design,AI,social media,Internet,government,data,addiction,machine learning,UX design,online privacy",Shou Chew,"TikTok CEO Shou Chew dives into how the trend-setting video app and cultural phenomenon works — from what distinguishes its algorithm and drives virality to the challenges of content moderation and digital addiction. In a wide-ranging conversation with head of TED Chris Anderson, he tells stories about the TikTok creators he loves and digs into thorny issues like data privacy and government manipulation — as well as speaking personally about his commitment to inspiring creativity and building community.","Chris Anderson: It's very nice to have you here. Let's see. First of all, congratulations. You really pulled off something remarkable on that grilling, you achieved something that very few people do, which was, you pulled off a kind of, a bipartisan consensus in US politics. It was great. (Laughter) The bad news was that that consensus largely seemed to be: ""We must ban TikTok."" So we're going to come to that in a bit. And I'm curious, but before we go there, we need to know about you. You seem to me like a remarkable person. I want to know a bit of your story and how you came to TikTok in the first place. Shou Chew: Thank you, Chris. Before we do that, can I just check, need to know my audience, how many of you here use TikTok? Oh, thank you. For those who don’t, the Wi-Fi is free. (Laughter) CA: There’s another question, which is, how many of you here have had your lives touched through TikTok, through your kids and other people in your lives? SC: Oh, that's great to see. CA: It's basically, if you're alive, you have had some kind of contact with TikTok at this point. So tell us about you. SC: So my name is Shou, and I’m from Singapore. Roughly 10 years ago, I met with two engineers who were building a product. And the idea behind this was to build a product that recommended content to people not based on who they knew, which was, if you think about it, 10 years ago, the social graph was all in the rage. And the idea was, you know, your content and the feed that you saw should be based on people that you knew. But 10 years ago, these two engineers thought about something different, which is, instead of showing you -- instead of showing you people you knew, why don't we show you content that you liked? And that's sort of the genesis and the birth of the early iterations of TikTok. And about five years ago, with the advent of 4G, short video, mobile phone penetration, TikTok was born. And a couple of years ago, I had the opportunity to run this company, and it still excites me every single day. CA: So I want to dig in a little more into this, about what was it that made this take-off so explosive? Because the language I hear from people who spent time on it, it's sort of like I mean, it is a different level of addiction to other media out there. And I don't necessarily mean this in a good way, we'll be coming on to it. There’s good and bad things about this type of addiction. But it’s the feeling that within a couple of days of experience of TikTok, it knows you and it surprises you with things that you didn't know you were going to be interested in, but you are. How? Is it really just, instead of the social graph -- What are these algorithms doing? SC: I think to describe this, to begin to answer your question, we have to talk about the mission of the company. Now the mission is to inspire creativity and to bring joy. And I think missions for companies like ours [are] really important. Because you have product managers working on the product every single day, and they need to have a North Star, you know, something to sort of, work towards together. Now, based on this mission, our vision is to provide three things to our users. We want to provide a window to discover, and I’ll talk about discovery, you talked about this, in a second. We want to give them a canvas to create, which is going to be really exciting with new technologies in AI that are going to help people create new things. And the final thing is bridges for people to connect. So that's sort of the vision of what we're trying to build. Now what really makes TikTok very unique and very different is the whole discovery engine behind it. So there are earlier apps that I have a lot of respect for, but they were built for a different purpose. For example, in the era of search, you know, there was an app that was built for people who wanted to search things so that is more easily found. And then in the era of social graphs, it was about connecting people and their followers. Now what we have done is that ... based on our machine-learning algorithms, we're showing people what they liked. And what this means is that we have given the everyday person a platform to be discovered. If you have talent, it is very, very easy to get discovered on TikTok. And I'll just give you one example of this. The biggest creator on TikTok is a guy called Khaby. Khaby was from Senegal, he lives in Italy, he was a factory worker. He, for the longest time, didn't even speak in any of his videos. But what he did was he had talent. He was funny, he had a good expression, he had creativity, so he kept posting. And today he has 160 million followers on our platform. So every single day we hear stories like that, businesses, people with talent. And I think it's very freeing to have a platform where, as long as you have talent, you're going to be heard and you have the chance to succeed. And that's what we're providing to our users. CA: So this is the amazing thing to me. Like, most of us have grown up with, say, network television, where, for decades you've had thousands of brilliant, creative people toiling in the trenches, trying to imagine stuff that will be amazing for an audience. And none of them ever remotely came up with anything that looked like many of your creators. So these algorithms, just by observing people's behavior and what they look like, have discovered things that thousands of brilliant humans never discovered. Tell me some of the things that it is looking at. So obvious things, like if someone presses like or stays on a video for a long time, that gives you a clue, ""more like that."" But is it subject matter? What are the array of things that you have noticed that you can actually track that provide useful clues? SC: I'm going to simplify this a lot, but the machine learning, the recommendation algorithm is really just math. So, for example, if you liked videos one, two, three and four, and I like videos one, two, three and five, maybe he liked videos one, two, three and six. Now what's going to happen is, because we like one, two, three at the same time, he's going to be shown four, five, six, and so are we. And you can think about this repeated at scale in real time across more than a billion people. That's basically what it is, it's math. And of course, you know, AI and machine learning has allowed this to be done at a very, very big scale. And what we have seen, the result of this, is that it learns the interest signals that people exhibit very quickly and shows you content that's really relevant for you in a very quick way. CA: So it's a form of collaborative filtering, from what you're saying. The theory behind it is that these humans are weird people, we don't really know what they're interested in, but if we see that one human is interested, with an overlap of someone else, chances are, you know, you could make use of the other pieces that are in that overlapped human's repertoire to feed them, and they'll be surprised. But the reason they like it is because their pal also liked it. SC: It's pattern recognition based on your interest signals. And I think the other thing here is that we don't actually ask you 20 questions on whether you like a piece of content, you know, what are your interests, we don't do that. We built that experience organically into the app experience. So you are voting with your thumbs by watching a video, by swiping it, by liking it, by sharing it, you are basically exhibiting interest signals. And what it does mathematically is to take those signals, put it in a formula and then matches it through pattern recognition. That's basically the idea behind it. CA: I mean, lots of start-ups have tried to use these types of techniques. I'm wondering what else played a role early on? I mean, how big a deal was it, that from the get-go you were optimizing for smartphones so that videos were shot in portrait format and they were short. Was that an early distinguishing thing that mattered? SC: I think we were the first to really try this at scale. You know, the recommendation algorithm is a very important reason as to why the platform is so popular among so many people. But beyond that, you know, you mentioned the format itself. So we talked about the vision of the company, which is to have a window to discover. And if you just open the app for the first time, you'll see that it takes up your whole screen. So that's the window that we want. You can imagine a lot of people using that window to discover new things in their lives. Then, you know, through this recommendation algorithm, we have found that it connects people together. People find communities, and I've heard so many stories of people who have found their communities because of the content that they're posting. Now, I'll give you an example. I was in DC recently, and I met with a bunch of creators. CA: I heard. (Laughter) SC: One of them was sitting next to me at a dinner, his name is Samuel. He runs a restaurant in Phoenix, Arizona, and it's a taco restaurant. He told me he has never done this before, first venture. He started posting all this content on TikTok, and I saw his content, I was hungry after looking at it, it's great content. And he's generated so much interest in his business, that last year he made something like a million dollars in revenue just via TikTok. One restaurant. And again and again, I hear these stories, you know, by connecting people together, by giving people the window to discover, we have given many small businesses and many people, your common person, a voice that they will never otherwise have. And I think that's the power of the platform. CA: So you definitely have identified early just how we're social creatures, we need affirmation. I've heard a story, and you can tell me whether true or not, that one of the keys to your early liftoff was that you wanted to persuade creators who were trying out TikTok that this was a platform where they would get response, early on, when you're trying to grow something, the numbers aren't there for response. So you had the brilliant idea of goosing those numbers a bit, basically finding ways to give people, you know, a bigger sense of like, more likes, more engagement than was actually the case, by using AI agents somehow in the process. Is that a brilliant idea, or is that just a myth? SC: I would describe it in a different way. So there are other platforms that exist before TikTok. And if you think about those platforms, you sort of have to be famous already in order to get followers. Because the way it’s built is that people come and follow people. And if you aren't already famous, the chances that you get discovered are very, very low. Now, what we have done, again, because of the difference in the way we're recommending content, is that we have given anyone, any single person with enough talent a stage to be able to be discovered. And I think that actually is the single, probably the most important thing contributing to the growth of the platform. And again and again, you will hear stories from people who use the platform, who post regularly on it, that if they have something they want to say, the platform gives them the chance and the stage to connect with their audience in a way that I think no other product in the past has ever offered them. CA: So I'm just trying to play back what you said there. You said you were describing a different way what I said. Is it then the case that like, to give someone a decent chance, someone who's brilliant but doesn't come with any followers initially, that you've got some technique to identify talent and that you will almost encourage them, you will give them some kind of, you know, artificially increase the number of followers or likes or whatever that they have, so that others are encouraged to go, ""Wow, there's something there."" Like it's this idea of critical mass that kind of, every entrepreneur, every party planner kind of knows about of ""No, no, this is the hot place in town, everyone come,"" and that that is how you actually gain critical mass? SC: We want to make sure that every person who posts a video is given an equal chance to be able to have some audience to begin with. But this idea that you are maybe alluding to, that we can get people to like something, it doesn't really work like that. CA: Could you get AI agents to like something? Could you seed the network with extra AI agents that could kind of, you know, give someone early encouragement? SC: Ultimately, what the machine does is it recognizes people's interests. So if you post something that's not interesting to a lot of people, even if you gave it a lot of exposure, you're not going to get the virality that you want. So it's a lot of ... There is no push here. It's not like you can go and push something, because I like Chris, I'm going to push your content, it doesn't work like that. You've got to have a message that resonates with people, and if it does, then it will automatically just have the virality itself. That's the beauty of user-generated content. It's not something that can be engineered or over-thought. It really is something that has to resonate with the audience. And if it does, then it goes viral. CA: Speaking privately with an investor who knows your company quite well, who said that actually the level of sophistication of the algorithms you have going is just another order of magnitude to what competitors like, you know, Facebook or YouTube have going. Is that just hype or do you really believe you -- like, how complex are these algorithms? SC: Well, I think in terms of complexity, there are many companies who have a lot of resources and a lot of talent. They will figure out even the most complex algorithms. I think what is very different is your mission of your company, how you started the company. Like I said, you know, we started with this idea that this was the main use case. The most important use case is you come and you get to see recommended content. Now for some other apps out there, they are very significant and have a lot of users, they are built for a different original purpose. And if you are built for something different, then your users are used to that because the community comes in and they expect that sort of experience. So I think the pivot away from that is not really just a matter of engineering and algorithms, it’s a matter of what your company is built to begin with. Which is why I started this by saying you need to have a vision, you need to have a mission, and that's the North Star. You can't just shift it halfway. CA: Right. And is it fair to say that because your start point has been interest algorithms rather than social graph algorithms, you've been able to avoid some of the worst of the sort of, the filter bubbles that have happened in other social media where you have tribes kind of declaring war on each other effectively. And so much of the noise and energy is around that. Do you believe that you've largely avoided that on TikTok? SC: The diversity of content that our users see is very key. You know, in order for the discovery -- the mission is to discover -- sorry, the vision is to discover. So in order to facilitate that, it is very important to us that what the users see is a diversity of content. Now, generally speaking, you know, there are certain issues that you mentioned that the industry faces, you know. There are some bad actors who come on the internet, they post bad content. Now our approach is that we have very clear community guidelines. We're very transparent about what is allowed and what is not allowed on our platform. No executives make any ad hoc decisions. And based on that, we have built a team that is tens of thousands of people plus machines in order to identify content that is bad and actively and proactively remove it from the platform. CA: Talk about what some of those key guidelines are. SC: We have it published on our website. In March, we just iterated a new version to make it more readable. So there are many things like, for example, no pornography, clearly no child sexual abuse material and other bad things, no violence, for example. We also make it clear that it's a differentiated experience if you're below 18 years old. So if you're below 18 years old, for example, your entire app experience is actually more restricted. We don't allow, as an example, users below 16, by default, to go viral. We don't allow that. If you're below 16, we don’t allow you to use the instant messaging feature in app. If you’re below 18, we don’t allow you to use the livestreaming features. And of course, we give parents a whole set of tools to control their teenagers’ experience as well. CA: How do you know the age of your users? SC: In our industry, we rely mainly on something called age gating, which is when you sign up for the app for the first time and we ask you for the age. Now, beyond that, we also have built tools to go through your public profile for example, when you post a video, we try to match the age that you said with the video that you just posted. Now, there are questions of can we do more? And the question always has, for every company, by the way, in our industry, has to be balanced with privacy. Now, if, for example, we scan the faces of every single user, then we will significantly increase the ability to tell their age. But we will also significantly increase the amount of data that we collect on you. Now, we don't want to collect data. We don't want to scan data on your face to collect that. So that balance has to be maintained, and it's a challenge that we are working through together with industry, together with the regulators as well. CA: So look, one thing that is unquestionable is that you have created a platform for literally millions of people who never thought they were going to be a content creator. You've given them an audience. I'd actually like to hear from you one other favorite example of someone who TikTok has given an audience to that never had that before. SC: So when again, when I travel around the world, I meet with a whole bunch of creators on our platform. I was in South Korea just yesterday, and before that I met with -- yes, before that I met with a bunch of -- People don't expect, for example, teachers. There is an English teacher from Arkansas. Her name is Claudine, and I met her in person. She uses our platform to reach out to students. There is another teacher called Chemical Kim. And Chemical Kim teaches chemistry. What she does is she uses our platform to reach out to a much broader student base than she has in her classroom. And they're both very, very popular. You know, in fact, what we have realized is that STEM content has over 116 billion views on our platform globally. And it's so significant -- CA: In a year? SC: Cumulatively. CA: [116] billion. SC: It's so significant, that in the US we have started testing, creating a feed just for STEM content. Just for STEM content. I’ve been using it for a while, and I learned something new. You want to know what it is? Apparently if you flip an egg on your tray, the egg will last longer. It's science, there’s a whole video on this, I learned this on TikTok. You can search for this. CA: You want to know something else about an egg? If you put it in just one hand and squeeze it as hard as you can, it will never break. SC: Yes, I think I read about that, too. CA: It's not true. (Laughter) SC: We can search for it. CA: But look, here's here's the flip side to all this amazingness. And honestly, this is the key thing, that I want to have an honest, heart-to-heart conversation with you because it's such an important issue, this question of human addiction. You know, we are ... animals with a prefrontal cortex. That's how I think of us. We have these addictive instincts that go back millions of years, and we often are in the mode of trying to modulate our own behavior. It turns out that the internet is incredibly good at activating our animal cells and getting them so damn excited. And your company, the company you've built, is better at it than any other company on the planet, I think. So what are the risks of this? I mean, how ... From a company point of view, for example, it's in your interest to have people on there as long as possible. So some would say, as a first pass, you want people to be addicted as long as possible. That's how advertising money will flow and so forth, and that's how your creators will be delighted. What is too much? SC: I don't actually agree with that. You know, as a company, our goal is not to optimize and maximize time spent. It is not. In fact, in order to address people spending too much time on our platform, we have done a number of things. I was just speaking with some of your colleagues backstage. One of them told me she has encountered this as well. If you spend too much time on our platform, we will proactively send you videos to tell you to get off the platform. We will. And depending on the time of the day, if it's late at night, it will come sooner. We have also built in tools to limit, if you below 18 years old, by default, we set a 60-minute default time limit. CA: How many? SC: Sixty minutes. And we've given parents tools and yourself tools, if you go to settings, you can set your own time limit. We've given parents tools so that you can pair, for the parents who don't know this, go to settings, family pairing, you can pair your phone with your teenager's phone and set the time limit. And we really encourage parents to have these conversations with their teenagers on what is the right amount of screen time. I think there’s a healthy relationship that you should have with your screen, and as a business, we believe that that balance needs to be met. So it's not true that we just want to maximize time spent. CA: If you were advising parents here what time they should actually recommend to their teenagers, what do you think is the right setting? SC: Well, 60 minutes, we did not come up with it ourselves. So I went to the Digital Wellness Lab at the Boston Children's Hospital, and we had this conversation with them. And 60 minutes was the recommendation that they gave to us, which is why we built this into the app. So 60 minutes, take it for what it is, it’s something that we’ve had some discussions of experts. But I think for all parents here, it is very important to have these conversations with your teenage children and help them develop a healthy relationship with screens. I think we live in an age where it's completely inevitable that we're going to interact with screens and digital content, but I think we should develop healthy habits early on in life, and that's something I would encourage. CA: Curious to ask the audience, which of you who have ever had that video on TikTok appear saying, “Come off.” OK, I mean ... So maybe a third of the audience seem to be active TikTok users, and about 20 people maybe put their hands up there. Are you sure that -- like, it feels to me like this is a great thing to have, but are you ... isn't there always going to be a temptation in any given quarter or whatever, to just push it a bit at the boundary and just dial back a bit on that so that you can hit revenue goals, etc? Are you saying that this is used scrupulously? SC: I think, you know, in terms ... Even if you think about it from a commercial point of view, it is always best when your customers have a very healthy relationship with your product. It's always best when it's healthy. So if you think about very short-term retention, maybe, but that's not the way we think about it. If you think about it from a longer-term perspective, what you really want to have is a healthy relationship, you know. You don’t want people to develop very unhealthy habits, and then at some point they're going to drop it. So I think everything in moderation. CA: There's a claim out there that in China, there's a much more rigorous standards imposed on the amount of time that children, especially, can spend on the TikTok equivalent of that. SC: That is unfortunately a misconception. So that experience that is being mentioned for Douyin, which is a different app, is for an under 14-year-old experience. Now, if you compare that in the United States, we have an under-13 experience in the US. It's only available in the US, it's not available here in Canada, in Canada, we just don't allow it. If you look at the under-13 experience in the US, it's much more restricted than the under-14 experience in China. It's so restrictive, that every single piece of content is vetted by our third-party child safety expert. And we don't allow any under-13s in the US to publish, we don’t allow them to post, and we don't allow them to use a lot of features. So I think that that report, I've seen that report too, it's not doing a fair comparison. CA: What do you make of this issue? You know, you've got these millions of content creators and all of them, in a sense, are in a race for attention, and that race can pull them in certain directions. So, for example, teenage girls on TikTok, sometimes people worry that, to win attention, they've discovered that by being more sexual that they can gain extra viewers. Is this a concern? Is there anything you can do about this? SC: We address this in our community guidelines as well. You know, if you look at sort of the sexualized content on our guidelines, if you’re below a certain age, you know, for certain themes that are mature, we actually remove that from your experience. Again, I come back to this, you know, we want to have a safe platform. In fact, at my congressional hearing, I made four commitments to our users and to the politicians in the US. And the first one is that we take safety, especially for teenagers, extremely seriously, and we will continue to prioritize that. You know, I believe that we need to give our teenage users, and our users in general, a very safe experience, because if we don't do that, then we cannot fulfill -- the mission is to inspire creativity and to bring joy. If they don't feel safe, I cannot fulfill my mission. So it's all very organic to me as a business to make sure I do that. CA: But in the strange interacting world of human psychology and so forth, weird memes can take off. I mean, you had this outbreak a couple years back with these devious licks where kids were competing with each other to do vandalism in schools and, you know, get lots of followers from it. How on Earth do you battle something like that? SC: So dangerous challenges are not allowed on our platform. If you look at our guidelines, it's violative. We proactively invest resources to identify them and remove them from our platform. In fact, if you search for dangerous challenges on our platform today, we will redirect you to a safety resource page. And we actually worked with some creators as well to come up with campaigns. This is another campaign. It's the ""Stop, Think, Decide Before You Act"" campaign where we work with the creators to produce videos, to explain to people that some things are dangerous, please don't do it. And we post these videos actively on our platform as well. CA: That's cool. And you've got lots of employees. I mean, how many employees do you have who are specifically looking at these content moderation things, or is that the wrong question? Are they mostly identified by AI initially and then you have a group who are overseeing and making the final decision? SC: The group is based in Ireland and it's a lot of people, it's tens of thousands of people. CA: Tens of thousands? SC: It's one of the most important cost items on my PnL, and I think it's completely worth it. Now, most of the moderation has to be done by machines. The machines are good, they're quite good, but they're not as good as, you know, they're not perfect at this point. So you have to complement them with a lot of human beings today. And I think, by the way, a lot of the progress in AI in general is making that kind of content moderation capabilities a lot better. So we're going to get more precise. You know, we’re going to get more specific. And it’s going to be able to handle larger scale. And that's something I think that I'm personally looking forward to. CA: What about this perceived huge downside of use of, certainly Instagram, I think TikTok as well. What people worry that you are amplifying insecurities, especially of teenagers and perhaps especially of teenage girls. They see these amazing people on there doing amazing things, they feel inadequate, there's all these reported cases of depression, insecurity, suicide and so forth. SC: I take this extremely seriously. So in our guidelines, for certain themes that we think are mature and not suitable for teenagers, we actually proactively remove it from their experience. At the same time, if you search certain terms, we will make sure that you get redirected to a resource safety page. Now we are always working with experts to understand some of these new trends that could emerge and proactively try to manage them, if that makes sense. Now, this is a problem that predates us, that predates TikTok. It actually predates the internet. But it's our responsibility to make sure that we invest enough to understand and to address the concerns, to keep the experience as safe as possible for as many people as possible. CA: Now, in Congress, the main concern seemed to be not so much what we've talked about, but data, the data of users, the fact that you're owned by ByteDance, Chinese company, and the concern that at any moment Chinese government might require or ask for data. And in fact, there have been instances where, I think you've confirmed, that some data of journalists on the platform was made available to ByteDance's engineers and from there, who knows what. Now, your response to this was to have this Project Texas, where you're moving data to be controlled by Oracle here in the US. Can you talk about that project and why, if you believe it so, why we should not worry so much about this issue? SC: I will say a couple of things about this, if you don't mind. The first thing I would say is that the internet is built on global interoperability, and we are not the only company that relies on the global talent pool to make our products as good as possible. Technology is a very collaborative effort. I think many people here would say the same thing. So we are not the first company to have engineers in all countries, including in China. We're not the first one. Now, I understand some of these concerns. You know, the data access by employees is not data accessed by government. This is very different, and there’s a clear difference in this. But we hear the concerns that are raised in the United States. We did not try to avoid discussing. We did not try to argue our way out of it. What we did was we built an unprecedented project where we localize American data to be stored on American soil by an American company overseen by American personnel. So this kind of protection for American data is beyond what any other company in our industry has ever done. Well, money is not the only issue here, but it's very expensive to build something like that. And more importantly, you know, we are basically localizing data in a way that no other company has done. So we need to be very careful that whilst we are pursuing what we call digital sovereignty in the US and we are also doing a version of this in Europe, that we don't balkanize the internet. Now we are the first to do it. And I expect that, you know, other companies are probably looking at this and trying to figure out how you balance between protecting, protected data, you know, to make sure that everybody feels secure about it while at the same time allowing for interoperability to continue to happen, because that's what makes technology and the internet so great. So that's something that we are doing. CA: How far are you along that journey with Project Texas? SC: We are very, very far along today. CA: When will there be a clear you know, here it is, it’s done, it’s firewalled, this data is protected? SC: Today, by default, all new US data is already stored in the Oracle cloud infrastructure. So it's in this protected US environment that we talked about in the United States. We still have some legacy data to delete in our own servers in Virginia and in Singapore. Our data has never been stored in China, by the way. That deletion is a very big engineering effort. So as we said, as I said at the hearing, it's going to take us a while to delete them, but I expect it to be done this year. CA: How much power do you have over your own ability to control certain things? So, for example, suppose that, for whatever reason, the Chinese government was to look at an upcoming US election and say, ""You know what, we would like this party to win,"" let's say, or ""We would like civil war to break out"" or whatever. How ... ""And we could do this by amplifying the content of certain troublemaking, disturbing people, causing uncertainty, spreading misinformation,"" etc. If you were required via ByteDance to do this, like, first of all, is there a pathway where theoretically that is possible? What's your personal line in the sand on this? SC: So during the congressional hearing, I made four commitments, we talked about the first one, which is safety. The third one is to keep TikTok a place of freedom of expression. By the way, if you go on TikTok today, you can search for anything you want, as long as it doesn't violate our community guidelines. And to keep it free from any government manipulation. And the fourth one is transparency and third-party monitoring. So the way we are trying to address this concern is an unprecedented amount of transparency. What do I mean by this? We're actually allowing third-party reviewers to come in and review our source code. I don't know any other company that does this, by the way. Because everything, as you know, is driven by code. So to allow someone else to review the source code is to give this a significant amount of transparency to ensure that the scenarios that you described that are highly hypothetical, cannot happen on our platform. Now, at the same time, we are releasing more research tools for researchers so that they can study the output. So the source code is the input. We are also allowing researchers to study the output, which is the content on our platform. I think the easiest way to sort of fend this off is transparency. You know, we give people access to monitor us, and we just make it very, very transparent. And that's our approach to the problem. CA: So you will say directly to this group that the scenario I talked about, of theoretical Chinese government interference in an American election, you can say that will not happen? SC: I can say that we are building all the tools to prevent any of these actions from happening. And I'm very confident that with an unprecedented amount of transparency that we're giving on the platform, we can reduce this risk to as low as zero as possible. CA: To as low as zero as possible. SC: To as close to zero as possible. CA: As close to zero as possible. That's fairly reassuring. Fairly. (Laughter) I mean, how would the world know? If you discovered this or you thought you had to do it, is this a line in the sand for you? Like, are you in a situation you would not let the company that you know now and that you are running do this? SC: Absolutely. That's the reason why we're letting third parties monitor, because if they find out, you know, they will disclose this. We also have transparency reports, by the way, where we talk about a whole bunch of things, the content that we remove, you know, that violates our guidelines, government requests. You know, it's all published online. All you have to do is search for it. CA: So you're super compelling and likable as a CEO, I have to say. And I would like to, as we wrap this up, I'd like to give you a chance just to paint, like, what's the vision? As you look at what TikTok could be, let's move the clock out, say, five years from now. How should we think about your contribution to our collective future? SC: I think it's still down to the vision that we have. So in terms of the window of discovery, I think there's a huge benefit to the world when people can discover new things. You know, people think that TikTok is all about dancing and singing, and there’s nothing wrong with that, because it’s super fun. There's still a lot of that, but we're seeing science content, STEM content, have you about BookTok? It's a viral trend that talks about books and encourages people to read. That BookTok has 120 billion views globally, 120 billion. CA: Billion, with a B. SC: People are learning how to cook, people are learning about science, people are learning how to golf -- well, people are watching videos on golfing, I guess. (Laughter) I haven't gotten better by looking at the videos. I think there's a huge, huge opportunity here on discovery and giving the everyday person a voice. If you talk to our creators, you know, a lot of people will tell you this again and again, that before TikTok, they would never have been discovered. And we have given them the platform to do that. And it's important to maintain that. Then we talk about creation. You know, there’s all this new technology coming in with AI-generated content that will help people create even more creative content. I think there's going to be a collaboration between, and I think there's a speaker who is going to talk about this, between people and AI where they can unleash their creativity in a different way. You know, like for example, I'm terrible at drawing personally, but if I had some AI to help me, then maybe I can express myself even better. Then we talk about bridges to connect and connecting people and the communities together. This could be products, this could be commerce, five million businesses in the US benefit from TikTok today. I think we can get that number to a much higher number. And of course, if you look around the world, including in Canada, that number is going to be massive. So I think these are the biggest opportunities that we have, and it's really very exciting. CA: So courtesy of your experience in Congress, you actually became a bit of a TikTok star yourself, I think. Some of your videos have gone viral. You've got your phone with you. Do you want to make a little little TikTok video right now? Let's do this. SC: If you don't mind ... CA: What do you think, should we do this? SC: We're just going to do a selfie together, how's that? So why don't we just say ""Hi."" Hi! Audience: Hi! CA: Hello from TED. SC: All right, thank you, I hope it goes viral. (Laughter) CA: If that one goes viral, I think I've given up on your algorithm, actually. (Laughter) Shou Chew, you're one of the most influential and powerful people in the world, whether you know it or not. And I really appreciate you coming and sharing your vision. I really, really hope the upside of what you're talking about comes about. Thank you so much for coming today. SC: Thank you, Chris. CA: It's really interesting. (Applause)",PT39M22S,2023-04-21T16:01:08Z
The inside story of ChatGPT's astonishing potential,"technology,computers,innovation,creativity,future,language,AI,machine learning",Greg Brockman,"In a talk from the cutting edge of technology, OpenAI cofounder Greg Brockman explores the underlying design principles of ChatGPT and demos some mind-blowing, unreleased plug-ins for the chatbot that sent shockwaves across the world. After the talk, head of TED Chris Anderson joins Brockman to dig into the timeline of ChatGPT's development and get Brockman's take on the risks, raised by many in the tech industry and beyond, of releasing such a powerful tool into the world.","We started OpenAI seven years ago because we felt like something really interesting was happening in AI and we wanted to help steer it in a positive direction. It's honestly just really amazing to see how far this whole field has come since then. And it's really gratifying to hear from people like Raymond who are using the technology we are building, and others, for so many wonderful things. We hear from people who are excited, we hear from people who are concerned, we hear from people who feel both those emotions at once. And honestly, that's how we feel. Above all, it feels like we're entering an historic period right now where we as a world are going to define a technology that will be so important for our society going forward. And I believe that we can manage this for good. So today, I want to show you the current state of that technology and some of the underlying design principles that we hold dear. So the first thing I'm going to show you is what it's like to build a tool for an AI rather than building it for a human. So we have a new DALL-E model, which generates images, and we are exposing it as an app for ChatGPT to use on your behalf. And you can do things like ask, you know, suggest a nice post-TED meal and draw a picture of it. (Laughter) Now you get all of the, sort of, ideation and creative back-and-forth and taking care of the details for you that you get out of ChatGPT. And here we go, it's not just the idea for the meal, but a very, very detailed spread. So let's see what we're going to get. But ChatGPT doesn't just generate images in this case -- sorry, it doesn't generate text, it also generates an image. And that is something that really expands the power of what it can do on your behalf in terms of carrying out your intent. And I'll point out, this is all a live demo. This is all generated by the AI as we speak. So I actually don't even know what we're going to see. This looks wonderful. (Applause) I'm getting hungry just looking at it. Now we've extended ChatGPT with other tools too, for example, memory. You can say ""save this for later."" And the interesting thing about these tools is they're very inspectable. So you get this little pop up here that says ""use the DALL-E app."" And by the way, this is coming to you, all ChatGPT users, over upcoming months. And you can look under the hood and see that what it actually did was write a prompt just like a human could. And so you sort of have this ability to inspect how the machine is using these tools, which allows us to provide feedback to them. Now it's saved for later, and let me show you what it's like to use that information and to integrate with other applications too. You can say, “Now make a shopping list for the tasty thing I was suggesting earlier.” And make it a little tricky for the AI. ""And tweet it out for all the TED viewers out there."" (Laughter) So if you do make this wonderful, wonderful meal, I definitely want to know how it tastes. But you can see that ChatGPT is selecting all these different tools without me having to tell it explicitly which ones to use in any situation. And this, I think, shows a new way of thinking about the user interface. Like, we are so used to thinking of, well, we have these apps, we click between them, we copy/paste between them, and usually it's a great experience within an app as long as you kind of know the menus and know all the options. Yes, I would like you to. Yes, please. Always good to be polite. (Laughter) And by having this unified language interface on top of tools, the AI is able to sort of take away all those details from you. So you don't have to be the one who spells out every single sort of little piece of what's supposed to happen. And as I said, this is a live demo, so sometimes the unexpected will happen to us. But let's take a look at the Instacart shopping list while we're at it. And you can see we sent a list of ingredients to Instacart. Here's everything you need. And the thing that's really interesting is that the traditional UI is still very valuable, right? If you look at this, you still can click through it and sort of modify the actual quantities. And that's something that I think shows that they're not going away, traditional UIs. It's just we have a new, augmented way to build them. And now we have a tweet that's been drafted for our review, which is also a very important thing. We can click “run,” and there we are, we’re the manager, we’re able to inspect, we're able to change the work of the AI if we want to. And so after this talk, you will be able to access this yourself. And there we go. Cool. Thank you, everyone. (Applause) So we’ll cut back to the slides. Now, the important thing about how we build this, it's not just about building these tools. It's about teaching the AI how to use them. Like, what do we even want it to do when we ask these very high-level questions? And to do this, we use an old idea. If you go back to Alan Turing's 1950 paper on the Turing test, he says, you'll never program an answer to this. Instead, you can learn it. You could build a machine, like a human child, and then teach it through feedback. Have a human teacher who provides rewards and punishments as it tries things out and does things that are either good or bad. And this is exactly how we train ChatGPT. It's a two-step process. First, we produce what Turing would have called a child machine through an unsupervised learning process. We just show it the whole world, the whole internet and say, “Predict what comes next in text you’ve never seen before.” And this process imbues it with all sorts of wonderful skills. For example, if you're shown a math problem, the only way to actually complete that math problem, to say what comes next, that green nine up there, is to actually solve the math problem. But we actually have to do a second step, too, which is to teach the AI what to do with those skills. And for this, we provide feedback. We have the AI try out multiple things, give us multiple suggestions, and then a human rates them, says “This one’s better than that one.” And this reinforces not just the specific thing that the AI said, but very importantly, the whole process that the AI used to produce that answer. And this allows it to generalize. It allows it to teach, to sort of infer your intent and apply it in scenarios that it hasn't seen before, that it hasn't received feedback. Now, sometimes the things we have to teach the AI are not what you'd expect. For example, when we first showed GPT-4 to Khan Academy, they said, ""Wow, this is so great, We're going to be able to teach students wonderful things. Only one problem, it doesn't double-check students' math. If there's some bad math in there, it will happily pretend that one plus one equals three and run with it."" So we had to collect some feedback data. Sal Khan himself was very kind and offered 20 hours of his own time to provide feedback to the machine alongside our team. And over the course of a couple of months we were able to teach the AI that, ""Hey, you really should push back on humans in this specific kind of scenario."" And we've actually made lots and lots of improvements to the models this way. And when you push that thumbs down in ChatGPT, that actually is kind of like sending up a bat signal to our team to say, “Here’s an area of weakness where you should gather feedback.” And so when you do that, that's one way that we really listen to our users and make sure we're building something that's more useful for everyone. Now, providing high-quality feedback is a hard thing. If you think about asking a kid to clean their room, if all you're doing is inspecting the floor, you don't know if you're just teaching them to stuff all the toys in the closet. This is a nice DALL-E-generated image, by the way. And the same sort of reasoning applies to AI. As we move to harder tasks, we will have to scale our ability to provide high-quality feedback. But for this, the AI itself is happy to help. It's happy to help us provide even better feedback and to scale our ability to supervise the machine as time goes on. And let me show you what I mean. For example, you can ask GPT-4 a question like this, of how much time passed between these two foundational blogs on unsupervised learning and learning from human feedback. And the model says two months passed. But is it true? Like, these models are not 100-percent reliable, although they’re getting better every time we provide some feedback. But we can actually use the AI to fact-check. And it can actually check its own work. You can say, fact-check this for me. Now, in this case, I've actually given the AI a new tool. This one is a browsing tool where the model can issue search queries and click into web pages. And it actually writes out its whole chain of thought as it does it. It says, I’m just going to search for this and it actually does the search. It then it finds the publication date and the search results. It then is issuing another search query. It's going to click into the blog post. And all of this you could do, but it’s a very tedious task. It's not a thing that humans really want to do. It's much more fun to be in the driver's seat, to be in this manager's position where you can, if you want, triple-check the work. And out come citations so you can actually go and very easily verify any piece of this whole chain of reasoning. And it actually turns out two months was wrong. Two months and one week, that was correct. (Applause) And we'll cut back to the side. And so thing that's so interesting to me about this whole process is that it’s this many-step collaboration between a human and an AI. Because a human, using this fact-checking tool is doing it in order to produce data for another AI to become more useful to a human. And I think this really shows the shape of something that we should expect to be much more common in the future, where we have humans and machines kind of very carefully and delicately designed in how they fit into a problem and how we want to solve that problem. We make sure that the humans are providing the management, the oversight, the feedback, and the machines are operating in a way that's inspectable and trustworthy. And together we're able to actually create even more trustworthy machines. And I think that over time, if we get this process right, we will be able to solve impossible problems. And to give you a sense of just how impossible I'm talking, I think we're going to be able to rethink almost every aspect of how we interact with computers. For example, think about spreadsheets. They've been around in some form since, we'll say, 40 years ago with VisiCalc. I don't think they've really changed that much in that time. And here is a specific spreadsheet of all the AI papers on the arXiv for the past 30 years. There's about 167,000 of them. And you can see there the data right here. But let me show you the ChatGPT take on how to analyze a data set like this. So we can give ChatGPT access to yet another tool, this one a Python interpreter, so it’s able to run code, just like a data scientist would. And so you can just literally upload a file and ask questions about it. And very helpfully, you know, it knows the name of the file and it's like, ""Oh, this is CSV,"" comma-separated value file, ""I'll parse it for you."" The only information here is the name of the file, the column names like you saw and then the actual data. And from that it's able to infer what these columns actually mean. Like, that semantic information wasn't in there. It has to sort of, put together its world knowledge of knowing that, “Oh yeah, arXiv is a site that people submit papers and therefore that's what these things are and that these are integer values and so therefore it's a number of authors in the paper,"" like all of that, that’s work for a human to do, and the AI is happy to help with it. Now I don't even know what I want to ask. So fortunately, you can ask the machine, ""Can you make some exploratory graphs?"" And once again, this is a super high-level instruction with lots of intent behind it. But I don't even know what I want. And the AI kind of has to infer what I might be interested in. And so it comes up with some good ideas, I think. So a histogram of the number of authors per paper, time series of papers per year, word cloud of the paper titles. All of that, I think, will be pretty interesting to see. And the great thing is, it can actually do it. Here we go, a nice bell curve. You see that three is kind of the most common. It's going to then make this nice plot of the papers per year. Something crazy is happening in 2023, though. Looks like we were on an exponential and it dropped off the cliff. What could be going on there? By the way, all this is Python code, you can inspect. And then we'll see word cloud. So you can see all these wonderful things that appear in these titles. But I'm pretty unhappy about this 2023 thing. It makes this year look really bad. Of course, the problem is that the year is not over. So I'm going to push back on the machine. [Waitttt that's not fair!!! 2023 isn't over. What percentage of papers in 2022 were even posted by April 13?] So April 13 was the cut-off date I believe. Can you use that to make a fair projection? So we'll see, this is the kind of ambitious one. (Laughter) So you know, again, I feel like there was more I wanted out of the machine here. I really wanted it to notice this thing, maybe it's a little bit of an overreach for it to have sort of, inferred magically that this is what I wanted. But I inject my intent, I provide this additional piece of, you know, guidance. And under the hood, the AI is just writing code again, so if you want to inspect what it's doing, it's very possible. And now, it does the correct projection. (Applause) If you noticed, it even updates the title. I didn't ask for that, but it know what I want. Now we'll cut back to the slide again. This slide shows a parable of how I think we ... A vision of how we may end up using this technology in the future. A person brought his very sick dog to the vet, and the veterinarian made a bad call to say, “Let’s just wait and see.” And the dog would not be here today had he listened. In the meanwhile, he provided the blood test, like, the full medical records, to GPT-4, which said, ""I am not a vet, you need to talk to a professional, here are some hypotheses."" He brought that information to a second vet who used it to save the dog's life. Now, these systems, they're not perfect. You cannot overly rely on them. But this story, I think, shows that a human with a medical professional and with ChatGPT as a brainstorming partner was able to achieve an outcome that would not have happened otherwise. I think this is something we should all reflect on, think about as we consider how to integrate these systems into our world. And one thing I believe really deeply, is that getting AI right is going to require participation from everyone. And that's for deciding how we want it to slot in, that's for setting the rules of the road, for what an AI will and won't do. And if there's one thing to take away from this talk, it's that this technology just looks different. Just different from anything people had anticipated. And so we all have to become literate. And that's, honestly, one of the reasons we released ChatGPT. Together, I believe that we can achieve the OpenAI mission of ensuring that artificial general intelligence benefits all of humanity. Thank you. (Applause) (Applause ends) Chris Anderson: Greg. Wow. I mean ... I suspect that within every mind out here there's a feeling of reeling. Like, I suspect that a very large number of people viewing this, you look at that and you think, “Oh my goodness, pretty much every single thing about the way I work, I need to rethink."" Like, there's just new possibilities there. Am I right? Who thinks that they're having to rethink the way that we do things? Yeah, I mean, it's amazing, but it's also really scary. So let's talk, Greg, let's talk. I mean, I guess my first question actually is just how the hell have you done this? (Laughter) OpenAI has a few hundred employees. Google has thousands of employees working on artificial intelligence. Why is it you who's come up with this technology that shocked the world? Greg Brockman: I mean, the truth is, we're all building on shoulders of giants, right, there's no question. If you look at the compute progress, the algorithmic progress, the data progress, all of those are really industry-wide. But I think within OpenAI, we made a lot of very deliberate choices from the early days. And the first one was just to confront reality as it lays. And that we just thought really hard about like: What is it going to take to make progress here? We tried a lot of things that didn't work, so you only see the things that did. And I think that the most important thing has been to get teams of people who are very different from each other to work together harmoniously. CA: Can we have the water, by the way, just brought here? I think we're going to need it, it's a dry-mouth topic. But isn't there something also just about the fact that you saw something in these language models that meant that if you continue to invest in them and grow them, that something at some point might emerge? GB: Yes. And I think that, I mean, honestly, I think the story there is pretty illustrative, right? I think that high level, deep learning, like we always knew that was what we wanted to be, was a deep learning lab, and exactly how to do it? I think that in the early days, we didn't know. We tried a lot of things, and one person was working on training a model to predict the next character in Amazon reviews, and he got a result where -- this is a syntactic process, you expect, you know, the model will predict where the commas go, where the nouns and verbs are. But he actually got a state-of-the-art sentiment analysis classifier out of it. This model could tell you if a review was positive or negative. I mean, today we are just like, come on, anyone can do that. But this was the first time that you saw this emergence, this sort of semantics that emerged from this underlying syntactic process. And there we knew, you've got to scale this thing, you've got to see where it goes. CA: So I think this helps explain the riddle that baffles everyone looking at this, because these things are described as prediction machines. And yet, what we're seeing out of them feels ... it just feels impossible that that could come from a prediction machine. Just the stuff you showed us just now. And the key idea of emergence is that when you get more of a thing, suddenly different things emerge. It happens all the time, ant colonies, single ants run around, when you bring enough of them together, you get these ant colonies that show completely emergent, different behavior. Or a city where a few houses together, it's just houses together. But as you grow the number of houses, things emerge, like suburbs and cultural centers and traffic jams. Give me one moment for you when you saw just something pop that just blew your mind that you just did not see coming. GB: Yeah, well, so you can try this in ChatGPT, if you add 40-digit numbers -- CA: 40-digit? GB: 40-digit numbers, the model will do it, which means it's really learned an internal circuit for how to do it. And the really interesting thing is actually, if you have it add like a 40-digit number plus a 35-digit number, it'll often get it wrong. And so you can see that it's really learning the process, but it hasn't fully generalized, right? It's like you can't memorize the 40-digit addition table, that's more atoms than there are in the universe. So it had to have learned something general, but that it hasn't really fully yet learned that, Oh, I can sort of generalize this to adding arbitrary numbers of arbitrary lengths. CA: So what's happened here is that you've allowed it to scale up and look at an incredible number of pieces of text. And it is learning things that you didn't know that it was going to be capable of learning. GB Well, yeah, and it’s more nuanced, too. So one science that we’re starting to really get good at is predicting some of these emergent capabilities. And to do that actually, one of the things I think is very undersung in this field is sort of engineering quality. Like, we had to rebuild our entire stack. When you think about building a rocket, every tolerance has to be incredibly tiny. Same is true in machine learning. You have to get every single piece of the stack engineered properly, and then you can start doing these predictions. There are all these incredibly smooth scaling curves. They tell you something deeply fundamental about intelligence. If you look at our GPT-4 blog post, you can see all of these curves in there. And now we're starting to be able to predict. So we were able to predict, for example, the performance on coding problems. We basically look at some models that are 10,000 times or 1,000 times smaller. And so there's something about this that is actually smooth scaling, even though it's still early days. CA: So here is, one of the big fears then, that arises from this. If it’s fundamental to what’s happening here, that as you scale up, things emerge that you can maybe predict in some level of confidence, but it's capable of surprising you. Why isn't there just a huge risk of something truly terrible emerging? GB: Well, I think all of these are questions of degree and scale and timing. And I think one thing people miss, too, is sort of the integration with the world is also this incredibly emergent, sort of, very powerful thing too. And so that's one of the reasons that we think it's so important to deploy incrementally. And so I think that what we kind of see right now, if you look at this talk, a lot of what I focus on is providing really high-quality feedback. Today, the tasks that we do, you can inspect them, right? It's very easy to look at that math problem and be like, no, no, no, machine, seven was the correct answer. But even summarizing a book, like, that's a hard thing to supervise. Like, how do you know if this book summary is any good? You have to read the whole book. No one wants to do that. (Laughter) And so I think that the important thing will be that we take this step by step. And that we say, OK, as we move on to book summaries, we have to supervise this task properly. We have to build up a track record with these machines that they're able to actually carry out our intent. And I think we're going to have to produce even better, more efficient, more reliable ways of scaling this, sort of like making the machine be aligned with you. CA: So we're going to hear later in this session, there are critics who say that, you know, there's no real understanding inside, the system is going to always -- we're never going to know that it's not generating errors, that it doesn't have common sense and so forth. Is it your belief, Greg, that it is true at any one moment, but that the expansion of the scale and the human feedback that you talked about is basically going to take it on that journey of actually getting to things like truth and wisdom and so forth, with a high degree of confidence. Can you be sure of that? GB: Yeah, well, I think that the OpenAI, I mean, the short answer is yes, I believe that is where we're headed. And I think that the OpenAI approach here has always been just like, let reality hit you in the face, right? It's like this field is the field of broken promises, of all these experts saying X is going to happen, Y is how it works. People have been saying neural nets aren't going to work for 70 years. They haven't been right yet. They might be right maybe 70 years plus one or something like that is what you need. But I think that our approach has always been, you've got to push to the limits of this technology to really see it in action, because that tells you then, oh, here's how we can move on to a new paradigm. And we just haven't exhausted the fruit here. CA: I mean, it's quite a controversial stance you've taken, that the right way to do this is to put it out there in public and then harness all this, you know, instead of just your team giving feedback, the world is now giving feedback. But ... If, you know, bad things are going to emerge, it is out there. So, you know, the original story that I heard on OpenAI when you were founded as a nonprofit, well you were there as the great sort of check on the big companies doing their unknown, possibly evil thing with AI. And you were going to build models that sort of, you know, somehow held them accountable and was capable of slowing the field down, if need be. Or at least that's kind of what I heard. And yet, what's happened, arguably, is the opposite. That your release of GPT, especially ChatGPT, sent such shockwaves through the tech world that now Google and Meta and so forth are all scrambling to catch up. And some of their criticisms have been, you are forcing us to put this out here without proper guardrails or we die. You know, how do you, like, make the case that what you have done is responsible here and not reckless. GB: Yeah, we think about these questions all the time. Like, seriously all the time. And I don't think we're always going to get it right. But one thing I think has been incredibly important, from the very beginning, when we were thinking about how to build artificial general intelligence, actually have it benefit all of humanity, like, how are you supposed to do that, right? And that default plan of being, well, you build in secret, you get this super powerful thing, and then you figure out the safety of it and then you push “go,” and you hope you got it right. I don't know how to execute that plan. Maybe someone else does. But for me, that was always terrifying, it didn't feel right. And so I think that this alternative approach is the only other path that I see, which is that you do let reality hit you in the face. And I think you do give people time to give input. You do have, before these machines are perfect, before they are super powerful, that you actually have the ability to see them in action. And we've seen it from GPT-3, right? GPT-3, we really were afraid that the number one thing people were going to do with it was generate misinformation, try to tip elections. Instead, the number one thing was generating Viagra spam. (Laughter) CA: So Viagra spam is bad, but there are things that are much worse. Here's a thought experiment for you. Suppose you're sitting in a room, there's a box on the table. You believe that in that box is something that, there's a very strong chance it's something absolutely glorious that's going to give beautiful gifts to your family and to everyone. But there's actually also a one percent thing in the small print there that says: “Pandora.” And there's a chance that this actually could unleash unimaginable evils on the world. Do you open that box? GB: Well, so, absolutely not. I think you don't do it that way. And honestly, like, I'll tell you a story that I haven't actually told before, which is that shortly after we started OpenAI, I remember I was in Puerto Rico for an AI conference. I'm sitting in the hotel room just looking out over this wonderful water, all these people having a good time. And you think about it for a moment, if you could choose for basically that Pandora’s box to be five years away or 500 years away, which would you pick, right? On the one hand you're like, well, maybe for you personally, it's better to have it be five years away. But if it gets to be 500 years away and people get more time to get it right, which do you pick? And you know, I just really felt it in the moment. I was like, of course you do the 500 years. My brother was in the military at the time and like, he puts his life on the line in a much more real way than any of us typing things in computers and developing this technology at the time. And so, yeah, I'm really sold on the you've got to approach this right. But I don't think that's quite playing the field as it truly lies. Like, if you look at the whole history of computing, I really mean it when I say that this is an industry-wide or even just almost like a human-development- of-technology-wide shift. And the more that you sort of, don't put together the pieces that are there, right, we're still making faster computers, we're still improving the algorithms, all of these things, they are happening. And if you don't put them together, you get an overhang, which means that if someone does, or the moment that someone does manage to connect to the circuit, then you suddenly have this very powerful thing, no one's had any time to adjust, who knows what kind of safety precautions you get. And so I think that one thing I take away is like, even you think about development of other sort of technologies, think about nuclear weapons, people talk about being like a zero to one, sort of, change in what humans could do. But I actually think that if you look at capability, it's been quite smooth over time. And so the history, I think, of every technology we've developed has been, you've got to do it incrementally and you've got to figure out how to manage it for each moment that you're increasing it. CA: So what I'm hearing is that you ... the model you want us to have is that we have birthed this extraordinary child that may have superpowers that take humanity to a whole new place. It is our collective responsibility to provide the guardrails for this child to collectively teach it to be wise and not to tear us all down. Is that basically the model? GB: I think it's true. And I think it's also important to say this may shift, right? We've got to take each step as we encounter it. And I think it's incredibly important today that we all do get literate in this technology, figure out how to provide the feedback, decide what we want from it. And my hope is that that will continue to be the best path, but it's so good we're honestly having this debate because we wouldn't otherwise if it weren't out there. CA: Greg Brockman, thank you so much for coming to TED and blowing our minds. (Applause)",PT30M09S,2023-04-20T15:00:05Z
What will the dream car of the future be like?,"technology,design,transportation,computers,future,travel,AI",Alex Koster,"Fasten your seat belt as software engineer Alex Koster takes us on a journey in what he calls the ""software dream car"" of the future. He breaks down how massive technological shifts are transforming the automotive industry and paints a vivid picture of where cars are headed -- from AI drivers to interiors and exteriors shaped by augmented and virtual reality.","So I'd love to talk a little bit about cars today. So cars have always been an irrational affair. If you think about it, it makes no sense whatsoever to take two tons of metal to move a person from point A to point B. Nevertheless, we have 1.5 billion cars in the world today, and they have a very dear place in many of our hearts. So for some of us, the car represents the ultimate engine of freedom. Like, blow it all off and drive away. And for others it is the coming of age. Like, maybe you had your first kiss in a car. I see you smile. Or maybe the car even had a name. So if we look to China, only 16 percent of people say that a car does not increase your status significantly. And in 2018, the world was watching when in Saudi women took to the steering wheel for the first time. Musicians aren't writing songs about microwaves or Fitbits, but Prince had his song ""Little Red Corvette."" And Rihanna had a great number one Billboard chart song with ""Shut Up and Drive."" Now, when you look at car commercials, they typically look like this. Super romantic. Basically, they take you in a very safe space, fantastic, gorgeous landscape with your dear ones around you. And you can basically drive forever into beautiful land. Now, of course, we all know that this is extremely glamorous but doesn't really reflect reality. When you do buy a car today, what this feels like usually is like this, right? You stand in traffic, you look for parking space, you breathe recycled air, clearly not glamorous. The other thing is, if you put about 100 cars from all kinds of brands next to each other, I guess you will probably also agree that they look more or less all the same. Individuality is clearly dead. So now next to this crisis of the brand and of car industry, there are three major technology trends that are hitting the industry massively. The first one is well-known, which is the shift from fossil energy to clean energy. That one is actually well underway, well understood. By 2028, we expect that pure battery electric vehicles will be the most sold type. But I'd like to focus more on the other two. So one of them is the shift from mechanical to software. And what it does is you don't actually have anymore a fixed product, but it will continue to evolve, to learn, to change. It's a bit like a touch screen instead of a keyboard. The other one is you add AI on top, and it means that you will have the ability to switch from human operator to machine operated, and the robot takes over as the driver. Now, of course, car companies have also understood these trends, and they are working to incorporate some of the capabilities into the cars. But I think we have a way longer way to go and we will soon see the emergence of what I call the “software dream car” as opposed to the freedom dream car or the other mechanical dream car that you had in the past. So actually, you may be surprised I'm actually not a car guy, so I don't have petrol running in my veins or something. So I’m actually a software engineer, and I had the pleasure to work in many other industries before, including telecoms, media, phones, TVs, drones, whatever comes to your mind, and all of them have already gone through the disruption of software. And it made massive changes to all of those industries. So I'm now very proud that I can work with auto companies who are just at the beginning of this journey. Now we have been approached, my team and myself, just very recently, by one of the iconic car brands to think through the future of that software dream car. How does it look like? So we actually co-created something, and we reached out to a bunch of people, working with the auto engineers and the designers for sure. But then we also looked at the professors in neuroscience and biomechanics, creative artists, people from science fiction all the way to luxury industries. And we brought it all together in a multidisciplinary perspective, and I'd like to share some of that today. So I hope you are ready for that. Off we go. So let's start at the center of the description. This is really the core functionality of the car, and obviously it will have an AI driver driving you around. Now that AI driver needs to be a factor of ten better than any human, which means less than one fatality per one billion miles traveled. This is a very hard problem, and the industry has been littered with failures over the past 20 years to get this done. But we are very close. And when it does happen, it will become only one part of the functionality of the vehicle. The other part of the vehicle is actually going to be the in-car experience all around you because you will now be driven by the AI driver. So this is going to be shaped by augmented reality and virtual reality. Now to show a little bit how this could feel like, I'd like to just give a glimpse from a movie that maybe many of you have seen, ""Blade Runner 2049."" Fantastic movie, I hope you all loved it, the ones who saw it. Now there's one character in there, he's the bad guy, so I love to talk about him, it's Wallace. And he actually is vision impaired, so he can't see by himself. But he has a set of floating sensors in a room, and that gives him the ability to see from multiple angles at the same time. Now, it's a bit creepy because it's shown with little bugs floating around. So I don't think that's how it's going to be implemented in the software dream car exactly. But you get the idea that also in the software dream car, you will be able to actually get like an overlay picture to the normal reality that will have all kinds of information and entertainment around you. And you will come to expect that when you drive in the physical world to a destination, that you will also want to be in the virtual world during the same time. And the vehicle will be the thing, the device that takes you to both places. Now, if I turn it up a little bit and I look at the next layer, I want to talk about the performance and the quality. The software dream car will have basically a computing power that is about ten times faster than any other device that you have. And it better does, right? Because I guess we all got very impatient when we push a button in our cars and like, nothing happens for five seconds. So you will get really furious if it takes more than a split second to switch from virtual reality to real reality, so to say. And what you really want is it should feel more like a well-orchestrated dance between the human and the machine on precision, very fast, immediate. And the same is true then again, for the AI driving. If it feels like a choppy ride, like if you are in an Uber from the 2020s, this is not the experience you want to have. You want it to be smooth, getting beside the traffic and not like getting stuck all the time. Then let me go through the last layer on top of this, which is the one that I find most exciting because this is bringing together the full experience. And the comparison I'd like to make is a bit like a photograph. A photograph can become a piece of art. And it does so because it makes you stop, start thinking, search also for meaning. And you will have a similar impression with the software dream car because it will be able, when you have downloaded that NFT photograph, hopefully not of a monkey or so, but some other kind of nice photograph, and you want to put it on the outside of the vehicle, well, you will actually be able to do that, no problem. When you are creative and you want to change the functionality of the car, well, you can program it, download it, and it will do what you want to do. So again, you are in control of both the interior and the exterior experience of the vehicle. You will also have sensors in all the surfaces, so you can basically imagine the car will be able to sense any tensions in your muscles but also any bumps in the road, and it can adjust to both of them. Now, what’s actually surprising maybe: the technology is all there, it's not the barrier. We have all the LiDARs, the radars, the cameras, the holograms. We have quantum computers. We even have brain-computer interfaces, and they are operational for many years. People have chips implanted, and it works. So why don't we see those software dream cars on the road? Well, it's a big step, right, if we look at the automotive companies, they obviously have managed to get cars on the road for more than 100 years. But this step here requires so much new technology understanding data and the ability to innovate fast that it is a huge step forward for them. And you have about 15 million people working directly in the industry globally, plus another 50 to 100 that are in adjacent industries. And many of them will have to change, retrain, get into the new world. So it's a long way to go. And then you have the tech companies well, those guys, they go wild. They have also made us already addicted to smartphones and media and so on. So I'm sure they can make us addicted as well to their car, but they shy away from the liabilities because at the end of the day, this is a life and death product. So you need to be really careful about this. So as we think about the future, I'm totally convinced this will happen very soon, it's around the corner. Now the question will be: Is this now good or bad? And that's a fair challenge. Now, if we put it all together, it's actually complicated. Because first of all, that car will mostly be available for the rich few initially. Also, as I said before, it will upend the lives of many people that are especially in mechanical jobs. You will have plenty of new jobs coming along, but they tend to be in totally different places and also in different skills. And also what's going to happen if you make a product more attractive, you will actually have many more of those on the street rather than fewer. So the good thing is, again, you take out the human in the driving so it becomes a little bit more controlled and safe. And then what about that “Little Red Corvette” from Prince that we had a bit earlier? Well, it's also going to be quite different because the software dream car will quite literally be able to read your mind, which is kind of scary. But at the same time, it also is really good at predicting what you want to have and your preferences, which is kind of nice. And also the camera that you will have in the car will actually be able to also detect when that little child has been left in the overheated vehicle and the vehicle is able to basically open the windows, put on the air conditioning, call emergency services and save lives, in their own way. Now, the other big thing I want to point out is, remember individuality, which is dead today? I think if we now look into the future, we will actually see that the software dream car will enable you to really escape into a world that you can shape on yourself, both digitally and physically. It is really up to you to make it your personal experience, again, in the virtual world and in the real world. It's going to be possible in a way that is way further than anything that you have seen in any recent past. So if we take this all together, I guess the question comes down to: Are you guys technology optimists or pessimists? And really what matters to me is over the next 20 years, when this software dream car will come to the real world, it will be a bit like a white sheet of paper. It’s not good or bad [in] itself, but it is the largest real-life experiment of AI and humans interacting on a daily basis. So I would encourage all of you to embrace it, see the beauty, but also shape it and drive it in a way that it is good for the wider humanity. And I can only speak for myself to say that I'm really excited about what the future holds for us. But I guess for all of you, the question will be when Rihanna will release a song in '28, ""Shut up and program your future self."" (Laughter) Thank you very much. (Applause)",PT11M46S,2023-04-06T14:30:27Z
The most important century in human history,"technology,education,future,history,AI,humanity,TED-Ed,animation", TED-Ed,"Is it possible that this century is the most important one in human history? The 21st century has already proven to be a period of rapid growth. We're on the cusp of developing new technologies that could entirely change the way people live— and could contribute to unprecedented levels of existential risk. Explore how the decisions we make now might have a major impact on humanity's future. [Directed by Jon Mayes, AIM Creative Studios, narrated by George Zaidan, music by André Aires].","What's the most important century in human history? Some might argue it’s a period  of extensive military campaigning, like Alexander the Great’s  in the 300s BCE, which reshaped political and cultural borders. Others might cite the emergence  of a major religion, such as Islam in the 7th century, which codified and spread values across such borders. Or perhaps it’s the Industrial Revolution of the 1700s that transformed global commerce and redefined humanity's relationship with labor. Whatever the answer, it seems like  any century vying for that top spot is at a moment of great change— when the actions of our ancestors shifted humanity’s trajectory for centuries to come. So if this is our metric,  is it possible that right now— this century—  is the most important one yet? The 21st century has already proven to be a period of rapid technological growth. Phones and computers have accelerated the pace of life. And we’re likely on the cusp of developing new transformative technologies, like advanced artificial intelligence, that could entirely change  the way people live. Meanwhile, many technologies  we already have contribute to humanity’s unprecedented  levels of existential risk— that’s the risk of our species going extinct or experiencing some kind of disaster that permanently limits humanity’s ability to grow and thrive. The invention of the atomic bomb marked a major rise in existential risk, and since then we’ve only increased the odds against us. It’s profoundly difficult to estimate the odds of an existential collapse  occurring this century. Very rough guesses put the risk  of existential catastrophe due to nuclear winter and climate change at around 0.1%, with the odds of a pandemic causing the same kind of collapse at a frightening 3%. Given that any of these disasters could  mean the end of life as we know it, these aren’t exactly small figures, And it’s possible this century could see the rise of new technologies that introduce more existential risks. AI experts have a wide range  of estimates regarding when artificial general intelligence will emerge, but according to some surveys, many believe it could happen this century. Currently, we have relatively narrow forms of artificial intelligence, which are designed to do specific tasks  like play chess or recognize faces. Even narrow AIs that do creative work are limited to their singular specialty. But artificial general intelligences, or AGIs, would be able to adapt to and perform any number of tasks, quickly outpacing  their human counterparts. There are a huge variety of guesses about what AGI could look like, and what it would mean for humanity to share the Earth with another sentient entity. AGIs might help us achieve our goals, they might regard us as inconsequential, or, they might see us as an obstacle to swiftly remove. So in terms of existential risk, it's imperative the values of this new technology align with our own. This is an incredibly difficult philosophical and engineering challenge that will require a lot  of delicate, thoughtful work. Yet, even if we succeed, AGI could still lead to another complicated outcome. Let’s imagine an AGI emerges  with deep respect for human life and a desire to solve all humanity’s troubles. But to avoid becoming misaligned, it's been developed to be incredibly rigid about its beliefs. If these machines became  the dominant power on Earth, their strict values might become hegemonic, locking humanity into one ideology that would be incredibly resistant to change. History has taught us that no matter how enlightened a civilization thinks they are, they are rarely up to the moral standards of later generations. And this kind of value lock in could permanently distort or constrain humanity’s moral growth. There's a ton of uncertainty around AGI, and it’s profoundly difficult to predict how any existential risks will play out over the next century. It’s also possible that new,  more pressing concerns might render these risks moot. But even if we can't definitively say that ours is the most important century, it still seems like the decisions  we make might have a major impact on humanity’s future. So maybe we should all live  like the future depends on us— because actually, it just might.",PT05M02S,2023-04-04T15:03:55Z
Does AI actually understand us?,"technology,innovation,language,AI,humanity,machine learning,emotions",Alona Fyshe,"Is AI as smart as it seems? Exploring the ""brain"" behind machine learning, neural networker Alona Fyshe delves into the language processing abilities of talkative tech (like the groundbreaking chatbot and internet obsession ChatGPT) and explains how different it is from your own brain -- even though it can sound convincingly human.","People are funny. We're constantly trying to understand and interpret the world around us. I live in a house with two black cats, and let me tell you, every time I see a black, bunched up sweater out of the corner of my eye, I think it's a cat. It's not just the things we see. Sometimes we attribute more intelligence than might actually be there. Maybe you've seen the dogs on TikTok. They have these little buttons that say things like ""walk"" or ""treat."" They can push them to communicate some things with their owners, and their owners think they use them to communicate some pretty impressive things. But do the dogs know what they're saying? Or perhaps you've heard the story of Clever Hans the horse, and he could do math. And not just like, simple math problems, really complicated ones, like, if the eighth day of the month falls on a Tuesday, what's the date of the following Friday? It's like, pretty impressive for a horse. Unfortunately, Hans wasn't doing math, but what he was doing was equally impressive. Hans had learned to watch the people in the room to tell when he should tap his hoof. So he communicated his answers by tapping his hoof. It turns out that if you know the answer to ""if the eighth day of the month falls on a Tuesday, what's the date of the following Friday,"" you will subconsciously change your posture once the horse has given the correct 18 taps. So Hans couldn't do math, but he had learned to watch the people in the room who could do math, which, I mean, still pretty impressive for a horse. But this is an old picture, and we would not fall for Clever Hans today. Or would we? Well, I work in AI, and let me tell you, things are wild. There have been multiple examples of people being completely convinced that AI understands them. In 2022, a Google engineer thought that Google’s AI was sentient. And you may have had a really human-like conversation with something like ChatGPT. But models we're training today are so much better than the models we had even five years ago. It really is remarkable. So at this super crazy moment in time, let’s ask the super crazy question: Does AI understand us, or are we having our own Clever Hans moment? Some philosophers think that computers will never understand language. To illustrate this, they developed something they call the Chinese room argument. In the Chinese room, there is a person, hypothetical person, who does not understand Chinese, but he has along with him a set of instructions that tell him how to respond in Chinese to any Chinese sentence. Here's how the Chinese room works. A piece of paper comes in through a slot in the door, has something written in Chinese on it. The person uses their instructions to figure out how to respond. They write the response down on a piece of paper and then send it back out through the door. To somebody who speaks Chinese, standing outside this room, it might seem like the person inside the room speaks Chinese. But we know they do not, because no knowledge of Chinese is required to follow the instructions. Performance on this task does not show that you know Chinese. So what does that tell us about AI? Well, when you and I stand outside of the room, when we speak to one of these AIs like ChatGPT, we are the person standing outside the room. We're feeding in English sentences, we're getting English sentences back. It really looks like the models understand us. It really looks like they know English. But under the hood, these models are just following a set of instructions, albeit complex. How do we know if AI understands us? To answer that question, let's go back to the Chinese room again. Let's say we have two Chinese rooms. In one Chinese room is somebody who actually speaks Chinese, and in the other room is our impostor. When the person who actually speaks Chinese gets a piece of paper that says something in Chinese in it, they can read it, no problem. But when our imposter gets it again, he has to use his set of instructions to figure out how to respond. From the outside, it might be impossible to distinguish these two rooms, but we know inside something really different is happening. To illustrate that, let's say inside the minds of our two people, inside of our two rooms, is a little scratch pad. And everything they have to remember in order to do this task has to be written on that little scratch pad. If we could see what was written on that scratch pad, we would be able to tell how different their approach to the task is. So though the input and the output of these two rooms might be exactly the same, the process of getting from input to output -- completely different. So again, what does that tell us about AI? Again, if AI, even if it generates completely plausible dialogue, answers questions just like we would expect, it may still be an imposter of sorts. If we want to know if AI understands language like we do, we need to know what it's doing. We need to get inside to see what it's doing. Is it an imposter or not? We need to see its scratch pad, and we need to be able to compare it to the scratch pad of somebody who actually understands language. But like scratch pads in brains, that's not something we can actually see, right? Well, it turns out that we can kind of see scratch pads in brains. Using something like fMRI or EEG, we can take what are like little snapshots of the brain while it’s reading. So have people read words or stories and then take pictures of their brain. And those brain images are like fuzzy, out-of-focus pictures of the scratch pad of the brain. They tell us a little bit about how the brain is processing and representing information while you read. So here are three brain images taken while a person read the word ""apartment,"" ""house"" and ""celery."" You can see just with your naked eye that the brain image for ""apartment"" and ""house"" are more similar to each other than they are to the brain image for ""celery."" And you know, of course that apartments and houses are more similar than they are to celery, just the words. So said another way, the brain uses its scratchpad when reading the words ""apartment"" and ""house"" in a way that's more similar than when you read the word ""celery."" The scratch pad tells us a little bit about how the brain represents the language. It's not a perfect picture of what the brain's doing, but it's good enough. OK, so we have scratch pads for the brain. Now we need a scratch pad for AI. So inside a lot of AIs is a neural network. And inside of a neural network is a bunch of these little neurons. So here the neurons are like these little gray circles. And we would like to know what is the scratch pad of a neural network? Well, when we feed in a word into a neural network, each of the little neurons computes a number. Those little numbers I'm representing here with colors. So every neuron computes this little number, and those numbers tell us something about how the neural network is processing language. Taken together, all of those little circles paint us a picture of how the neural network is representing language, and they give us the scratch pad of the neural network. OK, great. Now we have two scratch pads, one from the brain and one from AI. And we want to know: Is AI doing something like what the brain is doing? How can we test that? Here's what researchers have come up with. We're going to train a new model. That new model is going to look at neural network scratch pad for a particular word and try to predict the brain scratch pad for the same word. We can do it, by the way, around two. So let's train a new model. It’s going to look at the neural network scratch pad for a particular word and try to predict the brain scratchpad. If the brain and AI are doing nothing alike, have nothing in common, we won't be able to do this prediction task. It won't be possible to predict one from the other. So we've reached a fork in the road and you can probably tell I'm about to tell you one of two things. I’m going to tell you AI is amazing, or I'm going to tell you AI is an imposter. Researchers like me love to remind you that AI is nothing like the brain. And that is true. But could it also be the AI and the brain share something in common? So we’ve done this scratch pad prediction task, and it turns out, 75 percent of the time the predicted neural network scratchpad for a particular word is more similar to the true neural network scratchpad for that word than it is to the neural network scratch pad for some other randomly chosen word -- 75 percent is much better than chance. What about for more complicated things, not just words, but sentences, even stories? Again, this scratch pad prediction task works. We’re able to predict the neural network scratch pad from the brain and vice versa. Amazing. So does that mean that neural networks and AI understand language just like we do? Well, truthfully, no. Though these scratch pad prediction tasks show above-chance accuracy, the underlying correlations are still pretty weak. And though neural networks are inspired by the brain, they don't have the same kind of structure and complexity that we see in the brain. Neural networks also don't exist in the world. A neural network has never opened a door or seen a sunset, heard a baby cry. Can a neural network that doesn't actually exist in the world, hasn't really experienced the world, really understand language about the world? Still, these scratch pad prediction experiments have held up -- multiple brain imaging experiments, multiple neural networks. We've also found that as the neural networks get more accurate, they also start to use their scratch pad in a way that becomes more brain-like. And it's not just language. We've seen similar results in navigation and vision. So AI is not doing exactly what the brain is doing, but it's not completely random either. So from where I sit, if we want to know if AI really understands language like we do, we need to get inside of the Chinese room. We need to know what the AI is doing, and we need to be able to compare that to what people are doing when they understand language. AI is moving so fast. Today, I'm asking you, does AI understand language that might seem like a silly question in ten years. Or ten months. (Laughter) But one thing will remain true. We are meaning-making humans, and we are going to continue to look for meaning and interpret the world around us. And we will need to remember that if we only look at the input and output of AI, it's very easy to be fooled. We need to get inside of the metaphorical room of AI in order to see what's happening. It's what's inside the counts. Thank you. (Applause)",PT10M28S,2023-03-30T14:46:00Z
Are insect brains the secret to great AI?,"science,technology,computers,innovation,biomimicry,future,insects,AI,neuroscience,machine learning",Frances S. Chance,"Are insects the key to brain-inspired computing? Neuroscientist Frances S. Chance thinks so. In this buzzy talk, she shares examples of the incredible capabilities of insects -- like the dragonfly's deadly accurate hunting skills and the African dung beetle's superstrength -- and shows how untangling the mysterious web of neurons in their tiny brains could lead to breakthroughs in computers, AI and more.","Creating intelligence on a computer. This has been the Holy Grail for artificial intelligence for quite some time. But how do we get there? So we view ourselves as highly intelligent beings. So it's logical to study our own brains, the substrate of our cognition, for creating artificial intelligence. Imagine if we could replicate how our own brains work on a computer. But now consider the journey that would be required. The human brain contains 86 billion neurons. Each is constantly communicating with thousands of others, and each has individual characteristics of its own. Capturing the human brain on a computer may simply be too big and too complex a problem to tackle with the technology and the knowledge that we have today. I believe that we can capture a brain on a computer, but we have to start smaller. Much smaller. These insects have three of the most fascinating brains in the world to me. While they do not possess human-level intelligence, each is remarkable at a particular task. Think of them as highly trained specialists. African dung beetles are really good at rolling large balls in straight lines. (Laughter) Now, if you've ever made a snowman, you know that rolling a large ball is not easy. Now picture trying to make that snowman when the ball of snow is as big as you are and you're standing on your head. (Laughter) Sahara desert ants are navigation specialists. They might have to wander a considerable distance to forage for food. But once they do find sustenance, they know how to calculate the straightest path home. And the dragonfly is a hunting specialist. In the wild, dragonflies capture approximately 95 percent of the prey they choose to go after. These insects are so good at their specialties that neuroscientists such as myself study them as model systems to understand how animal nervous systems solve particular problems. And in my own research, I study brains to bring these solutions, the best that biology has to offer, to computers. So consider the dragonfly brain. It has only on the order of one million neurons. Now, it's still not easy to unravel a circuit of even one million neurons. But given the choice between trying to tease apart the one-million-neuron brain versus the 86-billion-neuron brain, which would you choose to try first? (Laughter) When studying these smaller insect brains, the immediate goal is not human intelligence. We study these brains for what the insects do well. And in the case of the dragonfly, that's interception. So when dragonflies are hunting, they do more than just fly straight at the prey. They fly in such a way that they will intercept it. They aim for where the prey is going to be. Much like a soccer player, running to intercept a pass. To do this correctly, dragonflies need to perform what is known as a coordinate transformation, going from the eye’s frame of reference, or what the dragonfly sees, to the body's frame of reference, or how the dragonfly needs to turn its body to intercept. Coordinate transformations are a basic calculation that animals need to perform to interact with the world. We do them instinctively every time we reach for something. When I reach for an object straight in front of me, my arm takes a very different trajectory than if I turn my head, look at that same object when it is off to one side and reach for it there. In both cases, my eyes see the same image of that object, but my brain is sending my arm on a very different trajectory based on the position of my neck. And dragonflies are fast. This means they calculate fast. The latency, or the time it takes for a dragonfly to respond once it sees the prey turn, is about 50 milliseconds. This latency is remarkable. For one thing, it's only half the time of a human eye blink. But for another thing, it suggests that dragonflies capture how to intercept in only relatively or surprisingly few computational steps. So in the brain, a computational step is a single neuron or a layer of neurons working in parallel. It takes a single neuron about 10 milliseconds to add up all its inputs and respond. The 50-millisecond response time means that once the dragonfly sees its prey turn, there's only time for maybe four of these computational steps or four layers of neurons, working in sequence, one after the other, to calculate how the dragonfly needs to turn. In other words, if I want to study how the dragonfly does coordinate transformations, the neural circuit that I need to understand, the neural circuit that I need to study, can have at most four layers of neurons. Each layer may have many neurons, but this is a small neural circuit. Small enough that we can identify it and study it with the tools that are available today. And this is what I'm trying to do. I have built a model of what I believe is the neural circuit that calculates how the dragonfly should turn. And here is the cool result. In the model, dragonflies do coordinate transformations in only one computational step, one layer of neurons. This is something we can test and understand. In a computer simulation, I can predict the activities of individual neurons while the dragonfly is hunting. For example, here I am predicting the action potentials, or the spikes, that are fired by one of these neurons when the dragonfly sees the prey move. To test the model, my collaborators and I are now comparing these predicted neural responses with responses of neurons recorded in living dragonfly brains. These are ongoing experiments in which we put living dragonflies in virtual reality. (Laughter) Now, it's not practical to put VR goggles on a dragonfly. So instead, we show movies of moving targets to the dragonfly, while an electrode records activity patterns of individual neurons in the brain. Yeah, he likes the movies. If the responses that we record in the brain match those predicted by the model, we will have identified which neurons are responsible for coordinate transformations. The next step will be to understand the specifics of how these neurons work together to do the calculation. But this is how we begin to understand how brains do basic or primitive calculations. Calculations that I regard as building blocks for more complex functions, not only for interception but also for cognition. The way that these neurons compute may be different from anything that exists on a computer today. And the goal of this work is to do more than just write code that replicates the activity patterns of neurons. We aim to build a computer chip that not only does the same things as biological brains but does them in the same way as biological brains. This could lead to drones driven by computers the same size of the dragonfly's brain that captures some targets and avoid others. Personally, I'm hoping for a small army of these to defend my backyard from mosquitoes in the summer. (Laughter) The GPS on your phone could be replaced by a new navigation device based on dung beetles or ants that could guide you to the straight or the easy path home. And what would the power requirements of these devices be like? As small as it is -- Or, sorry -- as large as it is, the human brain is estimated to have the same power requirements as a 20-watt light bulb. Imagine if all brain-inspired computers had the same extremely low-power requirements. Your smartphone or your smartwatch probably needs charging every day. Your new brain-inspired device might only need charging every few months, or maybe even every few years. The famous physicist, Richard Feynman, once said, ""What I cannot create, I do not understand."" What I see in insect nervous systems is an opportunity to understand brains through the creation of computers that work as brains do. And creation of these computers will not just be for knowledge. There's potential for real impact on your devices, your vehicles, maybe even artificial intelligences. So next time you see an insect, consider that these tiny brains can lead to remarkable computers. And think of the potential that they offer us for the future. Thank you. (Applause)",PT09M32S,2022-12-19T15:08:51Z
How will AI change the world?,"technology,education,future,AI,TED-Ed,animation", TED-Ed,"In the coming years, artificial intelligence is probably going to change your life— and likely the entire world. But people have a hard time agreeing on exactly how AI will affect our society. Can we build AI systems that help us fix the world? Or are we doomed to a robotic takeover? Explore the limitations of artificial intelligence and the possibility of creating human-compatible technology. [Directed by Christoph Sarow, AIM Creative Studios, narrated by George Zaidan and Stuart Russell, music by André Aires]. ","In the coming years,  artificial intelligence is probably going to change your life,  and likely the entire world. But people have a hard time agreeing on exactly how. The following are excerpts  from a World Economic Forum interview where renowned computer science professor  and AI expert Stuart Russell helps separate the sense  from the nonsense. There’s a big difference between asking a human to do something and giving that as the objective to an AI system. When you ask a human to get you a cup of coffee, you don’t mean this should be  their life’s mission, and nothing else in the universe matters. Even if they have to kill everybody else in Starbucks to get you the coffee before it closes— they should do that. No, that’s not what you mean. All the other things that we mutually care about, they should factor into your behavior as well. And the problem with the way  we build AI systems now is we give them a fixed objective. The algorithms require us to specify everything in the objective. And if you say, can we fix the acidification of the oceans? Yeah, you could have a catalytic reaction that does that extremely efficiently, but it consumes a quarter  of the oxygen in the atmosphere, which would apparently cause us to die fairly slowly and unpleasantly over the course of several hours. So, how do we avoid this problem? You might say, okay, well, just be more careful about specifying the objective— don’t forget the atmospheric oxygen. And then, of course, some side effect of the reaction in the ocean poisons all the fish. Okay, well I meant don’t kill the fish either. And then, well, what about the seaweed? Don’t do anything that’s going to cause all the seaweed to die. And on and on and on. And the reason that we don’t have to do that with humans is that humans often know that they don’t know  all the things that we care about. If you ask a human to get you a cup of coffee, and you happen to be  in the Hotel George Sand in Paris, where the coffee is 13 euros a cup, it’s entirely reasonable to come back and say, well, it’s 13 euros, are you sure you want it,  or I could go next door and get one? And it’s a perfectly normal thing for a person to do. To ask, I’m going to repaint your house— is it okay if I take off the drainpipes and then put them back? We don't think of this as a terribly sophisticated capability, but AI systems don’t have it  because the way we build them now, they have to know the full objective. If we build systems that know that  they don’t know what the objective is, then they start to exhibit these behaviors, like asking permission before getting rid of all the oxygen in the atmosphere. In all these senses,  control over the AI system comes from the machine’s uncertainty  about what the true objective is. And it’s when you build machines that believe with certainty that they have the objective, that’s when you get this sort of psychopathic behavior. And I think we see  the same thing in humans. What happens when general purpose AI hits the real economy? How do things change? Can we adapt? This is a very old point. Amazingly, Aristotle actually has a passage where he says, look, if we had fully automated  weaving machines and plectrums that could pluck the lyre  and produce music without any humans, then we wouldn’t need any workers. That idea, which I think it was Keynes who called it technological unemployment  in 1930, is very obvious to people. They think, yeah, of course,  if the machine does the work, then I'm going to be unemployed. You can think about the warehouses  that companies are currently operating for e-commerce,  they are half automated. The way it works is that an old warehouse— where you’ve got tons of stuff piled up all over the place  and humans go and rummage around and then bring it back and send it off— there’s a robot who goes and gets the shelving unit that contains the thing that you need, but the human has to pick the object  out of the bin or off the shelf, because that’s still too difficult. But, at the same time, would you make a robot that is accurate enough to be able to pick pretty much any object within a very wide  variety of objects that you can buy? That would, at a stroke,  eliminate 3 or 4 million jobs? There's an interesting story that E.M. Forster wrote, where everyone is entirely machine dependent. The story is really about the fact that if you hand over the management of your civilization to machines, you then lose the incentive to understand it yourself or to teach the next generation  how to understand it. You can see “WALL-E” actually as a modern version, where everyone is enfeebled and infantilized by the machine, and that hasn’t been possible up to now. We put a lot of our civilization  into books, but the books can’t run it for us. And so we always have to teach the next generation. If you work it out, it’s about a trillion person years of teaching and learning and an unbroken chain that goes back  tens of thousands of generations. What happens if that chain breaks? I think that’s something we have  to understand as AI moves forward. The actual date of arrival of general purpose AI— you’re not going to be able to pinpoint, it isn’t a single day. It’s also not the case  that it’s all or nothing. The impact is going to be increasing. So with every advance in AI, it significantly expands the range of tasks. So in that sense, I think most experts say by the end of the century, we’re very, very likely to have  general purpose AI. The median is something around 2045. I'm a little more on the conservative side. I think the problem is harder than we think. I like what John McAfee,  he was one of the founders of AI, when he was asked this question, he said,  somewhere between five and 500 years. And we're going to need, I think, several Einsteins to make it happen.",PT05M39S,2022-12-06T16:01:49Z
AI-generated creatures that stretch the boundaries of imagination,"technology,biotech,nature,creativity,art,biology,biomimicry,biodiversity,AI,humanity,conservation",Sofia Crespo,"Can AI help us see beyond our human capabilities? Through a kaleidoscopic blend of technology, nature and art, neural artist Sofia Crespo brings to life animals that push the boundaries of creativity and imagination. Her artistic renditions of chimeras combine images of real-world endangered species to create something totally new -- with the intention of inspiring real-world conservation. Witness a speculative study of creatures that never existed, brought to life by AI.","I'd like to start by asking you to imagine a color that you've never seen before. Just for a second give this a try. Can you actually visualize a color that you've never been able to perceive? I never seem to get tired of trying this although I know it's not an easy challenge. And the thing is, we can't imagine something without drawing upon our experiences. A color we haven't yet seen outside the spectrum we can perceive is outside our ability to conjure up. It's almost like there's a boundary to our imagination where all the colors we can imagine can only be various shades of other colors we have previously seen. Yet we know for a fact that those color frequencies outside our visible spectrum are there. And scientists believe that there are species that have many more photo receptors than just the three color ones we humans have. Which, by the way, not all humans see the world in the same way. Some of us are colorblind to various degrees, and very often we don't even agree on small things, like if a dress on the internet is blue and black or white and gold. But my favorite creature, one of my favorite creatures, is the peacock mantis shrimp, which is estimated to have 12 to 16 photo receptors. And that indicates the world to them might look so much more colorful. So what about artificial intelligence? Can AI help us see beyond our human capabilities? Well, I've been working with AI for the past five years, and in my experience, it can see within the data it gets fed. But then you might be wondering, OK, if AI can't help imagine anything new, why would an artist see any point in using it? And my answer to that is because I think that it can help augment our creativity as there's value in creating combinations of known elements to form new ones. And this boundary of what we can imagine based on what we have experienced is the place that I have been exploring. For me, it started with jellyfish on a screen at an aquarium and wearing those old 3D glasses, which I hope you remember, the ones with the blue and red lens. And this experience made me want to recreate their textures. But not just that, I also wanted to create new jellyfish that I hadn't seen before, like these. And what started with jellyfish, very quickly escalated to other sea creatures like sea anemone, coral and fish. And then from there came amphibians, birds and insects. And this became a series called “Neural Zoo”. But when you look closely, what do you see? There's no single creature in these images. And AI augments my creative process by allowing me to distill and recombine textures. And that's something that would otherwise take me months to draw by hand. Plus I'm actually terrible at drawing. So you could say, in a way, what I'm doing is a contemporary version of something that humans have already been doing for a long time, even before cameras existed. In medieval times, people went on expeditions, and when they came back they would share about what they saw to an illustrator. And the illustrator, having never seen what was being described, would end up drawing based on the creatures that they had previously seen and in the process creating hybrid animals of some sort. So an explorer might describe a beaver, but having never seen one, the illustrator might give it the head of a rodent, the body of a dog and a fish-like tail. In the series “Artificial Natural History”, I took thousands of illustrations from a natural history archives, and I fed them to a neural network to generate new versions of them. But up until now, all my work was done in 2D. And with the help of my studio partner, Feileacan McCormick, we decided to train a neural network on a data set of 3D scanned beetles. But I must warn you that our first results were extremely blurry, and they looked like the blobs you see here. And this could be due to many reasons, but one of them being that there aren't really a lot of openly available data sets of 3D insects. And also we were repurposing a neural network that normally gets used to generate images to generate 3D. So believe it or not, these are very exciting blobs to us. But with time and some very hacky solutions like data augmentation, where we threw in ants and other beetle-like insects to enhance the data set, we ended up getting this, which we've been told they look like grilled chicken. (Laughter) But hungry for more, we pushed our technique, and eventually they ended up looking like this. We use something called 3D style transfer to map textures onto them, and we also trained a natural language model to generate scientific-like names and anatomical descriptions. And eventually we even found a network architecture that could handle 3D meshes. So they ended up looking like this. And for us, this became a way of creating kind of a speculative study -- (Applause) A speculative study of creatures that never existed, kind of like a speculative biology. But I didn't want to talk about AI and its potential unless it brought me closer to a real species. Which of these do you think is easier to find data about online? (Laughter) Yeah, well, as you guessed correctly, the red panda. And this maybe could be due to many reasons, but one of them being how cute they are, which means we photograph and talk about them a lot, unlike the boreal felt lichen. But both of them are classified as endangered. So I wanted to bring visibility to other endangered species that don't get the same amount of digital representation as a cute, fluffy red panda. And to do this, we trained an AI on millions of images of the natural world, and then we prompted with text to generate some of these creatures. So when prompted with a text, ""an image of a critically endangered spider, the peacock tarantula"" and its scientific name, our model generated this. And here's an image of the real peacock tarantula, which is a wonderful spider endemic to India. But when prompted with a text ""an image of a critically endangered bird, the mangrove finch,"" our model generated this. And here's a photo of the real mangrove finch. Both these creatures exist in the wild, but the accuracy of each generated image is fully dependent on the data available. These chimeras of our everyday data to me are a different way of how the future could be. Not in a literal sense, perhaps, but in the sense that through practicing the expanding of our own imagination about the ecosystems we are a part of, we might just be better equipped to recognize new opportunities and potential. Knowing that there's a boundary to our imagination doesn't have to feel limiting. On the contrary, it can help motivate us to expand that boundary further and to seek out colors and things we haven't yet seen and perhaps enrich our imagination as a result. So thank you. (Applause)",PT09M16S,2022-11-21T15:38:47Z
How AI could empower any business,"technology,engineering,business,entrepreneur,software,innovation,AI,data,machine learning,manufacturing",Andrew Ng,"Expensive to build and often needing highly skilled engineers to maintain, artificial intelligence systems generally only pay off for large tech companies with vast amounts of data. But what if your local pizza shop could use AI to predict which flavor would sell best each day of the week? Andrew Ng shares a vision for democratizing access to AI, empowering any business to make decisions that will increase their profit and productivity. Learn how we could build a richer society – all with just a few self-provided data points.","When I think about the rise of AI, I'm reminded by the rise of literacy. A few hundred years ago, many people in society thought that maybe not everyone needed to be able to read and write. Back then, many people were tending fields or herding sheep, so maybe there was less need for written communication. And all that was needed was for the high priests and priestesses and monks to be able to read the Holy Book, and the rest of us could just go to the temple or church or the holy building and sit and listen to the high priest and priestesses read to us. Fortunately, it was since figured out that we can build a much richer society if lots of people can read and write. Today, AI is in the hands of the high priests and priestesses. These are the highly skilled AI engineers, many of whom work in the big tech companies. And most people have access only to the AI that they build for them. I think that we can build a much richer society if we can enable everyone to help to write the future. But why is AI largely concentrated in the big tech companies? Because many of these AI projects have been expensive to build. They may require dozens of highly skilled engineers, and they may cost millions or tens of millions of dollars to build an AI system. And the large tech companies, particularly the ones with hundreds of millions or even billions of users, have been better than anyone else at making these investments pay off because, for them, a one-size-fits-all AI system, such as one that improves web search or that recommends better products for online shopping, can be applied to [these] very large numbers of users to generate a massive amount of revenue. But this recipe for AI does not work once you go outside the tech and internet sectors to other places where, for the most part, there are hardly any projects that apply to 100 million people or that generate comparable economics. Let me illustrate an example. Many weekends, I drive a few minutes from my house to a local pizza store to buy a slice of Hawaiian pizza from the gentleman that owns this pizza store. And his pizza is great, but he always has a lot of cold pizzas sitting around, and every weekend some different flavor of pizza is out of stock. But when I watch him operate his store, I get excited, because by selling pizza, he is generating data. And this is data that he can take advantage of if he had access to AI. AI systems are good at spotting patterns when given access to the right data, and perhaps an AI system could spot if Mediterranean pizzas sell really well on a Friday night, maybe it could suggest to him to make more of it on a Friday afternoon. Now you might say to me, ""Hey, Andrew, this is a small pizza store. What's the big deal?"" And I say, to the gentleman that owns this pizza store, something that could help him improve his revenues by a few thousand dollars a year, that will be a huge deal to him. I know that there is a lot of hype about AI's need for massive data sets, and having more data does help. But contrary to the hype, AI can often work just fine even on modest amounts of data, such as the data generated by a single pizza store. So the real problem is not that there isn’t enough data from the pizza store. The real problem is that the small pizza store could never serve enough customers to justify the cost of hiring an AI team. I know that in the United States there are about half a million independent restaurants. And collectively, these restaurants do serve tens of millions of customers. But every restaurant is different with a different menu, different customers, different ways of recording sales that no one-size-fits-all AI would work for all of them. What would it be like if we could enable small businesses and especially local businesses to use AI? Let's take a look at what it might look like at a company that makes and sells T-shirts. I would love if an accountant working for the T-shirt company can use AI for demand forecasting. Say, figure out what funny memes to prints on T-shirts that would drive sales, by looking at what's trending on social media. Or for product placement, why can’t a front-of-store manager take pictures of what the store looks like and show it to an AI and have an AI recommend where to place products to improve sales? Supply chain. Can an AI recommend to a buyer whether or not they should pay 20 dollars per yard for a piece of fabric now, or if they should keep looking because they might be able to find it cheaper elsewhere? Or quality control. A quality inspector should be able to use AI to automatically scan pictures of the fabric they use to make T-shirts to check if there are any tears or discolorations in the cloth. Today, large tech companies routinely use AI to solve problems like these and to great effect. But a typical T-shirt company or a typical auto mechanic or retailer or school or local farm will be using AI for exactly zero of these applications today. Every T-shirt maker is sufficiently different from every other T-shirt maker that there is no one-size-fits-all AI that will work for all of them. And in fact, once you go outside the internet and tech sectors in other industries, even large companies such as the pharmaceutical companies, the car makers, the hospitals, also struggle with this. This is the long-tail problem of AI. If you were to take all current and potential AI projects and sort them in decreasing order of value and plot them, you get a graph that looks like this. Maybe the single most valuable AI system is something that decides what ads to show people on the internet. Maybe the second most valuable is a web search engine, maybe the third most valuable is an online shopping product recommendation system. But when you go to the right of this curve, you then get projects like T-shirt product placement or T-shirt demand forecasting or pizzeria demand forecasting. And each of these is a unique project that needs to be custom-built. Even T-shirt demand forecasting, if it depends on trending memes on social media, is a very different project than pizzeria demand forecasting, if that depends on the pizzeria sales data. So today there are millions of projects sitting on the tail of this distribution that no one is working on, but whose aggregate value is massive. So how can we enable small businesses and individuals to build AI systems that matter to them? For most of the last few decades, if you wanted to build an AI system, this is what you have to do. You have to write pages and pages of code. And while I would love for everyone to learn to code, and in fact, online education and also offline education are helping more people than ever learn to code, unfortunately, not everyone has the time to do this. But there is an emerging new way to build AI systems that will let more people participate. Just as pen and paper, which are a vastly superior technology to stone tablet and chisel, were instrumental to widespread literacy, there are emerging new AI development platforms that shift the focus from asking you to write lots of code to asking you to focus on providing data. And this turns out to be much easier for a lot of people to do. Today, there are multiple companies working on platforms like these. Let me illustrate a few of the concepts using one that my team has been building. Take the example of an inspector wanting AI to help detect defects in fabric. An inspector can take pictures of the fabric and upload it to a platform like this, and they can go in to show the AI what tears in the fabric look like by drawing rectangles. And they can also go in to show the AI what discoloration on the fabric looks like by drawing rectangles. So these pictures, together with the green and pink rectangles that the inspector's drawn, are data created by the inspector to explain to AI how to find tears and discoloration. After the AI examines this data, we may find that it has seen enough pictures of tears, but not yet enough pictures of discolorations. This is akin to if a junior inspector had learned to reliably spot tears, but still needs to further hone their judgment about discolorations. So the inspector can go back and take more pictures of discolorations to show to the AI, to help it deepen this understanding. By adjusting the data you give to the AI, you can help the AI get smarter. So an inspector using an accessible platform like this can, in a few hours to a few days, and with purchasing a suitable camera set up, be able to build a custom AI system to detect defects, tears and discolorations in all the fabric being used to make T-shirts throughout the factory. And once again, you may say, ""Hey, Andrew, this is one factory. Why is this a big deal?"" And I say to you, this is a big deal to that inspector whose life this makes easier and equally, this type of technology can empower a baker to use AI to check for the quality of the cakes they're making, or an organic farmer to check the quality of the vegetables, or a furniture maker to check the quality of the wood they're using. Platforms like these will probably still need a few more years before they're easy enough to use for every pizzeria owner. But many of these platforms are coming along, and some of them are getting to be quite useful to someone that is tech savvy today, with just a bit of training. But what this means is that, rather than relying on the high priests and priestesses to write AI systems for everyone else, we can start to empower every accountant, every store manager, every buyer and every quality inspector to build their own AI systems. I hope that the pizzeria owner and many other small business owners like him will also take advantage of this technology because AI is creating tremendous wealth and will continue to create tremendous wealth. And it's only by democratizing access to AI that we can ensure that this wealth is spread far and wide across society. Hundreds of years ago. I think hardly anyone understood the impact that widespread literacy will have. Today, I think hardly anyone understands the impact that democratizing access to AI will have. Building AI systems has been out of reach for most people, but that does not have to be the case. In the coming era for AI, we’ll empower everyone to build AI systems for themselves, and I think that will be incredibly exciting future. Thank you very much. (Applause)",PT11M16S,2022-09-27T14:53:08Z
What if you could sing in your favorite musician's voice?,"technology,music,performance,creativity,art,future,AI",Holly Herndon,"What if you could create new music using your favorite musician's voice? Sharing her melodic gifts with the world, multidisciplinary artist Holly Herndon introduces Holly+, an AI-powered instrument that lets people sing with her own voice. Musician Pher joins her onstage to demonstrate this mind-blowing tech while singing into two microphones -- one that amplifies his natural voice and another that makes him sound just like Holly.","(Overlapping voices sing) (Singing ends) (Applause) Hi, my name is Holly, and I’m an artist. That was my voice, but I didn’t sing that clip. I trained an AI on my own voice, and now she can sing anything in multiple languages. Her name is Holly+, and you just heard her perform “El Cant de la Sibilla,” which is a traditional song arranged by Maria Arnal in Catalan. Not a language that I speak and not a vocal tradition that I've trained in. Those melismatic runs are really difficult to hit. To show you Holly+'s full range, I'll also play an example in German. This is ""Mack the Knife"" by Brecht. (Singing in German) (Music ends) Of course Holly+ can -- thank you. (Applause) Of course, Holly+ can also perform my own music, so here's an excerpt from my own song, ""Frontier."" (Singing ""Frontier"" in overlapping voices) (Singing ends) Holly+ uses a process called timbre transfer, where the timbre, or sound quality of one sound, may be mapped onto the performance of another. Timbre transfer is done by creating a machine-learning model of a sound. In this case, it's my voice. So I recorded a wide variety of phrases in my entire vocal range. To be clear, this version of Holly+ is reading notes from a score. So I'll also play you an example of her singing in Spanish, “Bésame Mucho” by Velázquez. (Music: ""Bésame Mucho"") (Music ends) Thank you. (Applause) I can't speak Spanish, by the way, so. Teaching an AI the sonic properties of one sound in order to generate an entirely new sound is what I like to call ""spawning."" Spawning is what allows Holly+ to create a wide range of vocals that I didn't sing from a set of recorded phrases that I did sing. I like to think of spawning as a kind of 21st-century corollary to the musical tradition of sampling, which had a really big impact on both music and intellectual property. But I think spawning is far more exciting and potentially really weird. (Laughter) So, for example, with sampling, usually you copy and remix a recording by someone else to create something new. But with spawning, you can perform as someone else based on trained information about them. And as an artist, this is making me rethink my own past work as not only my archive but potentially also I myself could become reanimated with AI. This also opens up the question of how we deal with a collective human archive if we can reanimate old media. It opens up really big ethical and intellectual property questions that require entirely new conceptual and legal frameworks. One way that I like to think about intellectual property I call identity play. So rather than limiting the use of my voice, I'm creating instruments to allow as many people as possible to create music with me, and even as me. That's why I've made versions of Holly+ freely available for anyone to use online. If I can allow people to play with my IP, my digital identity, my intellectual property, what might they come up with? Could someone else go on tour as me, with my permission? Could I be in a thousand different bands in multiple languages? And what would that even sound like? To be clear, musicians have been taught to be really protective over our IP, and that's for really good reason. But I'm trying to think of ways to make this new capability mutually beneficial. So to allow people to use my voice but still to maintain the ability to approve certain derivative works. So I invite you to consider, if given the opportunity, who would you like to perform through? And can you imagine someone else performing you? With that in mind, I'd like to invite the incredible musician Pher to the stage. (Applause) So today, Pher will be performing his own song, ""Murky."" And with one microphone you'll hear Pher's beautiful, natural voice. (Pher vocalizing) (Audience cheers) And with this microphone, you'll hear a live version of Holly+ developed with Voctro Labs. (Pher vocalizing in Holly+ voice) (Laughter and applause) Take it away Pher. (Pher singing ""Murky"") It gets so murky Loving what you know But I, I keep on swimming, I'm swimming Further up the road See, the problem is, I don't know what to say When you come around Acting this way Oh oh And, yes, the truth is I show you every day Cause you love to stay (Applause) Living in all the pain And it gets so murky Loving what you know But I'll keep on swimming Further up the road And I've been calling out Calling out your name Deep down in my sleep And I even memorized your face So ain't no leaving' me And I just handle my biz And keep the train moving on And you know what it is Is just you that I'm choosing Cause it gets so murky Loving what you know And I'll I'll keep on swimming Further up the road And it gets so murky Loving what you know And I’ll, I'll keep on swimming Further up the road Swimming I'm swimming Further up the road And I'll keep on swimming Swimming Further up the road (Vocalizing) (Cheers and applause) Holly Herndon: Thank you. Thank you.",PT09M24S,2022-09-07T14:51:35Z
Is humanity smart enough to survive itself?,"technology,future,history,AI,humanity",Jeanette Winterson,"With quick wit and sharp insight, writer Jeanette Winterson lays out a vision of the future where human and machine intelligence meld -- forming what she calls ""alternative intelligence"" -- and takes a philosophical look at our species, asking: Are we smart enough to survive how smart we are? (Followed by a Q&A with TED's head of curation Helen Walters)","Here's what gives me hope for humanity. I believe that we can change our human nature for the better. This chance is unique. It's where we stand in history right now. No other generation has stood here before us. Homo sapiens has been around for about 300,000 years. Other humanoid life forms have come and gone. We find their traces. We search for their stories. But we are the success story. We've survived natural disasters, famines, floods, earthquakes, plagues, woolly mammoths, mess-ups of our own making. We're smart, no question. The question is: Are we smart enough to survive how smart we are? It's not looking so good just now, is it? (Laughter) We stand facing the possibility of global conflict. If that happens, it will be our Third World War in not much more than 100 years. If Putin's violence stops today, the problem doesn't go away. As across the globe dictatorism threatens democracy. I wonder whether as a species we can survive any of this, even if those threats disappear. Those living inside the snow globe of their magical thinking will not be insulated against the facts of climate breakdown. And yet, still, we go on heating up the planet. Still we go on polluting the earth. Still we despoil our precious natural resources. 250 years ago, we kick-started the Industrial Revolution. The machine age. It's when we first hear the buzzwords of the modern moment. Disruption, acceleration. That's Karl Marx, actually. Acceleration of production, the factory system, acceleration of transport, the coming of the railways. No need for ships to wait for the wind. Coal-fired, steam-powered. Acceleration of information. The global village. And it's when we start digging fossil fuels out of the ground in planet-changing quantities, when we start pushing carbon dioxide into the atmosphere. In a salami slice of space-time, human beings have changed the way that we live on the planet forever. We've moved out of the agricultural economies of our evolutionary inheritance into the industrial economies and beyond of where we are now. There's no one else for us to turn to. There's no one else for us to blame. There is no us and them. There's only us. So will we continue to be the success story of the known universe, or are we writing our own obituary? The suicide species? Now I was raised in an evangelical household. We lived in end time. We were waiting for things to get so bad that Jesus would come back and save us. The apocalypse. It's what the prepper communities are, well, prepping for. And it's why super rich white guys are buying up tracts of land, hoping that they can live inside a kind of wi-fi-enabled Noah's Ark. (Laughter) Well, we can have the end time if we want it. We know those stories from our religions, our sci-fi, our movies. We are uniquely placed to bring on the apocalypse. And we're uniquely placed to save ourselves, too. If we could accept that as a species, Homo sapiens needs to evolve further. And that's what gives me hope, because we have the means to evolve further. And that's AI. Now look, I don't mean the geeks will inherit the earth. I'm sorry, geeks. You won't. (Laughter) I don’t really want to talk about narrow- goal artificial intelligence at all. That term of John McCarthy’s, “artificial intelligence,” is it any use to us now? I'd rather call it alternative intelligence. And I think humankind is in need of some alternative intelligence. In 1965, Jack Good talked about AI as our last invention. He meant superintelligence, the kind of thing that worries Bill Gates and Elon Musk. You know, the ""Terminator"" scenario. The final us and them. But I think that's all to do with our doomster mindset. We don't have to vote for the apocalypse. Jack Good worked at Bletchley Park with Alan Turing during World War II, building the early computing machinery that would crack the Nazi Enigma code. Now after the war, Turing, wrestling with the problems of a stored, programmed computer had a bigger dream on his mind. And in 1950, Alan Turing published a paper called “Computing Machinery and Intelligence.” And in there, there's a chapter that's titled ""Lady Lovelace's Objection"" where Turing time-travels back 100 years to have a conversation with that long-dead genius, Ada Lovelace, the first person to write mathematical programs for the computer not yet built by her friend Charles Babbage. Now Ada wrote that as well as doing awesome stuff with numbers, the computer would, if correctly programmed, be able to write novels and compose music. That is a pretty big insight in 1843. ""But,"" said Ada, ""The computer should never have any pretensions to originate anything."" She meant, think creatively. Well, Ada's father was Lord Byron, England's most famous poet, and England is the land of Shakespeare. More poets. And Ada had seen Charles Babbage's kit all over the floor, and she wasn’t having some steampunk, coal-fired nuts, bolts, bezels, levers, gears, cogs and chains, crank-handled machine writing poetry. ""Well,"" said Alan Turing, ""Was Lady Lovelace, correct? Could a computer ever be said to originate anything? And what would be the difference between computing intelligence and human intelligence?"" Well, I'll tell you one difference, and it's optimistic. Computing power uses binary, but computing intelligence is nonbinary. It's humans who are obsessed with false binaries. Male, female. Masculine, feminine. Black, white. Human, non-human. Us, them. AI has no skin color. AI has no race, no gender, no faith in a sky God. AI is not interested in men being superior to women, in white folks being smarter than people of color. Straight, gay, gay, trans are not separating categories for AI. AI does not distinguish between success and failure by gold bars, yachts and Ferraris. AI is not motivated by fame and fortune. If we develop alternative intelligence, it will be Buddhist in its non-needs. (Applause) Now I am aware that the algorithms ubiquitous in everyday life are racist, sexist, gendered, trivializing, stoke division, amplify bias. But what is this teaching us about ourselves? AI is a tool. We are the ones who are using the tool. Hatred and contempt, money and power are human agendas, not AI agendas. We've been forced to recognize the paucity, the inadequacy of our data sets. And humans are trained on data sets too. We've had to recognized the unacknowledged ideologies that we live by every day. Rationality, neutrality, logic, objective decision-making. What can we say about any of that when we see what we are reflected back to us in the small screen? And it's not a pretty sight. AI is not yet self-aware. But we are becoming more self-aware as we work with AI and we realize that Homo sapiens is no longer fit for purpose. We need a reboot. So what are we going to choose? Apocalypse or an alternative? I believe that humans have a strong future as a hybrid species as we start to merge with the biotechnology we're creating, whether that's nanobots in the bloodstream, monitoring our vital systems, whether it's genetic editing, whether it's 3D printing of bespoke body parts, whether it’s neural implants that will connect us directly to the web and to one another, insourcing information, enhancing our cognitive capacities. And if we manage to upload consciousness, I think that the shift from the transhuman to the posthuman world will seem natural, an evolutionary necessity. Why do I say that? I say that because for millennia, all human beings have been obsessed with the big question, the absurdity of death. We asked, ""Do we have souls?"" We watched the spirit wait to leave the body. We created the world's first disruptive startup, the afterlife. (Laughter) Now a multinational company, with a vast VR real estate portfolio. A mansion in the sky, sir? Our earliest extant written narrative, The Epic of Gilgamesh, is a journey to discover if there is life after death. And what is life after death? It is the extension of the self beyond biological limits. Now I’m a writer, and I wonder, have we been telling the story backwards? Did we know we would always get here, capable of creating the kind of superintelligence that we said created us? We're told were made in God's image. God is immortal. God is not a biological entity. Since the 17th century, the Enlightenment, science and religion have parted company. And science said, all that God thinking, the afterlife stuff, it's folly, It's ignorance, it's superstition. Suppose it was intuition. Suppose it was the only way we could talk about what we knew, a fundamental, deep truth that this is not the last word. This is not the end of the story. That we are not time-bound creatures caught in our bodies. That there is further to go. I'm fascinated that computing, science and religion, like parallel lines that do meet in space, are now asking the same question: Is consciousness obliged to materiality? Now ... I accept that machine intelligence will challenge human intelligence. But the mythos of the world is built around a group of stories that showcase an encounter between a human and a nonhuman entity. Think of Jacob wrestling the angel. Think of Prometheus bringing fire down from the gods. In these encounters, both parties are changed, not always for the better. But it generally works out. We've been thinking about this stuff forever. It's time that we created it. And we could have some fun stuff too. Who wants their own AI angel? Me. There's a message in a bottle about this 200 years ago. When Ada Lovelace was busy getting born, her father, Lord Byron, was on holiday on Lake Geneva with his friend, the poet Percy Shelley, and Shelley's wife, Mary Shelley. On a wet weekend with no internet Byron said -- (Laughter) ""Let's write horror stories."" You know what happened. Out of that came the world's most famous monster, Frankenstein. This is 1816, the start of the Industrial Revolution. Mary Shelley is just 19 years old. In that novel, there is an alternative intelligence made out of the body parts from the graveyard and electricity. An astonishing vision because electricity was not in any practical use at all and was hardly understood as a force. You know what happens. The monster is not named. Not educated. Is outcast by his panicky creator, Victor Frankenstein. And the whole thing ends in a chase across the Arctic ice towards a Götterdämmerung of death and destruction. The death wish that human beings are so drawn to, perhaps because it's easier to give up than to carry on. Well, we're the first generation who can read Mary Shelley's novel in the right way, as a flare flung across time, because we too could create an alternative intelligence, not out of the body parts from the graveyard using electricity, but out of zeros and ones of code. And how is this going to end? Utopia or dystopia? It's up to us. Endings are not set in stone. We change the story because we are the story. Now, Marvin Minsky called alternative intelligence our ""mind children."" Could we as proud parents accept that the new generation that we will create will be smarter than we are? And could we accept that the new generation we create need not be on a substrate made of meat? Thank you. (Applause) Thank you. Thank you very much. Thank you. Thank you. Helen Walters: Jeanette, stay right there. I have some questions. Jeanette Winterson: I know you want your lunch. Me too. (Laughter) HW: No. Everybody, stop. OK. That was amazing. Thank you. What would you say our odds were of survival? JW: Look -- (Laughter) I'm an optimist. I’m a glass-half-full girl. So I know that we’re running out of time, time is the most precious resource we have, and there isn't much of it. If we get this right soon, it can really work. If we get it wrong, will be fighting each other with sticks and stones for scraps of food and water on an overheated planet in the ruins of dictator-world. But -- (Laughter) OK. But we could get it right. That’s why I feel we’ve arrived at this moment before, and it might disappear back into space-time. Then we'll have to wait billions of years to get here again, which is so dull. So let's not fuck it up. (Laughter) (Applause) HW: I've got another question. I just wanted to have a moment of eye to eye. Don't fuck it up. Don't fuck it up. OK. JW: That’s the message. The whole TED Talk is: “Don’t fuck it up.” HW: But wait, I have another question. JW: I could have saved 12 minutes, 55 seconds. HW: Yeah, well, I'll just be the title that we put online. Jeanette Winterson: “Don’t fuck it up.” OK. I want to talk about love. So in your memoir, which everybody should read, it is called “Why Be Happy When You Can Be Normal?” The very end of that book is about -- it’s a good title -- t’s about love. And you write, ""Love, the difficult word. Where everything starts, where we always return. Love. Love's lack. The possibility of love."" Now I don't want to bastardize Ada Lovelace. She was all about originating. But what about love? What about alternative intelligence and love? JW: We'll teach it. HW: We’ll teach it? JW: Yeah. And listen, any of you who ever fell in love with your teddy bear, which is all of you, know what it's like to have an intense relationship with a nonbiological lifeform. (Laughter) HW: Jeanette Winterson, I love you. JW: Thank you. HW: Thank you so much. Thank you.",PT18M24S,2022-08-23T14:31:42Z
Intelligent floating machines inspired by nature,"technology,computers,robots,creativity,art,future,AI,machine learning",Anicka Yi,"Taking cues from soft robotics and the natural world, conceptual artist Anicka Yi builds lighter-than-air machines that roam and react like autonomous life forms. Her floating ""aerobes"" inspire us to think about new ways of living with machines -- and to ponder how they could evolve into living creatures. ""What if our machines could be more than just our tools, and instead, a new type of companion species?"" she asks.","It’s wonderful to be in a room with so many humans after all this time. In the last few years, I've been keeping company with a new species that happens to live in the air. And I'd like you all to meet my aerobe friend. (Music) (Music ends) (Cheers and applause) I get emotional every time I see it fly. In my work as an artist, I ask a lot of questions about the world around me. While the problems of the world don't have easy solutions, I think it's as important to ask the right questions. Why do our technologies instill so much fear in us? Why do our lives today feel so alienating, when our technologies are supposed to improve our lives? And why do we feel so disconnected, when our inventions are meant to connect each and every one of us? My artwork deals with evolution, biology and the senses. I'm interested in how organisms are composed of different life-forms, such as bacteria, fungi and viruses ... but also how different chemical and molecular interactions can happen whenever we simply take a breath. So I've always found modern technology to be quite limiting because of how cold and flat it is. But what if our machines could be more holistic? And what if our machines could relate to us in a more holistic way? And what if our machines could be more than just our tools, and instead, a new type of companion species? I believe we need to embrace who we are as symbiotic living beings, and design machines to reflect this. What if the world was populated by machines that were more like animals and plants, instead of something you'd find in a factory? This exploration could teach us so much more about life, in all its vast complexities. These ideas might sound like mere science fiction, but my mission as an artist is to create possibilities of other worlds, and other ways of living and being, even if just for a moment. So when I was invited to imagine a large-scale installation at the Tate Modern in London, I decided to make this world I'm describing a reality. The museum space is incredibly vast, and I knew right away that I wanted to transform it into an aquarium of machines. I asked myself, ""What would it feel like to live in the world with machines that could live in the wild and evolve on their own?"" To explore this question, I created two new machine species that I call ""aerobes."" I released 12 aerobes into the museum space so they could form their own small ecosystem. In order to minimize our human-centric biases, and to steer clear of simply mimicking the human form, I worked with my team to look at the natural world, and we were inspired by creatures like the comb jellyfish and the lion's mane mushroom. We also looked deeply into the field of soft robotics, and ultimately brought the aerobes to life by designing them with fluid movement and lighter-than-air construction. When you look at these aerobes, it gives you a feeling almost opposite to the uncanny valley. You know that they're mechanical, yet they feel palpably alive. You feel like you might be next to some majestic and remote life-form. Like swimming next to a humpback whale. Their size is imposing ... yet they inspire an emotion closer to awe than to fear. You sense that they're almost living, and it makes you want to embrace and coexist with them, to see them truly succeed. I wanted the aerobes to have a sense of freedom and unpredictability, so I worked with my team to create what is called an artificial life simulation. This software allows each aerobe to develop unique behaviors and to respond autonomously to its environment. The aerobes evolve their personalities as they interact and learn from each other. The more we flew them, the more we got to know their distinct personalities and their gentle nature. It was very important for me that the aerobes had their own sensory world. Most AI functions like a mind without a body. But all living creatures, from the humble tick to the stealthy panther, learn about the world through their bodies and their senses. These aerobes perceive each other through high-frequency radio waves, and they use thermal sensing to detect other living creatures in their environment. An aerobe might be curious about your heat signature, and fly over to greet and flirt with you, while other aerobes are more shy and might go out of their way to avoid a crowd. How will these machines and humans be together in the world? What will this world look like? This world may see machines and humans generating new intimacies and alternate perspectives. My hope is that these aerobes will help us appreciate a more diverse way of living and being. Their purpose is not to compete with us or to dominate us, but to ask what a more compatible future could look like. We all have a stake in this conversation about technologies. Can we actively align machines to better reflect our biological reality? I sincerely hope that you'll consider that machines can be so much more than what we've known, and how they might embody a wisdom drawn from biology. This project is a project for everybody, to imagine different worlds and different futures. It's not too late to change machine evolution and to consider new goals, where machines and humans can coexist in a more compassionate way. I believe this is where we can start the conversation. (Applause) (Music) (Music ends) Thank you. (Cheers and applause)",PT10M13S,2022-08-19T14:54:11Z
Education is a fundamental right for every child,"global issues,education,refugees,teaching",Makhtoum Abdalla,"For children growing up in refugee camps, education is a powerful tool of liberation. In this inspiring talk, Makhtoum Abdalla, displaced as a child in Sudan and now living with his family in the Otash camp in Darfur, shares his biggest dream: to ensure all children are educated and taught the skills needed to become ""captains of their destiny.""","Hello, I am Makhtoum Abdalla from Sudan, born in 2004, in Nyala, South Darfur. And on the map, the western part of Sudan is where you find Darfur. For over a decade, the state was affected [by] conflict, which we were still reeling from. And as a result, since I was four years old, my family have lived in a camp for internally displaced people: an IDP camp for families that have been driven from their homes for one reason or another, to another part of the country. My family was displaced because of conflict, and I was born in a world full of complexity, loss and deprivation. But despite that, my family and I are still here full of hopes and dreams for a better tomorrow. Before the camp, we used to live in a place called Otash village in South Darfur. And as the word “otash” comes from the Arabic word “A’atshan”, which means thirsty. And it has been called so for the lack of water resources in that area. And so the name of our IDP camp is Otash camp. Same name, same issue. It's home to almost 100,000 people from different villages. I was very young when we left our home, but I can remember the fear on my family's face as they're getting ready, leaving everything behind. I could see their pain, especially on my mother's face. I still remember. My family were one of the thousands of families that fled. Not everyone was lucky, some lost their lives while trying to escape. Because the journey was so dangerous, we had to hide frequently along the road. Where we used to live is only two hours away, but the journey took two days until we finally reached a safe place, the place that today we call Otash IDP camp. During the two days it took my family to the camp, we had nothing to eat and only a little water. My baby brother died from malnutrition. And he was not even two years old. My brother's passing affected me a lot. That's why I decided to go to college to study nutrition and medicine. Just to help others and find ways to prevent what happened to him. In a world with so much food and so much prosperity, no one should ever die of hunger. I feel like I owe it to my baby brother to be better and do better, so I can help people like us, whether from Darfur or anywhere in the world. I have big dreams, and I know education is my path to reaching them. Education is a basic right for every child. And it is the way each of us can get skills, tools and power to make change in our lives and in the world. Education is a basic right for all children, especially for those who have grown up only knowing violence and conflict. And education, also, is the way we make our future more secure and our present more peaceful. A generation needs power to make change. And the greatest power that's ever known to all is the power of knowledge. When I read books, I feel like I'm no longer in my tiny room made of hay without electricity in the camp. I feel like I'm confident enough to say that I'm the captain of my destiny. From my room in the camp, I've been able to travel through books to Cairo, London and France, and all the way to Columbia University in New York. And I know what most people may think about me, that I'm an internally displaced person. That may be true, but I am more than that. I am patience, I'm hope, dreams, and with education, I can get where I want to be. Education in an IDP camp is hard, and Otash has certainly had its ups and downs. At the beginning, there were no schools nearby. Now there are schools, but they are not always free. And we don't always have access to electricity or access to internet like schools in many other countries. But education should be free for every child, and access to quality education should be a priority and available for all children. Meanwhile, as more people in Darfur have been displaced, the schools inside our camp have become overcrowded. But still, they have big dreams too. And in my state primary school exams, I came in third in the ranking out of thousands of students, which was a huge achievement for me. Soon I will take another test at the end of my secondary school, and then I hope to progress onward to University, moving step by step closer to my ultimate goal. Along the way, I've learned to speak three languages. I speak fluently in Arabic, English and Turkish. And recently, I started to learn French. (French) J’ai de grands rêves. (English) Learning languages  makes me feel powerful, as they challenge my mind. And it also makes me feel I can communicate with other people, which is important to a connected and equitable world. And also, it's one of the many ways that can bring us all together as humans. Because of my academic achievement, a few years ago, UNICEF Sudan wrote an article about me in which I shared my biggest dream to one day go to Columbia University and study medicine. And now today I am one of UNICEF Sudan Youth Advocates, championing the rights of other children to access quality education. I told you, I have big dreams. Since becoming a Youth Advocate with UNICEF Sudan, I've been emphasizing the importance of peace and education. And I see how youth in the camp look up to me and how my parents and siblings ooze with pride with the opportunities I have been given, which makes me feel proud. And I love seeing everyone in the camp wants also to fight for education. Recently, I had the opportunity to fly to the capital of Sudan, Khartoum. It was the first time for me to be on an airplane and first time also to leave Nyala. I even had the opportunity to meet the prime minister of Sudan, along with other youth representing every state in Sudan, which was truly overwhelming for me. I, Makhtoum, from Otash IDP camp, with the Prime Minister of Sudan. But it was also an eye-opener. I connected with other youth who came to the capital with a heart full of hope and a mind filled with determination, wanting to share their stories from their parts in their country. I realized that, despite our different cultural backgrounds, we were all one. We have the same plight and needs. But we all wanted one thing: peace and fair treatment and access to services and opportunities. And this must be true with every child around the globe. The visit also reminded me that there is so much unfairness in this world. As I was lying in my hotel room with A/C on and electricity, I thought of how I struggle in the camp to study and to sleep because of lack of electricity. When I study, I rely on flashlights. I can’t help but ask the question: Why? I realized that I'm in so many ways so lucky because I have opportunities to succeed in life, and I have the great privilege to continue to increase awareness about education for all children like me and across the world. Imagine if they were given the skills, power and knowledge. Imagine the difference they would make in their life and in our world. They have big dreams, too. Education, just like electricity, shouldn't just be for some, it's a necessity and a right. Just like hopes, dreams and opportunity. Shining a way forward for each of us and, in turn, allowing us to make the world better for others. I am Makhtoum Abdalla from Otash IDP camp, but I am so much more than that. I say it proudly. With the power of education, I can make the world a better place. We all can. Thank you.",PT09M00S,2021-12-10T15:36:48Z
A path to higher education and employment for refugees,"global issues,education,Africa,refugees,teaching,Audacious Project",Chrystina Russell,"Out of the more than 70 million displaced people worldwide, only three percent have access to higher education. The Global Education Movement (GEM) is on a mission to change that with the first large-scale initiative of its kind to help refugee learners get bachelor's degrees and create pathways toward employment. Hear from students and the program's executive director, Chrystina Russell, about how GEM's flexible, competency-based model sets graduates up for success and empowerment wherever they are.","Saida Aden Said: I still have this horrific image in my mind. I could see people falling down, gunshots. I was so terrified. Really, I was crying a lot. Someone who knew my father and my mom grabbed my hand, and he said, ""Let's go! Let's go! Let's go!"" And I was like, ""Where's my mom? My mom? My mom?"" Noria Dambrine Dusabireme: During nights we would hear shots, we would hear guns. Elections were supposed to happen. We had young people going in the street, they were having strikes. And most of the young people died. SAS: We boarded a vehicle. It was overloaded. People were running for their lives. That is how I fled from Somalia. My mom missed me. Nobody told her where I went. NDD: The fact that we did not go to school, we couldn't go to the market, we were just stuck home made me realize that if I got an option to go for something better, I could just go for it and have a better future. (Music) Ignazio Matteini: Globally, displaced people in the world have been increasing. Now there are almost 60 million people displaced in the world. And unfortunately, it doesn't stop. Chrystina Russell: I think the humanitarian community is starting to realize from research and reality that we're talking about a much more permanent problem. Baylie Damtie Yeshita: These students, they need a tertiary education, a degree that they can use. If the students are living now in Rwanda, if they get relocated, still they can continue their study. Still, their degree is useful, wherever they are. CR: Our audacious project was to really test Southern New Hampshire University's Global Education Movement's ability to scale, to bring bachelor's degrees and pathways to employment to refugees and those who would otherwise not have access to higher education. SAS: It was almost impossible, as a refugee person, to further my education and to make my career. My name is Saida Aden Said, and I am from Somalia. I was nine years old when I came to Kakuma, and I started going to school at 17. Now I am doing my bachelor degree with SNHU. NDD: My name is Noria Dambrine Dusabireme. I'm doing my bachelor of arts in communications with a concentration in business. CR: We are serving students across five different countries: Lebanon, Kenya, Malawi, Rwanda and South Africa. Really proud to have 800 AA grads to over 400 bachelor's graduates and nearly 1,000 students enrolled right now. So, the magic of this is that we're addressing refugee lives as they exist. There are no classes. There are no lectures. There are no due dates. There are no final exams. This degree is competency-based and not time-bound. You choose when you start your project. You choose how you're going to approach it. NDD: When you open the platform, that's where you can see the goals. Under each goal, we can find projects. When you open a project, you get the competencies that you have to master, directions and overview of the project. CR: The secret sauce of SNHU is combining that competency-based online learning with the in-person learning that we do with partners to provide all the wraparound supports. That includes academic coaching. It means psychosocial support, medical support, and it's also that back-end employment support that's really resulting in the 95 percent graduation, the 88 percent employment. NDD: I'm a social media management intern. It's related to the communications degree I'm doing. I've learned so many things out of the project and in the real world. CR: The structured internship is really an opportunity for students to practice their skills, for us to create connections between that internship and a later job opportunity. (Music) This is a model that really stops putting time and university policies and procedures at the center and instead puts the student at the center. IM: The SNHU model is a big way to shake the tree. Huge. It's a huge shake to the traditional way of having tertiary education here. BDY: It can transform the lives of students from these vulnerable and refugee communities. NDD: If I get the degree, I can just come back and work everywhere that I want. I can go for a masters confidently in English, which is something that I would not have dreamt of before. And I have the confidence and the skills required to actually go out and just tackle the workplace without having to fear that I can't make it. SAS: I always wanted to work with the community. I want to establish a nonprofit. We advocate for women's education. I want to be someone who is, like, an ambassador and encourage them to learn and tell them it is never too late. It's a dream.",PT05M34S,2020-06-19T00:33:37Z
Why I fight for the education of refugee girls (like me),"global issues,education,social change,community,TEDx,refugees,teaching,kids",Mary Maker,"After fleeing war-torn South Sudan as a child, Mary Maker found security and hope in the school at Kenya's Kakuma Refugee Camp. Now a teacher of young refugees herself, she sees education as an essential tool for rebuilding lives -- and empowering a generation of girls who are too often denied entrance into the classroom. ""For the child of war, an education can turn their tears of loss into a passion for peace,"" Maker says.","We do not choose where to be born. We do not choose who our parents are. But we do choose how we are going to live our lives. I did not choose to be born in South Sudan, a country rife with conflict. I did not choose my name -- Nyiriak, which means ""war."" I've always rejected it and all the legacy it was born into. I choose to be called Mary. As a teacher, I've stood in front of 120 students, so this stage does not intimidate me. My students come from war-torn countries. They're so different from each other, but they have one thing in common: they fled their homes in order to stay alive. Some of them belong to parents back home in South Sudan who are killing each other because they belong to a different tribe or they have a different belief. Others come from other African countries devastated by war. But when they enter my class, they make friends, they walk home together, they do their homework together. There is no hatred allowed in my class. My story is like that of so many other refugees. The war came when I was still a baby. And my father, who had been absent in most of my early childhood, was doing what other men were doing: fighting for the country. He had two wives and many children. My mother was his second wife, married to him at the age of 16. This is simply because my mother came from a poor background, and she had no choice. My father, on the other hand, was rich. He had many cows. Gunshots were the order of the day. My community was constantly under attack. Communities would fight each other as they took water along the Nile. But that was not all. Planes would drop the spinning and terrifying bombs that chopped off people's limbs. But the most terrifying thing for every single parent was to see their children being abducted and turned into young soldiers. My mother dug a trench that soon became our home. But yet, we did not feel protected. She had to flee in search of a safe place for us. I was four years old, and my younger sister was two. We joined a huge mass of people, and together we walked for many agonizing days in search of a secure place. But we could barely rest before we were attacked again. I remember my mother was pregnant, when she would take turns to carry me and my younger sister. We finally made it across the Kenyan border, yes. But that was the longest journey that I have ever had in my whole life. My feet were raw with blisters. To our surprise, we found other family members who had fled into the camp earlier on, where you all are today, the Kakuma camp. Now, I want you all to be very quiet just for a moment. Do you hear that? The sound of silence. No gunshots. Peace, at last. That was my first memory of this camp. When you move from a war zone and come to a secure place like Kakuma, you've really gone far. I only stayed in the camp for three years, though. My father, who had been absent in most of my early childhood, came back into my life. And he organized for me to move with my uncle to our family in Nakuru. There, I found my father's first wife, my half sisters and my half brothers. I got enrolled in school. I remember my first day in school -- I could sing and laugh again -- and my first set of school uniforms, you bet. It was amazing. But then I came to realize that my uncle did not find it fit for me to go to school, simply because I was a girl. My half brothers were his first priority. He would say, ""Educating a girl is a waste of time."" And for that reason, I missed many days of school, because the fees were not paid. My father stepped in and organized for me to go to boarding school. I remember the faith that he put in me over the couple of years to come. He would say, ""Education is an animal that you have to overcome. With an education, you can survive. Education shall be your first husband."" And with these words came in his first big investment. I felt lucky! But I was missing something: my mother. My mother had been left behind in the camp, and I had not seen her since I left it. Six years without seeing her was really long. I was alone, in school, when I heard of her death. I've seen many people back in South Sudan lose their lives. I've heard from neighbors lose their sons, their husbands, their children. But I never thought that that would ever come into my life. A month earlier, my stepmother, who had been so good to me back in Nakuru, died first. Then I came to realize that after giving birth to four girls, my mother had finally given birth to something that could have made her be accepted into the community -- a baby boy, my baby brother. But he, too, joined the list of the dead. The most hurting part for me was the fact that I wasn't able to attend my mother's burial. I wasn't allowed. They said her family did not find it fit for her children, who are all girls, to attend her burial, simply because we were girls. They would lament to me and say, ""We are sorry, Mary, for your loss. We are sorry that your parents never left behind any children."" And I would wonder: What are we? Are we not children? In the mentality of my community, only the boy child counted. And for that reason, I knew this was the end of me. But I was the eldest girl. I had to take care of my siblings. I had to ensure they went to school. I was 13 years old. How could I have made that happen? I came back to the camp to take care of my siblings. I've never felt so stuck. But then, one of my aunts, Auntie Okoi, decided to take my sisters. My father sent me money from Juba for me to go back to school. Boarding school was heaven, but it was also so hard. I remember during the visiting days when parents would come to school, and my father would miss. But when he did come, he repeated the same faith in me. This time he would say, ""Mary, you cannot go astray, because you are the future of your siblings."" But then, in 2012, life took away the only thing that I was clinging on. My father died. My grades in school started to collapse, and when I sat for my final high school exams in 2015, I was devastated to receive a C grade. OK, I keep telling students in my class, ""It's not about the A's; it's about doing your best."" That was not my best. I was determined. I wanted to go back and try again. But my parents were gone. I had no one to take care of me, and I had no one to pay that fee. I felt so hopeless. But then, one of my best friends, a beautiful Kenyan lady, Esther Kaecha, called me during this devastating moment, and she was like, ""Mary, you have a strong will. And I have a plan, and it's going to work."" OK, when you're in those devastating moments, you accept anything, right? So the plan was, she organized some travel money for us to travel to Anester Victory Girls High School. I remember that day so well. It was raining when we entered the principal's office. We were shaking like two chickens that had been rained on, and we looked at him. He was asking, ""What do you want?"" And we looked at him with the cat face. ""We just want to go back to school."" Well, believe it or not, he not only paid our school fees but also our uniform and pocket money for food. Clap for him. (Applause) When I finished my high school career, I became the head girl. And when I sat for the KCSE for a second time, I was able to receive a B minus. Clap. (Applause) Thank you. So I really want to say thank you to Anester Victory, Mr. Gatimu and the whole Anester fraternity for giving me that chance. From time to time, members of my family will insist that my sister and I should get married so that somebody will take care of us. They will say, ""We have a man for you."" I really hate the fact that people took us as property rather than children. Sometimes they will jokingly say, ""You are going to lose your market value the more educated you become."" But the truth is, an educated woman is feared in my community. But I told them, this is not what I want. I don't want to get kids at 16 like my mother did. This is not my life. Even though my sisters and I are suffering, there's no way we are heading in that direction. I refuse to repeat history. Educating a girl will create equal and stable societies. And educated refugees will be the hope of rebuilding their countries someday. Girls and women have a part to play in this just as much as men. Well, we have men in my family that encourage me to move on: my half brothers and also my half sisters. When I finished my high school career, I moved my sisters to Nairobi, where they live with my stepsister. They live 17 people in a house. But don't pity us. The most important thing is that they all get a decent education. The winners of today are the losers of yesterday, but who never gave up. And that is who we are, my sisters and I. And I'm so proud of that. My biggest investment in life -- (Applause) is the education of my sisters. Education creates an equal and fair chance for everyone to make it. I personally believe education is not all about the syllabus. It's about friendship. It's about discovering our talents. It's about discovering our destiny. I will, for example, not forget the joy that I had when I first had singing lessons in school, which is still a passion of mine. But I wouldn't have gotten that anywhere else. As a teacher, I see my classroom as a laboratory that not only generates skills and knowledge but also understanding and hope. Let's take a tree. A tree may have its branches cut, but give it water, and it will grow new branches. For the child of war, an education can turn their tears of loss into a passion for peace. And for that reason, I refuse to give up on a single student in my class. (Applause) Education heals. The school environment gives you a focus to focus ahead. Let's take it this way: when you're busy solving mathematical equations, and you are memorizing poetry, you forget the violence that you witnessed back home. And that is the power of education. It creates this place for peace. Kakuma is teeming with learners. Over 85,000 students are enrolled in schools here, which makes up 40 percent of the refugee population. It includes children who lost years of education because of the war back home. And I want to ask you a question: If education is about building a generation of hope, why are there 120 students packed in my classroom? Why is it that only six percent of the primary school students are making it to high school, simply because we do not have enough places for them? And why is it that only one percent of the secondary school graduates are making it to university? I began by saying that I am a teacher. But once again, I have become a student. In March, I moved to Rwanda on a scholarship program called ""Bridge2Rwanda."" It prepares scholars for universities. They are able to get a chance to compete for universities abroad. I am now having teachers telling me what to do, instead of the other way round. People are once again investing in me. So I want to ask you all to invest in young refugees. Think of the tree that we mentioned earlier. We are the generation to plant it, so that the next generation can water it, and the one that follows will enjoy the shade. They will reap the benefits. And the greatest benefit of them all is an education that will last. Thank you. (Applause)",PT16M37S,2018-08-15T15:01:13Z
Food revolutionaries,"global issues,education,food,health,TED Prize,teaching",Jamie Oliver,"Jamie Oliver crystallized his vision for a food revolution in his TED Talk. At the Charlton Manor Primary School in London, head teacher Timothy Baker is putting his idea into action by offering not just food education but education centered on food.","Jamie Oliver: My wish ... is for you to help a strong, sustainable movement to educate every child about food. (Music) To inspire families to cook again and to empower people everywhere to fight obesity. I came here to start a food revolution that I profoundly believe in. (Applause) [Great Big Story in partnership with TED] Narrator: They had a big idea to change the world. But they couldn't do it alone. (Voices overlapping) So, my wish ... My wish ... I wish ... And now here's my wish. [Torchbearers] [Ideas in action] (Knife chop) (Music) JO: Food is simple. It's just raw ingredients. But it's the most powerful killer on the planet. Every child has the human right to be taught about food: where it comes from, how it affects their body. And they should be shown at school, because it's at the front line of the fight against obesity. [London, England] [Charlton Manor Primary School] What happens at Charlton Manor is that incredible head teacher took it to the next level. [Timothy Baker Head teacher] Timothy Baker: In the past, the children weren't eating the right things. I've been inspired by Jamie to educate this school about the fact that we're feeding the children the wrong food. And I thought, well, the timetable is an already crowded place -- there are so many lessons in the primary curriculum. How can you introduce another subject for teachers to teach? So we looked at English, we looked at maths, science, history, geography and we saw how we could put that around food. (Children laugh) Elizabeth: When you incorporate cooking, it's something everyone looks forward to. Male teacher: Today we're doing a little bit of science in the kitchen. Female teacher: We are going to combine our lessons on Diwali, but also our lessons on shape and symmetry. Male teacher: Is this a physical change or a chemical change? Children: Physical! Male teacher: You're right. TB: For history topics, we talk about the history of chocolate, and so we can do a whole topic around that. Male teacher: And the Aztecs have been cooking with chocolate -- TB: And it's interesting because it's not made as they think, with all the milk in that they would have had -- and the taste is very, very different. Some children like it; some children don't. (Laughter) For maths, simple weighing and measuring. Female teacher: We're going to be doing a lot of measuring because we have to measure out some liquids, which we did last term, we did some measuring. JO: What he's done is he's put food at the heart of the school, and he's fed the stomach and the mind. (Children yell excitedly) TB: Charlton Manor is a state school. About 80 percent of children come from areas that are identified as in poverty. The children had very little experience of being outside in the countryside, knowing about food-growing. Over a period of time, we were able to build a garden. (Children shout) Students: Welcome to the Secret Garden! Kehinde: This is our greenhouse. This is our compost bin. This is our wormery. (Music) This is our vegetable patch. And these are our chickens. Sean: The chickens come out, and they try and chase you. I had to run for my life. TB: Up at the community garden, we've got two polytunnels so we can grow year-round. Sean: I will pick onions, broccoli and carrots because they're all healthy, they make you stronger -- obviously -- and they just make me happy. So, yeah. (Birds chirp) TB: 12 or 13 years ago, there was a reluctance to engage in this sort of curriculum. There were many people that couldn't see what we were aiming for and what we were trying to do. Obesity hadn't reached the epidemic proportions it has reached now. We're getting children that look and appear more healthy. Concentration is so much higher. Behavior issues are incredibly lower than they have been in the past. Elizabeth: One the of the great things they've done is introduce us to worlds of healthy food. Kehinde: Before, I was a really picky eater. Sean: I feel better when I eat healthier food. TB: When you fail your maths A level, that's not going to shorten your life by 10 years. JO: You don't die young because you didn't do your geography homework. These kids die young if they don't know how to feed themselves. I think Tim and the team would inspire head teachers across the world -- and parents. Any teacher has the same capacity to be as brilliant as he is. His story, we want to replicate, but the truth is we've got so much more to do. TB: Jamie really revolutionized our school dinners, and it really has hugely impacted all the children, but so many in a deep way which is going to stick with them for the rest of their lives. When you change a life like that, it makes it all worthwhile. From all of us at the Charlton Manor, thank you, Jamie. Student: Thank you, Jamie. Student: Thank you, Jamie. [Join the food revolution JamiesFoodRevolution.org]",PT05M53S,2018-04-03T22:23:19Z
Teachers need real feedback,"culture,global issues,education,teaching",Bill Gates,"Until recently, many teachers only got one word of feedback a year: ""satisfactory."" And with no feedback, no coaching, there's just no way to improve. Bill Gates suggests that even great teachers can get better with smart feedback -- and lays out a program from his foundation to bring it to every classroom.","Everyone needs a coach. It doesn't matter whether you're a basketball player, a tennis player, a gymnast or a bridge player. (Laughter) My bridge coach, Sharon Osberg, says there are more pictures of the back of her head than anyone else's in the world. (Laughter) Sorry, Sharon. Here you go. We all need people who will give us feedback. That's how we improve. Unfortunately, there's one group of people who get almost no systematic feedback to help them do their jobs better, and these people have one of the most important jobs in the world. I'm talking about teachers. When Melinda and I learned how little useful feedback most teachers get, we were blown away. Until recently, over 98 percent of teachers just got one word of feedback: Satisfactory. If all my bridge coach ever told me was that I was ""satisfactory,"" I would have no hope of ever getting better. How would I know who was the best? How would I know what I was doing differently? Today, districts are revamping the way they evaluate teachers, but we still give them almost no feedback that actually helps them improve their practice. Our teachers deserve better. The system we have today isn't fair to them. It's not fair to students, and it's putting America's global leadership at risk. So today I want to talk about how we can help all teachers get the tools for improvement they want and deserve. Let's start by asking who's doing well. Well, unfortunately there's no international ranking tables for teacher feedback systems. So I looked at the countries whose students perform well academically, and looked at what they're doing to help their teachers improve. Consider the rankings for reading proficiency. The U.S. isn't number one. We're not even in the top 10. We're tied for 15th with Iceland and Poland. Now, out of all the places that do better than the U.S. in reading, how many of them have a formal system for helping teachers improve? Eleven out of 14. The U.S. is tied for 15th in reading, but we're 23rd in science and 31st in math. So there's really only one area where we're near the top, and that's in failing to give our teachers the help they need to develop their skills. Let's look at the best academic performer: the province of Shanghai, China. Now, they rank number one across the board, in reading, math and science, and one of the keys to Shanghai's incredible success is the way they help teachers keep improving. They made sure that younger teachers get a chance to watch master teachers at work. They have weekly study groups, where teachers get together and talk about what's working. They even require each teacher to observe and give feedback to their colleagues. You might ask, why is a system like this so important? It's because there's so much variation in the teaching profession. Some teachers are far more effective than others. In fact, there are teachers throughout the country who are helping their students make extraordinary gains. If today's average teacher could become as good as those teachers, our students would be blowing away the rest of the world. So we need a system that helps all our teachers be as good as the best. What would that system look like? Well, to find out, our foundation has been working with 3,000 teachers in districts across the country on a project called Measures of Effective Teaching. We had observers watch videos of teachers in the classroom and rate how they did on a range of practices. For example, did they ask their students challenging questions? Did they find multiple ways to explain an idea? We also had students fill out surveys with questions like, ""Does your teacher know when the class understands a lesson?"" ""Do you learn to correct your mistakes?"" And what we found is very exciting. First, the teachers who did well on these observations had far better student outcomes. So it tells us we're asking the right questions. And second, teachers in the program told us that these videos and these surveys from the students were very helpful diagnostic tools, because they pointed to specific places where they can improve. I want to show you what this video component of MET looks like in action. (Music) (Video) Sarah Brown Wessling: Good morning everybody. Let's talk about what's going on today. To get started, we're doing a peer review day, okay? A peer review day, and our goal by the end of class is for you to be able to determine whether or not you have moves to prove in your essays. My name is Sarah Brown Wessling. I am a high school English teacher at Johnston High School in Johnston, Iowa. Turn to somebody next to you. Tell them what you think I mean when I talk about moves to prove. I've talk about -- I think that there is a difference for teachers between the abstract of how we see our practice and then the concrete reality of it. Okay, so I would like you to please bring up your papers. I think what video offers for us is a certain degree of reality. You can't really dispute what you see on the video, and there is a lot to be learned from that, and there are a lot of ways that we can grow as a profession when we actually get to see this. I just have a flip camera and a little tripod and invested in this tiny little wide-angle lens. At the beginning of class, I just perch it in the back of the classroom. It's not a perfect shot. It doesn't catch every little thing that's going on. But I can hear the sound. I can see a lot. And I'm able to learn a lot from it. So it really has been a simple but powerful tool in my own reflection. All right, let's take a look at the long one first, okay? Once I'm finished taping, then I put it in my computer, and then I'll scan it and take a peek at it. If I don't write things down, I don't remember them. So having the notes is a part of my thinking process, and I discover what I'm seeing as I'm writing. I really have used it for my own personal growth and my own personal reflection on teaching strategy and methodology and classroom management, and just all of those different facets of the classroom. I'm glad that we've actually done the process before so we can kind of compare what works, what doesn't. I think that video exposes so much of what's intrinsic to us as teachers in ways that help us learn and help us understand, and then help our broader communities understand what this complex work is really all about. I think it is a way to exemplify and illustrate things that we cannot convey in a lesson plan, things you cannot convey in a standard, things that you cannot even sometimes convey in a book of pedagogy. Alrighty, everybody, have a great weekend. I'll see you later. [Every classroom could look like that] (Applause) Bill Gates: One day, we'd like every classroom in America to look something like that. But we still have more work to do. Diagnosing areas where a teacher needs to improve is only half the battle. We also have to give them the tools they need to act on the diagnosis. If you learn that you need to improve the way you teach fractions, you should be able to watch a video of the best person in the world teaching fractions. So building this complete teacher feedback and improvement system won't be easy. For example, I know some teachers aren't immediately comfortable with the idea of a camera in the classroom. That's understandable, but our experience with MET suggests that if teachers manage the process, if they collect video in their own classrooms, and they pick the lessons they want to submit, a lot of them will be eager to participate. Building this system will also require a considerable investment. Our foundation estimates that it could cost up to five billion dollars. Now that's a big number, but to put it in perspective, it's less than two percent of what we spend every year on teacher salaries. The impact for teachers would be phenomenal. We would finally have a way to give them feedback, as well as the means to act on it. But this system would have an even more important benefit for our country. It would put us on a path to making sure all our students get a great education, find a career that's fulfilling and rewarding, and have a chance to live out their dreams. This wouldn't just make us a more successful country. It would also make us a more fair and just one, too. I'm excited about the opportunity to give all our teachers the support they want and deserve. I hope you are too. Thank you. (Applause)",PT10M07S,2013-05-08T15:01:20Z
Use data to build better schools,"global issues,education,data,teaching,equality",Andreas Schleicher,"How can we measure what makes a school system work? Andreas Schleicher walks us through the PISA test, a global measurement that ranks countries against one another -- then uses that same data to help schools improve. Watch to find out where your country stacks up, and learn the single factor that makes some systems outperform others.","Radical openness is still a distant future in the field of school education. We have such a hard time figuring out that learning is not a place but an activity. But I want to tell you the story of PISA, OECD's test to measure the knowledge and skills of 15-year-olds around the world, and it's really a story of how international comparisons have globalized the field of education that we usually treat as an affair of domestic policy. Look at how the world looked in the 1960s, in terms of the proportion of people who had completed high school. You can see the United States ahead of everyone else, and much of the economic success of the United States draws on its long-standing advantage as the first mover in education. But in the 1970s, some countries caught up. In the 1980s, the global expansion of the talent pool continued. And the world didn't stop in the 1990s. So in the '60s, the U.S. was first. In the '90s, it was 13th, and not because standards had fallen, but because they had risen so much faster elsewhere. Korea shows you what's possible in education. Two generations ago, Korea had the standard of living of Afghanistan today, and was one of the lowest education performers. Today, every young Korean finishes high school. So this tells us that, in a global economy, it is no longer national improvement that's the benchmark for success, but the best performing education systems internationally. The trouble is that measuring how much time people spend in school or what degree they have got is not always a good way of seeing what they can actually do. Look at the toxic mix of unemployed graduates on our streets, while employers say they cannot find the people with the skills they need. And that tells you that better degrees don't automatically translate into better skills and better jobs and better lives. So with PISA, we try to change this by measuring the knowledge and skills of people directly. And we took a very special angle to this. We were less interested in whether students can simply reproduce what they have learned in school, but we wanted to test whether they can extrapolate from what they know and apply their knowledge in novel situations. Now, some people have criticized us for this. They say, you know, such a way of measuring outcomes is terribly unfair to people, because we test students with problems they haven't seen before. But if you take that logic, you know, you should consider life unfair, because the test of truth in life is not whether we can remember what we learned in school, but whether we are prepared for change, whether we are prepared for jobs that haven't been created, to use technologies that haven't been invented, to solve problems we just can't anticipate today. And once hotly contested, our way of measuring outcomes has actually quickly become the standard. In our latest assessment in 2009, we measured 74 school systems that together cover 87 percent of the economy. This chart shows you the performance of countries. In red, sort of below OECD average. Yellow is so-so, and in green are the countries doing really well. You can see Shanghai, Korea, Singapore in Asia; Finland in Europe; Canada in North America doing really well. You can also see that there is a gap of almost three and a half school years between 15-year-olds in Shanghai and 15-year-olds in Chile, and the gap grows to seven school years when you include the countries with really poor performance. There's a world of difference in the way in which young people are prepared for today's economy. But I want to introduce a second important dimension into this picture. Educators like to talk about equity. With PISA, we wanted to measure how they actually deliver equity, in terms of ensuring that people from different social backgrounds have equal chances. And we see that in some countries, the impact of social background on learning outcomes is very, very strong. Opportunities are unequally distributed. A lot of potential of young children is wasted. We see in other countries that it matters much less into which social context you're born. We all want to be there, in the upper right quadrant, where performance is strong and learning opportunities are equally distributed. Nobody, and no country, can afford to be there, where performance is poor and there are large social disparities. And then we can debate, you know, is it better to be there, where performance is strong at the price of large disparities? Or do we want to focus on equity and accept mediocrity? But actually, if you look at how countries come out on this picture, you see there are a lot of countries that actually are combining excellence with equity. In fact, one of the most important lessons from this comparison is that you don't have to compromise equity to achieve excellence. These countries have moved on from providing excellence for just some to providing excellence for all, a very important lesson. And that also challenges the paradigms of many school systems that believe they are mainly there to sort people. And ever since those results came out, policymakers, educators, researchers from around the world have tried to figure out what's behind the success of those systems. But let's step back for a moment and focus on the countries that actually started PISA, and I'm giving them a colored bubble now. And I'm making the size of the bubble proportional to the amount of money that countries spent on students. If money would tell you everything about the quality of learning outcomes, you would find all the large bubbles at the top, no? But that's not what you see. Spending per student only explains about, well, less than 20 percent of the performance variation among countries, and Luxembourg, for example, the most expensive system, doesn't do particularly well. What you see is that two countries with similar spending achieve very different results. You also see -- and I think that's one of the most encouraging findings -- that we no longer live in a world that is neatly divided between rich and well-educated countries, and poor and badly-educated ones, a very, very important lesson. Let's look at this in greater detail. The red dot shows you spending per student relative to a country's wealth. One way you can spend money is by paying teachers well, and you can see Korea investing a lot in attracting the best people into the teaching profession. And Korea also invests into long school days, which drives up costs further. Last but not least, Koreans want their teachers not only to teach but also to develop. They invest in professional development and collaboration and many other things. All that costs money. How can Korea afford all of this? The answer is, students in Korea learn in large classes. This is the blue bar which is driving costs down. You go to the next country on the list, Luxembourg, and you can see the red dot is exactly where it is for Korea, so Luxembourg spends the same per student as Korea does. But, you know, parents and teachers and policymakers in Luxembourg all like small classes. You know, it's very pleasant to walk into a small class. So they have invested all their money into there, and the blue bar, class size, is driving costs up. But even Luxembourg can spend its money only once, and the price for this is that teachers are not paid particularly well. Students don't have long hours of learning. And basically, teachers have little time to do anything else than teaching. So you can see two countries spent their money very differently, and actually how they spent their money matters a lot more than how much they invest in education. Let's go back to the year 2000. Remember, that was the year before the iPod was invented. This is how the world looked then in terms of PISA performance. The first thing you can see is that the bubbles were a lot smaller, no? We spent a lot less on education, about 35 percent less on education. So you ask yourself, if education has become so much more expensive, has it become so much better? And the bitter truth really is that, you know, not in many countries. But there are some countries which have seen impressive improvements. Germany, my own country, in the year 2000, featured in the lower quadrant, below average performance, large social disparities. And remember, Germany, we used to be one of those countries that comes out very well when you just count people who have degrees. Very disappointing results. People were stunned by the results. And for the very first time, the public debate in Germany was dominated for months by education, not tax, not other kinds of issues, but education was the center of the public debate. And then policymakers began to respond to this. The federal government dramatically raised its investment in education. A lot was done to increase the life chances of students with an immigrant background or from social disadvantage. And what's really interesting is that this wasn't just about optimizing existing policies, but data transformed some of the beliefs and paradigms underlying German education. For example, traditionally, the education of the very young children was seen as the business of families, and you would have cases where women were seen as neglecting their family responsibilities when they sent their children to kindergarten. PISA has transformed that debate, and pushed early childhood education right at the center of public policy in Germany. Or traditionally, the German education divides children at the age of 10, very young children, between those deemed to pursue careers of knowledge workers and those who would end up working for the knowledge workers, and that mainly along socioeconomic lines, and that paradigm is being challenged now too. A lot of change. And the good news is, nine years later, you can see improvements in quality and equity. People have taken up the challenge, done something about it. Or take Korea, at the other end of the spectrum. In the year 2000, Korea did already very well, but the Koreans were concerned that only a small share of their students achieved the really high levels of excellence. They took up the challenge, and Korea was able to double the proportion of students achieving excellence in one decade in the field of reading. Well, if you only focus on your brightest students, you know what happens is disparities grow, and you can see this bubble moving slightly to the other direction, but still, an impressive improvement. A major overhaul of Poland's education helped to dramatically reduce between variability among schools, turn around many of the lowest-performing schools, and raise performance by over half a school year. And you can see other countries as well. Portugal was able to consolidate its fragmented school system, raise quality and improve equity, and so did Hungary. So what you can actually see, there's been a lot of change. And even those people who complain and say that the relative standing of countries on something like PISA is just an artifact of culture, of economic factors, of social issues, of homogeneity of societies, and so on, these people must now concede that education improvement is possible. You know, Poland hasn't changed its culture. It didn't change its economy. It didn't change the compositions of its population. It didn't fire its teachers. It changed its education policies and practice. Very impressive. And all that raises, of course, the question: What can we learn from those countries in the green quadrant who have achieved high levels of equity, high levels of performance, and raised outcomes? And, of course, the question is, can what works in one context provide a model elsewhere? Of course, you can't copy and paste education systems wholesale, but these comparisons have identified a range of factors that high-performing systems share. Everybody agrees that education is important. Everybody says that. But the test of truth is, how do you weigh that priority against other priorities? How do countries pay their teachers relative to other highly skilled workers? Would you want your child to become a teacher rather than a lawyer? How do the media talk about schools and teachers? Those are the critical questions, and what we have learned from PISA is that, in high-performing education systems, the leaders have convinced their citizens to make choices that value education, their future, more than consumption today. And you know what's interesting? You won't believe it, but there are countries in which the most attractive place to be is not the shopping center but the school. Those things really exist. But placing a high value on education is just part of the picture. The other part is the belief that all children are capable of success. You have some countries where students are segregated early in their ages. You know, students are divided up, reflecting the belief that only some children can achieve world-class standards. But usually that is linked to very strong social disparities. If you go to Japan in Asia, or Finland in Europe, parents and teachers in those countries expect every student to succeed, and you can see that actually mirrored in student behavior. When we asked students what counts for success in mathematics, students in North America would typically tell us, you know, it's all about talent. If I'm not born as a genius in math, I'd better study something else. Nine out of 10 Japanese students say that it depends on my own investment, on my own effort, and that tells you a lot about the system that is around them. In the past, different students were taught in similar ways. High performers on PISA embrace diversity with differentiated pedagogical practices. They realize that ordinary students have extraordinary talents, and they personalize learning opportunities. High-performing systems also share clear and ambitious standards across the entire spectrum. Every student knows what matters. Every student knows what's required to be successful. And nowhere does the quality of an education system exceed the quality of its teachers. High-performing systems are very careful in how they recruit and select their teachers and how they train them. They watch how they improve the performances of teachers in difficulties who are struggling, and how they structure teacher pay. They provide an environment also in which teachers work together to frame good practice. And they provide intelligent pathways for teachers to grow in their careers. In bureaucratic school systems, teachers are often left alone in classrooms with a lot of prescription on what they should be teaching. High-performing systems are very clear what good performance is. They set very ambitious standards, but then they enable their teachers to figure out, what do I need to teach to my students today? The past was about delivered wisdom in education. Now the challenge is to enable user-generated wisdom. High performers have moved on from professional or from administrative forms of accountability and control -- sort of, how do you check whether people do what they're supposed to do in education -- to professional forms of work organization. They enable their teachers to make innovations in pedagogy. They provide them with the kind of development they need to develop stronger pedagogical practices. The goal of the past was standardization and compliance. High-performing systems have made teachers and school principals inventive. In the past, the policy focus was on outcomes, on provision. The high-performing systems have helped teachers and school principals to look outwards to the next teacher, the next school around their lives. And the most impressive outcomes of world-class systems is that they achieve high performance across the entire system. You've seen Finland doing so well on PISA, but what makes Finland so impressive is that only five percent of the performance variation amongst students lies between schools. Every school succeeds. This is where success is systemic. And how do they do that? They invest resources where they can make the most difference. They attract the strongest principals into the toughest schools, and the most talented teachers into the most challenging classroom. Last but not least, those countries align policies across all areas of public policy. They make them coherent over sustained periods of time, and they ensure that what they do is consistently implemented. Now, knowing what successful systems are doing doesn't yet tell us how to improve. That's also clear, and that's where some of the limits of international comparisons of PISA are. That's where other forms of research need to kick in, and that's also why PISA doesn't venture into telling countries what they should be doing. But its strength lies in telling them what everybody else has been doing. And the example of PISA shows that data can be more powerful than administrative control of financial subsidy through which we usually run education systems. You know, some people argue that changing educational administration is like moving graveyards. You just can't rely on the people out there to help you with this. (Laughter) But PISA has shown what's possible in education. It has helped countries to see that improvement is possible. It has taken away excuses from those who are complacent. And it has helped countries to set meaningful targets in terms of measurable goals achieved by the world's leaders. If we can help every child, every teacher, every school, every principal, every parent see what improvement is possible, that only the sky is the limit to education improvement, we have laid the foundations for better policies and better lives. Thank you. (Applause)",PT19M30S,2013-02-21T15:28:16Z
Let's use video to reinvent education,"culture,global issues,education,Internet,teaching",Sal Khan,"Salman Khan talks about how and why he created the remarkable Khan Academy, a carefully structured series of educational videos offering complete curricula in math and, now, other subjects. He shows the power of interactive exercises, and calls for teachers to consider flipping the traditional classroom script -- give students video lectures to watch at home, and do ""homework"" in the classroom with the teacher available to help.","Khan Academy is most known for its collection of videos, so before I go any further, let me show you a little bit of a montage. (Video) Salman Khan: So the hypotenuse is now going to be five. This animal's fossils are only found in this area of South America -- a nice clean band here -- and this part of Africa. We can integrate over the surface, and the notation usually is a capital sigma. National Assembly: They create the Committee of Public Safety, which sounds like a very nice committee. Notice, this is an aldehyde, and it's an alcohol. Start differentiating into effector and memory cells. A galaxy. Hey! There's another galaxy. Oh, look! There's another galaxy. And for dollars, is their 30 million, plus the 20 million dollars from the American manufacturer. If this does not blow your mind, then you have no emotion. (Laughter) (Applause) (Live) SK: We now have on the order of 2,200 videos, covering everything from basic arithmetic, all the way to vector calculus, and some of the stuff that you saw up there. We have a million students a month using the site, watching on the order of 100 to 200,000 videos a day. But what we're going to talk about in this is how we're going to the next level. But before I do that, I want to talk a little bit about really just how I got started. And some of you all might know, about five years ago, I was an analyst at a hedge fund, and I was in Boston, and I was tutoring my cousins in New Orleans, remotely. And I started putting the first YouTube videos up, really just as a kind of nice-to-have, just kind of a supplement for my cousins, something that might give them a refresher or something. And as soon as I put those first YouTube videos up, something interesting happened. Actually, a bunch of interesting things happened. The first was the feedback from my cousins. They told me that they preferred me on YouTube than in person. (Laughter) And once you get over the backhanded nature of that, there was actually something very profound there. They were saying that they preferred the automated version of their cousin to their cousin. At first it's very unintuitive, but when you think about it from their point of view, it makes a ton of sense. You have this situation where now they can pause and repeat their cousin, without feeling like they're wasting my time. If they have to review something that they should have learned a couple of weeks ago, or maybe a couple of years ago, they don't have to be embarrassed and ask their cousin. They can just watch those videos; if they're bored, they can go ahead. They can watch at their own time and pace. Probably the least-appreciated aspect of this is the notion that the very first time that you're trying to get your brain around a new concept, the very last thing you need is another human being saying, ""Do you understand this?"" And that's what was happening with the interaction with my cousins before, and now they can just do it in the intimacy of their own room. The other thing that happened is -- I put them on YouTube just -- I saw no reason to make it private, so I let other people watch it, and then people started stumbling on it, and I started getting some comments and some letters and all sorts of feedback from random people around the world. These are just a few. This is actually from one of the original calculus videos. Someone wrote it on YouTube, it was a YouTube comment: ""First time I smiled doing a derivative."" (Laughter) Let's pause here. This person did a derivative, and then they smiled. (Laughter) In response to that same comment -- this is on the thread, you can go on YouTube and look at the comments -- someone else wrote: ""Same thing here. I actually got a natural high and a good mood for the entire day, since I remember seeing all of this matrix text in class, and here I'm all like, 'I know kung fu.'"" (Laughter) We get a lot of feedback along those lines. This clearly was helping people. But then, as the viewership kept growing and kept growing, I started getting letters from people, and it was starting to become clear that it was more than just a nice-to-have. This is just an excerpt from one of those letters: ""My 12 year-old son has autism, and has had a terrible time with math. We have tried everything, viewed everything, bought everything. We stumbled on your video on decimals, and it got through. Then we went on to the dreaded fractions. Again, he got it. We could not believe it. He is so excited."" And so you can imagine, here I was, an analyst at a hedge fund -- it was very strange for me to do something of social value. (Laughter) (Applause) But I was excited, so I kept going. And then a few other things started to dawn on me; that not only would it help my cousins right now, or these people who were sending letters, but that this content will never grow old, that it could help their kids or their grandkids. If Isaac Newton had done YouTube videos on calculus, I wouldn't have to. (Laughter) Assuming he was good. We don't know. (Laughter) The other thing that happened -- and even at this point, I said, ""OK, maybe it's a good supplement. It's good for motivated students. It's good for maybe home-schoolers."" But I didn't think it would somehow penetrate the classroom. Then I started getting letters from teachers, and the teachers would write, saying, ""We've used your videos to flip the classroom. You've given the lectures, so now what we do --"" And this could happen in every classroom in America tomorrow -- ""what I do is I assign the lectures for homework, and what used to be homework, I now have the students doing in the classroom."" And I want to pause here -- (Applause) I want to pause here, because there's a couple of interesting things. One, when those teachers are doing that, there's the obvious benefit -- the benefit that now their students can enjoy the videos in the way that my cousins did, they can pause, repeat at their own pace, at their own time. But the more interesting thing -- and this is the unintuitive thing when you talk about technology in the classroom -- by removing the one-size-fits-all lecture from the classroom, and letting students have a self-paced lecture at home, then when you go to the classroom, letting them do work, having the teacher walk around, having the peers actually be able to interact with each other, these teachers have used technology to humanize the classroom. They took a fundamentally dehumanizing experience -- 30 kids with their fingers on their lips, not allowed to interact with each other. A teacher, no matter how good, has to give this one-size-fits-all lecture to 30 students -- blank faces, slightly antagonistic -- and now it's a human experience, now they're actually interacting with each other. So once the Khan Academy -- I quit my job, and we turned into a real organization -- we're a not-for-profit -- the question is, how do we take this to the next level? How do we take what those teachers were doing to its natural conclusion? And so, what I'm showing over here, these are actual exercises that I started writing for my cousins. The ones I started were much more primitive. This is a more competent version of it. But the paradigm here is, we'll generate as many questions as you need, until you get that concept, until you get 10 in a row. And the Khan Academy videos are there. You get hints, the actual steps for that problem, if you don't know how to do it. The paradigm here seems like a very simple thing: 10 in a row, you move on. But it's fundamentally different than what's happening in classrooms right now. In a traditional classroom, you have homework, lecture, homework, lecture, and then you have a snapshot exam. And that exam, whether you get a 70 percent, an 80 percent, a 90 percent or a 95 percent, the class moves on to the next topic. And even that 95 percent student -- what was the five percent they didn't know? Maybe they didn't know what happens when you raise something to the zeroth power. Then you build on that in the next concept. That's analogous to -- imagine learning to ride a bicycle. Maybe I give you a lecture ahead of time, and I give you a bicycle for two weeks, then I come back after two weeks, and say, ""Well, let's see. You're having trouble taking left turns. You can't quite stop. You're an 80 percent bicyclist."" So I put a big ""C"" stamp on your forehead -- (Laughter) and then I say, ""Here's a unicycle."" (Laughter) But as ridiculous as that sounds, that's exactly what's happening in our classrooms right now. And the idea is you fast forward and good students start failing algebra all of the sudden, and start failing calculus all of the sudden, despite being smart, despite having good teachers, and it's usually because they have these Swiss cheese gaps that kept building throughout their foundation. So our model is: learn math the way you'd learn anything, like riding a bicycle. Stay on that bicycle. Fall off that bicycle. Do it as long as necessary, until you have mastery. The traditional model, it penalizes you for experimentation and failure, but it does not expect mastery. We encourage you to experiment. We encourage you to fail. But we do expect mastery. This is just another one of the modules. This is trigonometry. This is shifting and reflecting functions. And they all fit together. We have about 90 of these right now. You can go to the site right now, it's all free, not trying to sell anything. But the general idea is that they all fit into this knowledge map. That top node right there, that's literally single-digit addition, it's like one plus one is equal to two. The paradigm is, once you get 10 in a row on that, it keeps forwarding you to more and more advanced modules. Further down the knowledge map, we're getting into more advanced arithmetic. Further down, you start getting into pre-algebra and early algebra. Further down, you start getting into algebra one, algebra two, a little bit of precalculus. And the idea is, from this we can actually teach everything -- well, everything that can be taught in this type of a framework. So you can imagine -- and this is what we are working on -- from this knowledge map, you have logic, you have computer programming, you have grammar, you have genetics, all based off of that core of, if you know this and that, now you're ready for this next concept. Now that can work well for an individual learner, and I encourage you to do it with your kids, but I also encourage everyone in the audience to do it yourself. It'll change what happens at the dinner table. But what we want to do is use the natural conclusion of the flipping of the classroom that those early teachers had emailed me about. And so what I'm showing you here, this is data from a pilot in the Los Altos school district, where they took two fifth-grade classes and two seventh-grade classes, and completely gutted their old math curriculum. These kids aren't using textbooks, or getting one-size-fits-all lectures. They're doing Khan Academy, that software, for roughly half of their math class. I want to be clear: we don't view this as a complete math education. What it does is -- this is what's happening in Los Altos -- it frees up time -- it's the blocking and tackling, making sure you know how to move through a system of equations, and it frees up time for the simulations, for the games, for the mechanics, for the robot-building, for the estimating how high that hill is based on its shadow. And so the paradigm is the teacher walks in every day, every kid works at their own pace -- this is actually a live dashboard from the Los Altos school district -- and they look at this dashboard. Every row is a student. Every column is one of those concepts. Green means the student's already proficient. Blue means they're working on it -- no need to worry. Red means they're stuck. And what the teacher does is literally just say, ""Let me intervene on the red kids."" Or even better, ""Let me get one of the green kids, who are already proficient in that concept, to be the first line of attack, and actually tutor their peer."" (Applause) Now, I come from a very data-centric reality, so we don't want that teacher to even go and intervene and have to ask the kid awkward questions: ""What don't you understand? What do you understand?"" and all the rest. So our paradigm is to arm teachers with as much data as possible -- data that, in any other field, is expected, in finance, marketing, manufacturing -- so the teachers can diagnose what's wrong with the students so they can make their interaction as productive as possible. Now teachers know exactly what the students have been up to, how long they've spent each day, what videos they've watched, when did they pause the videos, what did they stop watching, what exercises are they using, what have they focused on? The outer circle shows what exercises they were focused on. The inner circle shows the videos they're focused on. The data gets pretty granular, so you can see the exact problems the student got right or wrong. Red is wrong, blue is right. The leftmost question is the first one the student attempted. They watched the video over there. And you can see, eventually they were able to get 10 in a row. It's almost like you can see them learning over those last 10 problems. They also got faster -- the height is how long it took them. When you talk about self-paced learning, it makes sense for everyone -- in education-speak, ""differentiated learning"" -- but it's kind of crazy, what happens when you see it in a classroom. Because every time we've done this, in every classroom we've done, over and over again, if you go five days into it, there's a group of kids who've raced ahead and a group who are a little bit slower. In a traditional model, in a snapshot assessment, you say, ""These are the gifted kids, these are the slow kids. Maybe they should be tracked differently. Maybe we should put them in different classes."" But when you let students work at their own pace -- we see it over and over again -- you see students who took a little bit extra time on one concept or the other, but once they get through that concept, they just race ahead. And so the same kids that you thought were slow six weeks ago, you now would think are gifted. And we're seeing it over and over again. It makes you really wonder how much all of the labels maybe a lot of us have benefited from were really just due to a coincidence of time. Now as valuable as something like this is in a district like Los Altos, our goal is to use technology to humanize, not just in Los Altos, but on a global scale, what's happening in education. And that brings up an interesting point. A lot of the effort in humanizing the classroom is focused on student-to-teacher ratios. In our mind, the relevant metric is: student-to-valuable-human-time- with-the-teacher ratio. So in a traditional model, most of the teacher's time is spent doing lectures and grading and whatnot. Maybe five percent of their time is sitting next to students and working with them. Now, 100 percent of their time is. So once again, using technology, not just flipping the classroom, you're humanizing the classroom, I'd argue, by a factor of five or 10. As valuable as that is in Los Altos, imagine what it does to the adult learner, who's embarrassed to go back and learn stuff they should have known before going back to college. Imagine what it does to a street kid in Calcutta, who has to help his family during the day, and that's the reason he or she can't go to school. Now they can spend two hours a day and remediate, or get up to speed and not feel embarrassed about what they do or don't know. Now imagine what happens where -- we talked about the peers teaching each other inside of a classroom. But this is all one system. There's no reason why you can't have that peer-to-peer tutoring beyond that one classroom. Imagine what happens if that student in Calcutta all of the sudden can tutor your son, or your son can tutor that kid in Calcutta. And I think what you'll see emerging is this notion of a global one-world classroom. And that's essentially what we're trying to build. Thank you. (Applause) Bill Gates: I'll ask about two or three questions. Salman Khan: Oh, OK. (Applause continues) (Applause ends) BG: I've seen some things you're doing in the system, that have to do with motivation and feedback -- energy points, merit badges. Tell me what you're thinking there. SK: Oh yeah. No, we have an awesome team working on it. I have to be clear, it's not just me anymore. I'm still doing all the videos, but we have a rock-star team doing the software. We've put a bunch of game mechanics in there, where you get badges, we're going to start having leader boards by area, you get points. It's actually been pretty interesting. Just the wording of the badging, or how many points you get for doing something, we see on a system-wide basis, like tens of thousands of fifth-graders or sixth-graders going one direction or another, depending what badge you give them. (Laughter) BG: And the collaboration you're doing with Los Altos, how did that come about? SK: Los Altos, it was kind of crazy. Once again, I didn't expect it to be used in classrooms. Someone from their board came and said, ""What would you do if you had carte Blanche in a classroom?"" I said, ""Well, every student would work at their own pace, on something like this, we'd give a dashboard."" They said, ""This is kind of radical. We have to think about it."" Me and the rest of the team were like, ""They're never going to want to do this."" But literally the next day they were like, ""Can you start in two weeks?"" (Laughter) BG: So fifth-grade math is where that's going on right now? SK: It's two fifth-grade classes and two seventh-grade classes. They're doing it at the district level. I think what they're excited about is they can follow these kids, not only in school; on Christmas, we saw some of the kids were doing it. We can track everything, track them as they go through the entire district. Through the summers, as they go from one teacher to the next, you have this continuity of data that even at the district level, they can see. BG: So some of those views we saw were for the teacher to go in and track actually what's going on with those kids. So you're getting feedback on those teacher views to see what they think they need? SK: Oh yeah. Most of those were specs by the teachers. We made some of those for students so they could see their data, but we have a very tight design loop with the teachers themselves. And they're saying, ""Hey, this is nice, but --"" Like that focus graph, a lot of the teachers said, ""I have a feeling a lot of the kids are jumping around and not focusing on one topic."" So we made that focus diagram. So it's all been teacher-driven. It's been pretty crazy. BG: Is this ready for prime time? Do you think a lot of classes next school year should try this thing out? SK: Yeah, it's ready. We've got a million people on the site already, so we can handle a few more. (Laughter) No, no reason why it really can't happen in every classroom in America tomorrow. BG: And the vision of the tutoring thing. The idea there is, if I'm confused about a topic, somehow right in the user interface, I'd find people who are volunteering, maybe see their reputation, and I could schedule and connect up with those people? SK: Absolutely. And this is something I recommend everyone in this audience do. Those dashboards the teachers have, you can go log in right now and you can essentially become a coach for your kids, your nephews, your cousins, or maybe some kids at the Boys and Girls Club. And yeah, you can start becoming a mentor, a tutor, really immediately. But yeah, it's all there. BG: Well, it's amazing. I think you just got a glimpse of the future of education. BG: Thank you. SK: Thank you. (Applause)",PT20M10S,2011-03-09T14:46:00Z
Kids can teach themselves,"culture,global issues,cities,education,teaching,Best of the Web,kids",Sugata Mitra,"Speaking at LIFT 2007, Sugata Mitra talks about his Hole in the Wall project. Young kids in this project figured out how to use a PC on their own -- and then taught other kids. He asks, what else can children teach themselves?","I have a tough job to do. You know, when I looked at the profile of the audience here, with their connotations and design, in all its forms, and with so much and so many people working on collaborative and networks, and so on, that I wanted to tell you, I wanted to build an argument for primary education in a very specific context. In order to do that in 20 minutes, I have to bring out four ideas -- it's like four pieces of a puzzle. And if I succeed in doing that, maybe you would go back with the thought that you could build on, and perhaps help me do my work. The first piece of the puzzle is remoteness and the quality of education. Now, by remoteness, I mean two or three different kinds of things. Of course, remoteness in its normal sense, which means that as you go further and further away from an urban center, you get to remoter areas. What happens to education? The second, or a different kind of remoteness is that within the large metropolitan areas all over the world, you have pockets, like slums, or shantytowns, or poorer areas, which are socially and economically remote from the rest of the city, so it's us and them. What happens to education in that context? So keep both of those ideas of remoteness. We made a guess. The guess was that schools in remote areas do not have good enough teachers. If they do have, they cannot retain those teachers. They do not have good enough infrastructure. And if they had some infrastructure, they have difficulty maintaining it. But I wanted to check if this is true. So what I did last year was we hired a car, looked up on Google, found a route into northern India from New Delhi which, you know, which did not cross any big cities or any big metropolitan centers. Drove out about 300 kilometers, and wherever we found a school, administered a set of standard tests, and then took those test results and plotted them on a graph. The graph was interesting, although you need to consider it carefully. I mean, this is a very small sample; you should not generalize from it. But it was quite obvious, quite clear, that for this particular route that I had taken, the remoter the school was, the worse its results seemed to be. That seemed a little damning, and I tried to correlate it with things like infrastructure, or with the availability of electricity, and things like that. To my surprise, it did not correlate. It did not correlate with the size of classrooms. It did not correlate with the quality of the infrastructure. It did not correlate with the poverty levels. It did not correlate. But what happened was that when I administered a questionnaire to each of these schools, with one single question for the teachers -- which was, ""Would you like to move to an urban, metropolitan area?"" -- 69 percent of them said yes. And as you can see from that, they say yes just a little bit out of Delhi, and they say no when you hit the rich suburbs of Delhi -- because, you know, those are relatively better off areas -- and then from 200 kilometers out of Delhi, the answer is consistently yes. I would imagine that a teacher who comes or walks into class every day thinking that, I wish I was in some other school, probably has a deep impact on what happens to the results. So it looked as though teacher motivation and teacher migration was a powerfully correlated thing with what was happening in primary schools, as opposed to whether the children have enough to eat, and whether they are packed tightly into classrooms and that sort of thing. It appears that way. When you take education and technology, then I find in the literature that, you know, things like websites, collaborative environments -- you've been listening to all that in the morning -- it's always piloted first in the best schools, the best urban schools, and, according to me, biases the result. The literature -- one part of it, the scientific literature -- consistently blames ET as being over-hyped and under-performing. The teachers always say, well, it's fine, but it's too expensive for what it does. Because it's being piloted in a school where the students are already getting, let's say, 80 percent of whatever they could do. You put in this new super-duper technology, and now they get 83 percent. So the principal looks at it and says, 3 percent for 300,000 dollars? Forget it. If you took the same technology and piloted it into one of those remote schools, where the score was 30 percent, and, let's say, took that up to 40 percent -- that will be a completely different thing. So the relative change that ET, Educational Technology, would make, would be far greater at the bottom of the pyramid than at the top, but we seem to be doing it the other way about. So I came to this conclusion that ET should reach the underprivileged first, not the other way about. And finally came the question of, how do you tackle teacher perception? Whenever you go to a teacher and show them some technology, the teacher's first reaction is, you cannot replace a teacher with a machine -- it's impossible. I don't know why it's impossible, but, even for a moment, if you did assume that it's impossible -- I have a quotation from Sir Arthur C. Clarke, the science fiction writer whom I met in Colombo, and he said something which completely solves this problem. He said a teacher than can be replaced by a machine, should be. So, you know, it puts the teacher into a tough bind, you have to think. Anyway, so I'm proposing that an alternative primary education, whatever alternative you want, is required where schools don't exist, where schools are not good enough, where teachers are not available or where teachers are not good enough, for whatever reason. If you happen to live in a part of the world where none of this applies, then you don't need an alternative education. So far I haven't come across such an area, except for one case. I won't name the area, but somewhere in the world people said, we don't have this problem, because we have perfect teachers and perfect schools. There are such areas, but -- anyway, I'd never heard that anywhere else. I'm going to talk about children and self-organization, and a set of experiments which sort of led to this idea of what might an alternative education be like. They're called the hole-in-the-wall experiments. I'll have to really rush through this. They're a set of experiments. The first one was done in New Delhi in 1999. And what we did over there was pretty much simple. I had an office in those days which bordered a slum, an urban slum, so there was a dividing wall between our office and the urban slum. They cut a hole inside that wall -- which is how it has got the name hole-in-the-wall -- and put a pretty powerful PC into that hole, sort of embedded into the wall so that its monitor was sticking out at the other end, a touchpad similarly embedded into the wall, put it on high-speed Internet, put the Internet Explorer there, put it on Altavista.com -- in those days -- and just left it there. And this is what we saw. So that was my office in IIT. Here's the hole-in-the-wall. About eight hours later, we found this kid. To the right is this eight-year-old child who -- and to his left is a six-year-old girl, who is not very tall. And what he was doing was, he was teaching her to browse. So it sort of raised more questions than it answered. Is this real? Does the language matter, because he's not supposed to know English? Will the computer last, or will they break it and steal it -- and did anyone teach them? The last question is what everybody said, but you know, I mean, they must have poked their head over the wall and asked the people in your office, can you show me how to do it, and then somebody taught him. So I took the experiment out of Delhi and repeated it, this time in a city called Shivpuri in the center of India, where I was assured that nobody had ever taught anybody anything. (Laughter) So it was a warm day, and the hole in the wall was on that decrepit old building. This is the first kid who came there; he later on turned out to be a 13-year-old school dropout. He came there and he started to fiddle around with the touchpad. Very quickly, he noticed that when he moves his finger on the touchpad something moves on the screen -- and later on he told me, ""I have never seen a television where you can do something."" So he figured that out. It took him over two minutes to figure out that he was doing things to the television. And then, as he was doing that, he made an accidental click by hitting the touchpad -- you'll see him do that. He did that, and the Internet Explorer changed page. Eight minutes later, he looked from his hand to the screen, and he was browsing: he was going back and forth. When that happened, he started calling all the neighborhood children, like, children would come and see what's happening over here. And by the evening of that day, 70 children were all browsing. So eight minutes and an embedded computer seemed to be all that we needed there. So we thought that this is what was happening: that children in groups can self-instruct themselves to use a computer and the Internet. But under what circumstances? At this time there was a -- the main question was about English. People said, you know, you really ought to have this in Indian languages. So I said, have what, shall I translate the Internet into some Indian language? That's not possible. So, it has to be the other way about. But let's see, how do the children tackle the English language? I took the experiment out to northeastern India, to a village called Madantusi, where, for some reason, there was no English teacher, so the children had not learned English at all. And I built a similar hole-in-the-wall. One big difference in the villages, as opposed to the urban slums: there were more girls than boys who came to the kiosk. In the urban slums, the girls tend to stay away. I left the computer there with lots of CDs -- I didn't have any Internet -- and came back three months later. So when I came back there, I found these two kids, eight- and 12-year-olds, who were playing a game on the computer. And as soon as they saw me they said, ""We need a faster processor and a better mouse."" (Laughter) I was real surprised. You know, how on earth did they know all this? And they said, ""Well, we've picked it up from the CDs."" So I said, ""But how did you understand what's going on over there?"" So they said, ""Well, you've left this machine which talks only in English, so we had to learn English."" So then I measured, and they were using 200 English words with each other -- mispronounced, but correct usage -- words like exit, stop, find, save, that kind of thing, not only to do with the computer but in their day-to-day conversations. So, Madantusi seemed to show that language is not a barrier; in fact they may be able to teach themselves the language if they really wanted to. Finally, I got some funding to try this experiment out to see if these results are replicable, if they happen everywhere else. India is a good place to do such an experiment in, because we have all the ethnic diversities, all the -- you know, the genetic diversity, all the racial diversities, and also all the socio-economic diversities. So, I could actually choose samples to cover a cross section that would cover practically the whole world. So I did this for almost five years, and this experiment really took us all the way across the length and breadth of India. This is the Himalayas. Up in the north, very cold. I also had to check or invent an engineering design which would survive outdoors, and I was using regular, normal PCs, so I needed different climates, for which India is also great, because we have very cold, very hot, and so on. This is the desert to the west. Near the Pakistan border. And you see here a little clip of -- one of these villages -- the first thing that these children did was to find a website to teach themselves the English alphabet. Then to central India -- very warm, moist, fishing villages, where humidity is a very big killer of electronics. So we had to solve all the problems we had without air conditioning and with very poor power, so most of the solutions that came out used little blasts of air put at the right places to keep the machines running. I want to just cut this short. We did this over and over again. This sequence is also nice. This is a small child, a six-year-old, telling his eldest sister what to do. And this happens very often with these computers, that the younger children are found teaching the older ones. What did we find? We found that six- to 13-year-olds can self-instruct in a connected environment, irrespective of anything that we could measure. So if they have access to the computer, they will teach themselves, including intelligence. I couldn't find a single correlation with anything, but it had to be in groups. And that may be of great, you know, interest to this group, because all of you are talking about groups. So here was the power of what a group of children can do, if you lift the adult intervention. Just a quick idea of the measurements. We took standard statistical techniques, so I'm going to not talk about that. But we got a clean learning curve, almost exactly the same as what you would get in a school. I'll leave it at that, because, I mean, it sort of says it all, doesn't it? What could they learn to do? Basic Windows functions, browsing, painting, chatting and email, games and educational material, music downloads, playing video. In short, what all of us do. And over 300 children will become computer literate and be able to do all of these things in six months with one computer. So, how do they do that? If you calculated the actual time of access, it would work out to minutes per day, so that's not how it's happening. What you have, actually, is there is one child operating the computer. And surrounding him are usually three other children, who are advising him on what they should do. If you test them, all four will get the same scores in whatever you ask them. Around these four are usually a group of about 16 children, who are also advising, usually wrongly, about everything that's going on on the computer. And all of them also will clear a test given on that subject. So they are learning as much by watching as they learn by doing. It seems counter-intuitive to adult learning, but remember, eight-year-olds live in a society where most of the time they are told, don't do this, you know, don't touch the whiskey bottle. So what does the eight-year-old do? He observes very carefully how a whiskey bottle should be touched. And if you tested him, he would answer every question correctly on that topic. So, they seem to be able to acquire very quickly. So what was the conclusion over the six years of work? It was that primary education can happen on its own, or parts of it can happen on its own. It does not have to be imposed from the top downwards. It could perhaps be a self-organizing system, so that was the second bit that I wanted to tell you, that children can self-organize and attain an educational objective. The third piece was on values, and again, to put it very briefly, I conducted a test over 500 children spread across all over India, and asked them -- I gave them about 68 different values-oriented questions and simply asked them their opinions. We got all sorts of opinions. Yes, no or I don't know. I simply took those questions where I got 50 percent yeses and 50 percent noes -- so I was able to get a collection of 16 such statements. These were areas where the children were clearly confused, because half said yes and half said no. A typical example being, ""Sometimes it is necessary to tell lies."" They don't have a way to determine which way to answer this question; perhaps none of us do. So I leave you with this third question. Can technology alter the acquisition of values? Finally, self-organizing systems, about which, again, I won't say too much because you've been hearing all about it. Natural systems are all self-organizing: galaxies, molecules, cells, organisms, societies -- except for the debate about an intelligent designer. But at this point in time, as far as science goes, it's self-organization. But other examples are traffic jams, stock market, society and disaster recovery, terrorism and insurgency. And you know about the Internet-based self-organizing systems. So here are my four sentences then. Remoteness affects the quality of education. Educational technology should be introduced into remote areas first, and other areas later. Values are acquired; doctrine and dogma are imposed -- the two opposing mechanisms. And learning is most likely a self-organizing system. If you put all the four together, then it gives -- according to me -- it gives us a goal, a vision, for educational technology. An educational technology and pedagogy that is digital, automatic, fault-tolerant, minimally invasive, connected and self-organized. As educationists, we have never asked for technology; we keep borrowing it. PowerPoint is supposed to be considered a great educational technology, but it was not meant for education, it was meant for making boardroom presentations. We borrowed it. Video conferencing. The personal computer itself. I think it's time that the educationists made their own specs, and I have such a set of specs. This is a brief look at that. And such a set of specs should produce the technology to address remoteness, values and violence. So I thought I'd give it a name -- why don't we call it ""outdoctrination."" And could this be a goal for educational technology in the future? So I want to leave that as a thought with you. Thank you. (Applause)",PT20M37S,2008-08-27T01:00:00Z
My wish: Once Upon a School,"culture,global issues,design,entertainment,collaboration,education,activism,TED Prize,writing,teaching,kids",Dave Eggers,"Accepting his 2008 TED Prize, author Dave Eggers asks  the TED community to personally, creatively engage with local public schools. With spellbinding eagerness, he talks about how his 826 Valencia tutoring center inspired others around the world to open","Thank you so much everyone from TED, and Chris and Amy in particular. I cannot believe I'm here. I have not slept in weeks. Neil and I were sitting there comparing how little we've slept in anticipation for this. I've never been so nervous -- and I do this when I'm nervous, I just realized. (Laughter) So, I'm going to talk about sort of what we did at this organization called 826 Valencia, and then I'm going to talk about how we all might join in and do similar things. Back in about 2000, I was living in Brooklyn, I was trying to finish my first book, I was wandering around dazed every day because I wrote from 12 a.m. to 5 a.m. So I would walk around in a daze during the day. I had no mental acuity to speak of during the day, but I had flexible hours. In the Brooklyn neighborhood that I lived in, Park Slope, there are a lot of writers -- it's like a very high per capita ratio of writers to normal people. Meanwhile, I had grown up around a lot of teachers. My mom was a teacher, my sister became a teacher and after college so many of my friends went into teaching. And so I was always hearing them talk about their lives and how inspiring they were, and they were really sort of the most hard-working and constantly inspiring people I knew. But I knew so many of the things they were up against, so many of the struggles they were dealing with. And one of them was that so many of my friends that were teaching in city schools were having trouble with their students keeping up at grade level, in their reading and writing in particular. Now, so many of these students had come from households where English isn't spoken in the home, where a lot of them have different special needs, learning disabilities. And of course they're working in schools which sometimes and very often are under-funded. And so they would talk to me about this and say, ""You know, what we really need is just more people, more bodies, more one-on-one attention, more hours, more expertise from people that have skills in English and can work with these students one-on-one."" Now, I would say, ""Well, why don't you just work with them one-on-one?"" And they would say, ""Well, we have five classes of 30 to 40 students each. This can lead up to 150, 180, 200 students a day. How can we possibly give each student even one hour a week of one-on-one attention?"" You'd have to greatly multiply the workweek and clone the teachers. And so we started talking about this. And at the same time, I thought about this massive group of people I knew: writers, editors, journalists, graduate students, assistant professors, you name it. All these people that had sort of flexible daily hours and an interest in the English word -- I hope to have an interest in the English language, but I'm not speaking it well right now. (Laughter) I'm trying. That clock has got me. But everyone that I knew had an interest in the primacy of the written word in terms of nurturing a democracy, nurturing an enlightened life. And so they had, you know, their time and their interest, but at the same time there wasn't a conduit that I knew of in my community to bring these two communities together. So when I moved back to San Francisco, we rented this building. And the idea was to put McSweeney's -- McSweeney's Quarterly, that we published twice or three times a year, and a few other magazines -- we were going to move it into an office for the first time. It used to be in my kitchen in Brooklyn. We were going to move it into an office, and we were going to actually share space with a tutoring center. So we thought, ""We'll have all these writers and editors and everybody -- sort of a writing community -- coming into the office every day anyway, why don't we just open up the front of the building for students to come in there after school, get extra help on their written homework, so you have basically no border between these two communities?"" So the idea was that we would be working on whatever we're working on, at 2:30 p.m. the students flow in and you put down what you're doing, or you trade, or you work a little bit later or whatever it is. You give those hours in the afternoon to the students in the neighborhood. So, we had this place, we rented it, the landlord was all for it. We did this mural, that's a Chris Ware mural, that basically explains the entire history of the printed word, in mural form -- it takes a long time to digest and you have to stand in the middle of the road. So we rented this space. And everything was great except the landlord said, ""Well, the space is zoned for retail; you have to come up with something. You've gotta sell something. You can't just have a tutoring center."" So we thought, ""Ha ha! Really!"" And we couldn't think of anything necessarily to sell, but we did all the necessary research. It used to be a weight room, so there were rubber floors below, acoustic tile ceilings and fluorescent lights. We took all that down, and we found beautiful wooden floors, whitewashed beams and it had the look -- while we were renovating this place, somebody said, ""You know, it really kind of looks like the hull of a ship."" And we looked around and somebody else said, ""Well, you should sell supplies to the working buccaneer."" (Laughter) And so this is what we did. So it made everybody laugh, and we said, ""There's a point to that. Let's sell pirate supplies."" This is the pirate supply store. You see, this is sort of a sketch I did on a napkin. A great carpenter built all this stuff and you see, we made it look sort of pirate supply-like. Here you see planks sold by the foot and we have supplies to combat scurvy. We have the peg legs there, that are all handmade and fitted to you. Up at the top, you see the eyepatch display, which is the black column there for everyday use for your eyepatch, and then you have the pastel and other colors for stepping out at night -- special occasions, bar mitzvahs and whatever. So we opened this place. And this is a vat that we fill with treasures that students dig in. This is replacement eyes in case you lose one. These are some signs that we have all over the place: ""Practical Joking with Pirates."" While you're reading the sign, we pull a rope behind the counter and eight mop heads drop on your head. That was just my one thing -- I said we had to have something that drops on people's heads. It became mop heads. And this is the fish theater, which is just a saltwater tank with three seats, and then right behind it we set up this space, which was the tutoring center. So right there is the tutoring center, and then behind the curtain were the McSweeney's offices, where all of us would be working on the magazine and book editing and things like that. The kids would come in -- or we thought they would come in. I should back up. We set the place up, we opened up, we spent months and months renovating this place. We had tables, chairs, computers, everything. I went to a dot-com auction at a Holiday Inn in Palo Alto and I bought 11 G4s with a stroke of a paddle. Anyway, we bought 'em, we set everything up and then we waited. It was started with about 12 of my friends, people that I had known for years that were writers in the neighborhood. And we sat. And at 2:30 p.m. we put a sandwich board out on the front sidewalk and it just said, ""Free Tutoring for Your English-Related and Writing-Related Needs -- Just Come In, It's All Free."" And we thought, ""Oh, they're going to storm the gates, they're gonna love it."" And they didn't. And so we waited, we sat at the tables, we waited and waited. And everybody was becoming very discouraged because it was weeks and weeks that we waited, really, where nobody came in. And then somebody alerted us to the fact that maybe there was a trust gap, because we were operating behind a pirate supply store. (Laughter) We never put it together, you know? And so then, around that time, I persuaded a woman named Nineveh Caligari, a longtime San Francisco educator -- she was teaching in Mexico City, she had all the experience necessary, knew everything about education, was connected with all the teachers and community members in the neighborhood -- I convinced her to move up from Mexico City where she was teaching. She took over as executive director. Immediately, she made the inroads with the teachers and the parents and the students and everything, and so suddenly it was actually full every day. And what we were trying to offer every day was one-on-one attention. The goal was to have a one-to-one ratio with every one of these students. You know, it's been proven that 35 to 40 hours a year with one-on-one attention, a student can get one grade level higher. And so most of these students, English is not spoken in the home. They come there, many times their parents -- you can't see it, but there's a church pew that I bought in a Berkeley auction right there -- the parents will sometimes watch while their kids are being tutored. So that was the basis of it, was one-on-one attention. And we found ourselves full every day with kids. If you're on Valencia Street within those few blocks at around 2 p.m., 2:30 p.m., you will get run over, often, by the kids and their big backpacks, or whatever, actually running to this space, which is very strange, because it's school, in a way. But there was something psychological happening there that was just a little bit different. And the other thing was, there was no stigma. Kids weren't going into the ""Center-for-Kids-That-Need-More-Help,"" or something like that. It was 826 Valencia. First of all, it was a pirate supply store, which is insane. And then secondly, there's a publishing company in the back. And so our interns were actually working at the same tables very often, and shoulder-to-shoulder, computer-next-to-computer with the students. And so it became a tutoring center -- publishing center, is what we called it -- and a writing center. They go in, and they might be working with a high school student actually working on a novel -- because we had very gifted kids, too. So there's no stigma. They're all working next to each other. It's all a creative endeavor. They're seeing adults. They're modeling their behavior. These adults, they're working in their field. They can lean over, ask a question of one of these adults and it all sort of feeds on each other. There's a lot of cross-pollination. The only problem, especially for the adults working at McSweeney's who hadn't necessarily bought into all of this when they signed up, was that there was just the one bathroom. (Laughter) With like 60 kids a day, this is a problem. But you know, there's something about the kids finishing their homework in a given day, working one-on-one, getting all this attention -- they go home, they're finished. They don't stall. They don't do their homework in front of the TV. They're allowed to go home at 5:30 p.m., enjoy their family, enjoy other hobbies, get outside, play. And that makes a happy family. A bunch of happy families in a neighborhood is a happy community. A bunch of happy communities tied together is a happy city and a happy world. So the key to it all is homework! (Laughter) (Applause) There you have it, you know -- one-on-one attention. So we started off with about 12 volunteers, and then we had about 50, and then a couple hundred. And we now have 1,400 volunteers on our roster. And we make it incredibly easy to volunteer. The key thing is, even if you only have a couple of hours a month, those two hours shoulder-to-shoulder, next to one student, concentrated attention, shining this beam of light on their work, on their thoughts and their self-expression, is going to be absolutely transformative, because so many of the students have not had that ever before. So we said, ""Even if you have two hours one Sunday every six months, it doesn't matter. That's going to be enough."" So that's partly why the tutor corps grew so fast. Then we said, ""Well, what are we going to do with the space during the day, because it has to be used before 2:30 p.m.?"" So we started bringing in classes during the day. So every day, there's a field trip where they together create a book -- you can see it being typed up above. This is one of the classes getting way too excited about writing. You just point a camera at a class, and it always looks like this. So this is one of the books that they do. Notice the title of the book, ""The Book That Was Never Checked Out: Titanic."" And the first line of that book is, ""Once there was a book named Cindy that was about the Titanic."" So, meanwhile, there's an adult in the back typing this up, taking it completely seriously, which blows their mind. So then we still had more tutors to use. This is a shot of just some of the tutors during one of the events. The teachers that we work with -- and everything is different to teachers -- they tell us what to do. We went in there thinking, ""We're ultimately, completely malleable. You're going to tell us. The neighborhood's going to tell us, the parents are going to tell us. The teachers are going to tell us how we're most useful."" So then they said, ""Why don't you come into the schools? Because what about the students that wouldn't come to you, necessarily, who don't have really active parents that are bringing them in, or aren't close enough?"" So then we started saying, ""Well, we've got 1,400 people on our tutor roster. Let's just put out the word."" A teacher will say, ""I need 12 tutors for the next five Sundays. We're working on our college essays. Send them in."" So we put that out on the wire: 1,400 tutors. Whoever can make it signs up. They go in about a half an hour before the class. The teacher tells them what to do, how to do it, what their training is, what their project is so far. They work under the teacher's guide, and it's all in one big room. And that's actually the brunt of what we do is, people going straight from their workplace, straight from home, straight into the classroom and working directly with the students. So then we're able to work with thousands and thousands of more students. Then another school said, ""Well, what if we just give you a classroom and you can staff it all day?"" So this is the Everett Middle School Writers' Room, where we decorated it in buccaneer style. It's right off the library. And there we serve all 529 kids in this middle school. This is their newspaper, the ""Straight-Up News,"" that has an ongoing column from Mayor Gavin Newsom in both languages -- English and Spanish. So then one day Isabel Allende wrote to us and said, ""Hey, why don't you assign a book with high school students? I want them to write about how to achieve peace in a violent world."" And so we went into Thurgood Marshall High School, which is a school that we had worked with on some other things, and we gave that assignment to the students. And we said, ""Isabel Allende is going to read all your essays at the end. She's going to publish them in a book. She's going to sponsor the printing of this book in paperback form. It's going to be available in all the bookstores in the Bay Area and throughout the world, on Amazon and you name it."" So these kids worked harder than they've ever worked on anything in their lives, because there was that outside audience, there was Isabel Allende on the other end. I think we had about 170 tutors that worked on this book with them and so this worked out incredibly well. We had a big party at the end. This is a book that you can find anywhere. So that led to a series of these. You can see Amy Tan sponsored the next one, ""I Might Get Somewhere."" And this became an ongoing thing. More and more books. Now we're sort of addicted to the book thing. The kids will work harder than they've ever worked in their life if they know it's going to be permanent, know it's going to be on a shelf, know that nobody can diminish what they've thought and said, that we've honored their words, honored their thoughts with hundreds of hours of five drafts, six drafts -- all this attention that we give to their thoughts. And once they achieve that level, once they've written at that level, they can never go back. It's absolutely transformative. And so then they're all sold in the store. This is near the planks. We sell all the student books. Where else would you put them, right? So we sell 'em, and then something weird had been happening with the stores. The store, actually -- even though we started out as just a gag -- the store actually made money. So it was paying the rent. And maybe this is just a San Francisco thing -- I don't know, I don't want to judge. But people would come in -- and this was before the pirate movies and everything! It was making a lot of money. Not a lot of money, but it was paying the rent, paying a full-time staff member there. There's the ocean maps you can see on the left. And it became a gateway to the community. People would come in and say, ""What the --? What is this?"" I don't want to swear on the web. (Laughter) Is that a rule? I don't know. They would say, ""What is this?"" And people would come in and learn more about it. And then right beyond -- there's usually a little chain there -- right beyond, they would see the kids being tutored. This is a field trip going on. And so they would be shopping, and they might be more likely to buy some lard, or millet for their parrot, or, you know, a hook, or hook protector for nighttime, all of these things we sell. So the store actually did really well. But it brought in so many people -- teachers, donors, volunteers, everybody -- because it was street level. It was open to the public. It wasn't a non-profit buried, you know, on the 30th floor of some building downtown. It was right in the neighborhood that it was serving, and it was open all the time to the public. So, it became this sort of weird, happy accident. So all the people I used to know in Brooklyn, they said, ""Well, why don't we have a place like that here?"" And a lot of them had been former educators or would-be educators, so they combined with a lot of local designers, local writers, and they just took the idea independently and they did their own thing. They didn't want to sell pirate supplies. They didn't think that that was going to work there. So, knowing the crime-fighting community in New York, they opened the Brooklyn Superhero Supply Company. This is Sam Potts' great design that did this. And this was to make it look sort of like one of those keysmith's shops that has to have every service they've ever offered, you know, all over there. So they opened this place. Inside, it's like a Costco for superheroes -- all the supplies in kind of basic form. These are all handmade. These are all sort of repurposed other products, or whatever. All the packaging is done by Sam Potts. So then you have the villain containment unit, where kids put their parents. You have the office. This is a little vault -- you have to put your product in there, it goes up an electric lift and then the guy behind the counter tells you that you have to recite the vow of heroism, which you do, if you want to buy anything. And it limits, really, their sales. Personally, I think it's a problem. Because they have to do it hand on heart and everything. These are some of the products. These are all handmade. This is a secret identity kit. If you want to take on the identity of Sharon Boone, one American female marketing executive from Hoboken, New Jersey. It's a full dossier on everything you would need to know about Sharon Boone. So, this is the capery where you get fitted for your cape, and then you walk up these three steel-graded steps and then we turn on three hydraulic fans from every side and then you can see the cape in action. There's nothing worse than, you know, getting up there and the cape is bunching up or something like that. So then, the secret door -- this is one of the shelves you don't see when you walk in, but it slowly opens. You can see it there in the middle next to all the grappling hooks. It opens and then this is the tutoring center in the back. (Applause) So you can see the full effect! But this is -- I just want to emphasize -- locally funded, locally built. All the designers, all of the builders, everybody was local, all the time was pro-bono. I just came and visited and said, ""Yes, you guys are doing great,"" or whatever. That was it. You can see the time in all five boroughs of New York in the back. (Laughter) (Applause) So this is the space during tutoring hours. It's very busy. Same principles: one-on-one attention, complete devotion to the students' work and a boundless optimism and sort of a possibility of creativity and ideas. And this switch is flicked in their heads when they walk through those 18 feet of this bizarre store, right? So it's school, but it's not school. It's clearly not school, even though they're working shoulder-to-shoulder on tables, pencils and papers, whatever. This is one of the students, Khaled Hamdan. You can read this quote. Addicted to video games and TV. Couldn't concentrate at home. Came in. Got this concentrated attention. And he couldn't escape it. So, soon enough, he was writing. He would finish his homework early -- got really addicted to finishing his homework early. It's an addictive thing to sort of be done with it, and to have it checked, and to know he's going to achieve the next thing and be prepared for school the next day. So he got hooked on that, and then he started doing other things. He's now been published in five books. He co-wrote a mockumentary about failed superheroes called ""Super-Has-Beens."" He wrote a series on ""Penguin Balboa,"" which is a fighting -- a boxing -- penguin. And then he read aloud just a few weeks ago to 500 people at Symphony Space, at a benefit for 826 New York. So he's there every day. He's evangelical about it. He brings his cousins in now. There's four family members that come in every day. So, I'll go through really quickly. This is L.A., The Echo Park Time Travel Mart: ""Whenever You Are, We're Already Then."" (Laughter) This is sort of a 7-Eleven for time travelers. So you see everything: it's exactly as a 7-Eleven would be. Leeches. Mammoth chunks. They even have their own Slurpee machine: ""Out of Order. Come Back Yesterday."" (Laughter) (Applause) Anyway. So I'm going to jump ahead. These are spaces that are only affiliated with us, doing this same thing: Word St. in Pittsfield, Massachusetts; Ink Spot in Cincinnati; Youth Speaks, San Francisco, California, which inspired us; Studio St. Louis in St. Louis; Austin Bat Cave in Austin; Fighting Words in Dublin, Ireland, started by Roddy Doyle, this will be open in April. Now I'm going to the TED Wish -- is that okay? All right, I've got a minute. So, the TED Wish: I wish that you -- you personally and every creative individual and organization you know -- will find a way to directly engage with a public school in your area and that you'll then tell the story of how you got involved, so that within a year we have a thousand examples -- a thousand! -- of transformative partnerships. Profound leaps forward! And these can be things that maybe you're already doing. I know that so many people in this room are already doing really interesting things. I know that for a fact. So, tell us these stories and inspire others on the website. We created a website. I'm going to switch to ""we,"" and not ""I,"" hope: We hope that the attendees of this conference will usher in a new era of participation in our public schools. We hope that you will take the lead in partnering your innovative spirit and expertise with that of innovative educators in your community. Always let the teachers lead the way. They will tell you how to be useful. I hope that you'll step in and help out. There are a million ways. You can walk up to your local school and consult with the teachers. They'll always tell you how to help. So, this is with Hot Studio in San Francisco, they did this phenomenal job. This website is already up, it's already got a bunch of stories, a lot of ideas. It's called ""Once Upon a School,"" which is a great title, I think. This site will document every story, every project that comes out of this conference and around the world. So you go to the website, you see a bunch of ideas you can be inspired by and then you add your own projects once you get started. Hot Studio did a great job in a very tight deadline. So, visit the site. If you have any questions, you can ask this guy, who's our director of national programs. He'll be on the phone. You email him, he'll answer any question you possibly want. And he'll get you inspired and get you going and guide you through the process so that you can affect change. And it can be fun! That's the point of this talk -- it needn't be sterile. It needn't be bureaucratically untenable. You can do and use the skills that you have. The schools need you. The teachers need you. Students and parents need you. They need your actual person: your physical personhood and your open minds and open ears and boundless compassion, sitting next to them, listening and nodding and asking questions for hours at a time. Some of these kids just don't plain know how good they are: how smart and how much they have to say. You can tell them. You can shine that light on them, one human interaction at a time. So we hope you'll join us. Thank you so much.",PT24M15S,2008-03-18T01:59:00Z
Your right to mental privacy in the age of brain-sensing tech,"science,technology,health,future,brain,AI,data,ethics",Nita Farahany,"Neurotechnology, or devices that let you track your own brain activity, could help you deeply understand your health. But without privacy protections, your innermost thoughts, emotions and desires could be at risk of exploitation, says neurotech and AI ethicist Nita Farahany. She details some of the field's promising potential uses -- like tracking and treating diseases from depression to epilepsy -- and shares concerns about who collects our brain data and how they plan to use it, ultimately calling for the legal recognition of ""cognitive liberty"" as we connect our brains and minds to technology.","Today, we know and track virtually nothing that’s happening in our own brains. But in a future that is coming much faster than you realize, all of that is about to change. We're now familiar with sensors in our smart watches to our rings, that track everything from our heartbeats to our footsteps, breaths, body temperature, even our sleep. Now, consumer neurotech devices are being sold worldwide to enable us to track our own brain activity. As companies from Meta to Microsoft, Snap and even Apple begin to embed brain sensors in our everyday devices like our earbuds, headphones, headbands, watches and even wearable tattoos, we're reaching an inflection point in brain transparency. And those are just some of the company names we're familiar with. There are so many more. Consumer neurotech devices are moving from niche products with limited applications to becoming the way in which we'll learn about our own brain activity, our controller for virtual reality and augmented reality. And one of the primary ways we'll interact with all of the rest of our technology. Even conservative estimates of the neurotech industry put it at more than  38 billion dollars by 2032. This new category of technology presents unprecedented possibility, both good and bad. Consider how our physical health and well-being are increasing while neurological disease and suffering continue to rise. 55 million people around the world are struggling with dementia, with more than 60 to 70 percent of them suffering from Alzheimer's disease. Nearly a billion people struggle with mental health and drug use disorders. Depression affects more than 300 million. Consumer neurotech devices could finally enable us to treat our brain health and wellness as seriously as we treat the rest of our physical well-being. But making our brains transparent to others also introduces extraordinary risks. Which is why, before it's too late to do so, we must change the basic terms of service for neurotechnology in favour of individual rights. I say this not just as a law professor who believes in the power of law, nor just a philosopher trying to flesh out norms, but as a mother who's been personally and profoundly impacted by the use of neurotechnology in my own life. On Mother's Day in 2017, as my daughter Calista lay cradled in my arms, she took one last beautiful breath. After a prolonged hospitalization, complications following infections claimed her life. The harrowing trauma that she endured and we witnessed stretched into weeks. And I was left with lasting trauma that progressed into post-traumatic stress disorder. Sleep escaped me for years. As each time I closed my eyes, I relived everything, from the first moments that I was pushed out of the emergency room to her gut-wrenching cries. Ultimately, it was the help of a talented psychologist, using exposure therapy, and my use of neurofeedback that enabled me to sleep through the night. For others who are suffering from traumatic memories, an innovative new approach using decoded neurofeedback, or DecNef, may offer reprieve. This groundbreaking approach uses machine-learning algorithms to identify specific brain-activity patterns, including those associated with traumatic memories. Participants then play a game that enables them to retrain their brain activity on positive associations instead. If I had had DecNef available to me at the time, I might have overcome my PTSD more quickly without having to relive every sound, terror and smell in order to do so. I'm not the only one. Sarah described herself as being at the end of her life, no longer in a life worth living, because of her severe and intractable depression. Then, using implanted brain sensors that reset her brain activity like a pacemaker for the brain, Sarah reclaimed her will to live. While implanted neurotechnology advances have been extraordinary, it's the everyday brain sensors that are embedded in our ordinary technology that I believe will impact the majority of our lives. Like the one third of adults and nearly one quarter of children who are living with epilepsy for whom conventional anti-seizure medications fail. Now, researchers from Israel to Spain have developed brain sensors using the power of AI in pattern recognition and consumer electroencephalography to enable the detection of epileptic seizures minutes to up to an hour before they occur, sending potentially life-saving alerts to a mobile device. Regular use of brain sensors could even enable us to detect the earliest stages of the most aggressive forms of brain tumors, like glioblastoma, where early detection is crucial to saving lives. The same could hold true for Parkinson's disease, to Alzheimer's, traumatic brain injury, ADHD, and even depression. We may even change our brains for the better. The brain training game industry, worth a staggering 6.5 billion dollars in 2021, was for years met with controversy because of unsupported scientific claims about their efficacy. But now some brain-training platforms like Cognizant have proven powerful in improving brain processing speeds, memory, reasoning and even executive functioning when played repeatedly over time. When paired with neurofeedback devices for learning reinforcement, this could revolutionize how we learn and adapt to change. Other breakthroughs could be transformational for the human experience. Today, most human brain studies are based on a very small number of participants engaged in very specific tasks in a controlled laboratory environment. With widespread use of brain sensors, the data we could have to learn about the human brain would exponentially increase. With sufficiently large datasets of long-term, real-world data from people engaged in everyday activity, we just might address everything from neurological disease and suffering to creating transformational possibilities for the human experience. But all of this will only be possible if people can confidently share their brain data without fear that it will be misused against them. You see, the brain data that will be collected and generated by these devices won't be collected in traditional laboratory environments or in clinical research studies run by physicians and scientists. Instead, it will be the sellers of these new devices, the very companies who've been commodifying our personal data for years. Which is why we can't go into this new era naive about the risks or complacent about the challenges that the collection and sharing our brain data will pose. Scientific hurdles can and will be addressed in time, but the social hurdles will be the most challenging. Unlike the technologies of the past that track and hack the human brain, brain sensors provide direct access to the part of ourselves that we hold back, that we don't express through our words and our actions. Brain data in many instances will be more sensitive than the personal data of the past, because it reflects our feelings, our mental states, our emotions, our preferences, our desires, even our very thoughts. I would never have wanted the data that was collected as I worked through the trauma of my personal loss to have been commodified, shared and analyzed by others. These aren't just hypothetical risks. Take Entertek, a Hangzhou-based company, who has collected millions of instances of brain activity data as people have engaged in mind-controlled car racing, sleeping, working, even using neurofeedback with their devices. They've already entered into partnerships with other companies to share and analyze that data. Unless people have individual control over their brain data, it will be used for microtargeting or worse, instead of treating dementia. Like the employees worldwide who've already been subject to brain surveillance in the workplace to track their attention and fatigue, to governments, developing brain biometrics, to authenticate people at borders, to interrogate criminal suspects' brains and even weapons that are being crafted to disable and disorient the human brain. Brain wearables will have not only read but write capabilities, creating risks that our brains can be hacked, manipulated, and even subject to targeted attacks. We must act quickly to safeguard against the very real and terrifying risks to our innermost selves. Recognizing a human right to cognitive liberty would offer those safeguards. Cognitive liberty is a right from interference by others, but it is also a right to self-determination over our brains and mental experiences to enable human flourishing. To achieve this, we need to recognize three interrelated human rights and update our understanding of them to secure to us a right to mental privacy, to safeguard us from interference with our automatic reactions, our emotions and our thoughts. Freedom of thought as an absolute human right to protect us from interception, manipulation and punishment of our thoughts. And self-determination to secure self-ownership over our brains and mental experiences, to access and change them if we want to do so. There are important efforts already underway from the UN to UNESCO, in nations worldwide, over rights and regulations around neurotechnologies. But those rights need to be better aligned with a broader set of digital rights. Cognitive liberty is an update to liberty in the digital age as an umbrella concept of human flourishing across digital technologies. Because the right way forward isn't through metaverse rights or AI rights or neurotech rights and the like. It's to recognize that these technologies don't exist in silos, but in combination, affecting our brains and mental experiences. We are literally at a moment before. And I mean a moment. Consumer brain wearables have already arrived, and the commodification of our brains has already begun. It's now just a question of scale. We haven't yet passed the inflection point where most of our brains can be directly accessed and changed by others. But it is about to happen, giving us a final moment to make a change so that we don't look back in a few years' time and lament the world we've left behind. We can and should be hopeful and deliberate about the choices we make now to secure a right to self-determination over our brains and mental experiences. The possibilities, if we do so, are limited only by our imagination. Thank you. (Applause)",PT12M36S,2023-05-30T14:51:02Z
Why AI is incredibly smart and shockingly stupid,"science,technology,innovation,future,AI,humanity,teaching,machine learning,ethics",Yejin Choi,"Computer scientist Yejin Choi is here to demystify the current state of massive artificial intelligence systems like ChatGPT, highlighting three key problems with cutting-edge large language models (including some funny instances of them failing at basic commonsense reasoning.) She welcomes us into a new era in which AI is becoming almost like a new intellectual species -- and identifies the benefits of building smaller AI systems trained on human norms and values. (Followed by a Q&A with head of TED Chris Anderson)","So I'm excited to share a few spicy thoughts on artificial intelligence. But first, let's get philosophical by starting with this quote by Voltaire, an 18th century Enlightenment philosopher, who said, ""Common sense is not so common."" Turns out this quote couldn't be more relevant to artificial intelligence today. Despite that, AI is an undeniably powerful tool, beating the world-class ""Go"" champion, acing college admission tests and even passing the bar exam. I’m a computer scientist of 20 years, and I work on artificial intelligence. I am here to demystify AI. So AI today is like a Goliath. It is literally very, very large. It is speculated that the recent ones are trained on tens of thousands of GPUs and a trillion words. Such extreme-scale AI models, often referred to as ""large language models,"" appear to demonstrate sparks of AGI, artificial general intelligence. Except when it makes small, silly mistakes, which it often does. Many believe that whatever mistakes AI makes today can be easily fixed with brute force, bigger scale and more resources. What possibly could go wrong? So there are three immediate challenges we face already at the societal level. First, extreme-scale AI models are so expensive to train, and only a few tech companies can afford to do so. So we already see the concentration of power. But what's worse for AI safety, we are now at the mercy of those few tech companies because researchers in the larger community do not have the means to truly inspect and dissect these models. And let's not forget their massive carbon footprint and the environmental impact. And then there are these additional intellectual questions. Can AI, without robust common sense, be truly safe for humanity? And is brute-force scale really the only way and even the correct way to teach AI? So I’m often asked these days whether it's even feasible to do any meaningful research without extreme-scale compute. And I work at a university and nonprofit research institute, so I cannot afford a massive GPU farm to create enormous language models. Nevertheless, I believe that there's so much we need to do and can do to make AI sustainable and humanistic. We need to make AI smaller, to democratize it. And we need to make AI safer by teaching human norms and values. Perhaps we can draw an analogy from ""David and Goliath,"" here, Goliath being the extreme-scale language models, and seek inspiration from an old-time classic, ""The Art of War,"" which tells us, in my interpretation, know your enemy, choose your battles, and innovate your weapons. Let's start with the first, know your enemy, which means we need to evaluate AI with scrutiny. AI is passing the bar exam. Does that mean that AI is robust at common sense? You might assume so, but you never know. So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. A different one. I have 12-liter jug and six-liter jug, and I want to measure six liters. How do I do it? Just use the six liter jug, right? GPT-4 spits out some very elaborate nonsense. (Laughter) Step one, fill the six-liter jug, step two, pour the water from six to 12-liter jug, step three, fill the six-liter jug again, step four, very carefully, pour the water from six to 12-liter jug. And finally you have six liters of water in the six-liter jug that should be empty by now. (Laughter) OK, one more. Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws and broken glass? Yes, highly likely, GPT-4 says, presumably because it cannot correctly reason that if a bridge is suspended over the broken nails and broken glass, then the surface of the bridge doesn't touch the sharp objects directly. OK, so how would you feel about an AI lawyer that aced the bar exam yet randomly fails at such basic common sense? AI today is unbelievably intelligent and then shockingly stupid. (Laughter) It is an unavoidable side effect of teaching AI through brute-force scale. Some scale optimists might say, “Don’t worry about this. All of these can be easily fixed by adding similar examples as yet more training data for AI."" But the real question is this. Why should we even do that? You are able to get the correct answers right away without having to train yourself with similar examples. Children do not even read a trillion words to acquire such a basic level of common sense. So this observation leads us to the next wisdom, choose your battles. So what fundamental questions should we ask right now and tackle today in order to overcome this status quo with extreme-scale AI? I'll say common sense is among the top priorities. So common sense has been a long-standing challenge in AI. To explain why, let me draw an analogy to dark matter. So only five percent of the universe is normal matter that you can see and interact with, and the remaining 95 percent is dark matter and dark energy. Dark matter is completely invisible, but scientists speculate that it's there because it influences the visible world, even including the trajectory of light. So for language, the normal matter is the visible text, and the dark matter is the unspoken rules about how the world works, including naive physics and folk psychology, which influence the way people use and interpret language. So why is this common sense even important? Well, in a famous thought experiment proposed by Nick Bostrom, AI was asked to produce and maximize the paper clips. And that AI decided to kill humans to utilize them as additional resources, to turn you into paper clips. Because AI didn't have the basic human understanding about human values. Now, writing a better objective and equation that explicitly states: “Do not kill humans” will not work either because AI might go ahead and kill all the trees, thinking that's a perfectly OK thing to do. And in fact, there are endless other things that AI obviously shouldn’t do while maximizing paper clips, including: “Don’t spread the fake news,” “Don’t steal,” “Don’t lie,” which are all part of our common sense understanding about how the world works. However, the AI field for decades has considered common sense as a nearly impossible challenge. So much so that when my students and colleagues and I started working on it several years ago, we were very much discouraged. We’ve been told that it’s a research topic of ’70s and ’80s; shouldn’t work on it because it will never work; in fact, don't even say the word to be taken seriously. Now fast forward to this year, I’m hearing: “Don’t work on it because ChatGPT has almost solved it.” And: “Just scale things up and magic will arise, and nothing else matters.” So my position is that giving true common sense human-like robots common sense to AI, is still moonshot. And you don’t reach to the Moon by making the tallest building in the world one inch taller at a time. Extreme-scale AI models do acquire an ever-more increasing amount of commonsense knowledge, I'll give you that. But remember, they still stumble on such trivial problems that even children can do. So AI today is awfully inefficient. And what if there is an alternative path or path yet to be found? A path that can build on the advancements of the deep neural networks, but without going so extreme with the scale. So this leads us to our final wisdom: innovate your weapons. In the modern-day AI context, that means innovate your data and algorithms. OK, so there are, roughly speaking, three types of data that modern AI is trained on: raw web data, crafted examples custom developed for AI training, and then human judgments, also known as human feedback on AI performance. If the AI is only trained on the first type, raw web data, which is freely available, it's not good because this data is loaded with racism and sexism and misinformation. So no matter how much of it you use, garbage in and garbage out. So the newest, greatest AI systems are now powered with the second and third types of data that are crafted and judged by human workers. It's analogous to writing specialized textbooks for AI to study from and then hiring human tutors to give constant feedback to AI. These are proprietary data, by and large, speculated to cost tens of millions of dollars. We don't know what's in this, but it should be open and publicly available so that we can inspect and ensure [it supports] diverse norms and values. So for this reason, my teams at UW and AI2 have been working on commonsense knowledge graphs as well as moral norm repositories to teach AI basic commonsense norms and morals. Our data is fully open so that anybody can inspect the content and make corrections as needed because transparency is the key for such an important research topic. Now let's think about learning algorithms. No matter how amazing large language models are, by design they may not be the best suited to serve as reliable knowledge models. And these language models do acquire a vast amount of knowledge, but they do so as a byproduct as opposed to direct learning objective. Resulting in unwanted side effects such as hallucinated effects and lack of common sense. Now, in contrast, human learning is never about predicting which word comes next, but it's really about making sense of the world and learning how the world works. Maybe AI should be taught that way as well. So as a quest toward more direct commonsense knowledge acquisition, my team has been investigating potential new algorithms, including symbolic knowledge distillation that can take a very large language model as shown here that I couldn't fit into the screen because it's too large, and crunch that down to much smaller commonsense models using deep neural networks. And in doing so, we also generate, algorithmically, human-inspectable, symbolic, commonsense knowledge representation, so that people can inspect and make corrections and even use it to train other neural commonsense models. More broadly, we have been tackling this seemingly impossible giant puzzle of common sense, ranging from physical, social and visual common sense to theory of minds, norms and morals. Each individual piece may seem quirky and incomplete, but when you step back, it's almost as if these pieces weave together into a tapestry that we call human experience and common sense. We're now entering a new era in which AI is almost like a new intellectual species with unique strengths and weaknesses compared to humans. In order to make this powerful AI sustainable and humanistic, we need to teach AI common sense, norms and values. Thank you. (Applause) Chris Anderson: Look at that. Yejin, please stay one sec. This is so interesting, this idea of common sense. We obviously all really want this from whatever's coming. But help me understand. Like, so we've had this model of a child learning. How does a child gain common sense apart from the accumulation of more input and some, you know, human feedback? What else is there? Yejin Choi: So fundamentally, there are several things missing, but one of them is, for example, the ability to make hypothesis and make experiments, interact with the world and develop this hypothesis. We abstract away the concepts about how the world works, and then that's how we truly learn, as opposed to today's language model. Some of them is really not there quite yet. CA: You use the analogy that we can’t get to the Moon by extending a building a foot at a time. But the experience that most of us have had of these language models is not a foot at a time. It's like, the sort of, breathtaking acceleration. Are you sure that given the pace at which those things are going, each next level seems to be bringing with it what feels kind of like wisdom and knowledge. YC: I totally agree that it's remarkable how much this scaling things up really enhances the performance across the board. So there's real learning happening due to the scale of the compute and data. However, there's a quality of learning that is still not quite there. And the thing is, we don't yet know whether we can fully get there or not just by scaling things up. And if we cannot, then there's this question of what else? And then even if we could, do we like this idea of having very, very extreme-scale AI models that only a few can create and own? CA: I mean, if OpenAI said, you know, ""We're interested in your work, we would like you to help improve our model,"" can you see any way of combining what you're doing with what they have built? YC: Certainly what I envision will need to build on the advancements of deep neural networks. And it might be that there’s some scale Goldilocks Zone, such that ... I'm not imagining that the smaller is the better either, by the way. It's likely that there's right amount of scale, but beyond that, the winning recipe might be something else. So some synthesis of ideas will be critical here. CA: Yejin Choi, thank you so much for your talk. (Applause)",PT16M02S,2023-04-28T14:42:44Z
AI isn't as smart as you think -- but it could be,"technology,future,AI,algorithm,machine learning,ethics",Jeff Dean,"What is AI, really? Jeff Dean, the head of Google's AI efforts, explains the underlying technology that enables artificial intelligence to do all sorts of things, from understanding language to diagnosing disease -- and presents a roadmap for building better, more responsible systems that have a deeper understanding of the world. (Followed by a Q&A with head of TED Chris Anderson)","Hi, I'm Jeff. I lead AI Research and Health at Google. I joined Google more than 20 years ago, when we were all wedged into a tiny office space, above what's now a T-Mobile store in downtown Palo Alto. I've seen a lot of computing transformations in that time, and in the last decade, we've seen AI be able to do tremendous things. But we're still doing it all wrong in many ways. That's what I want to talk to you about today. But first, let's talk about what AI can do. So in the last decade, we've seen tremendous progress in how AI can help computers see, understand language, understand speech better than ever before. Things that we couldn't do before, now we can do. If you think about computer vision alone, just in the last 10 years, computers have effectively developed the ability to see; 10 years ago, they couldn't see, now they can see. You can imagine this has had a transformative effect on what we can do with computers. So let's look at a couple of the great applications enabled by these capabilities. We can better predict flooding, keep everyone safe, using machine learning. We can translate over 100 languages so we all can communicate better, and better predict and diagnose disease, where everyone gets the treatment that they need. So let's look at two key components that underlie the progress in AI systems today. The first is neural networks, a breakthrough approach to solving some of these difficult problems that has really shone in the last 15 years. But they're not a new idea. And the second is computational power. It actually takes a lot of computational power to make neural networks able to really sing, and in the last 15 years, we’ve been able to have that, and that's partly what's enabled all this progress. But at the same time, I think we're doing several things wrong, and that's what I want to talk to you about at the end of the talk. First, a bit of a history lesson. So for decades, almost since the very beginning of computing, people have wanted to be able to build computers that could see, understand language, understand speech. The earliest approaches to this, generally, people were trying to hand-code all the algorithms that you need to accomplish those difficult tasks, and it just turned out to not work very well. But in the last 15 years, a single approach unexpectedly advanced all these different problem spaces all at once: neural networks. So neural networks are not a new idea. They're kind of loosely based on some of the properties that are in real neural systems. And many of the ideas behind neural networks have been around since the 1960s and 70s. A neural network is what it sounds like, a series of interconnected artificial neurons that loosely emulate the properties of your real neurons. An individual neuron in one of these systems has a set of inputs, each with an associated weight, and the output of a neuron is a function of those inputs multiplied by those weights. So pretty simple, and lots and lots of these work together to learn complicated things. So how do we actually learn in a neural network? It turns out the learning process consists of repeatedly making tiny little adjustments to the weight values, strengthening the influence of some things, weakening the influence of others. By driving the overall system towards desired behaviors, these systems can be trained to do really complicated things, like translate from one language to another, detect what kind of objects are in a photo, all kinds of complicated things. I first got interested in neural networks when I took a class on them as an undergraduate in 1990. At that time, neural networks showed impressive results on tiny problems, but they really couldn't scale to do real-world important tasks. But I was super excited. (Laughter) I felt maybe we just needed more compute power. And the University of Minnesota had a 32-processor machine. I thought, ""With more compute power, boy, we could really make neural networks really sing."" So I decided to do a senior thesis on parallel training of neural networks, the idea of using processors in a computer or in a computer system to all work toward the same task, that of training neural networks. 32 processors, wow, we’ve got to be able to do great things with this. But I was wrong. Turns out we needed about a million times as much computational power as we had in 1990 before we could actually get neural networks to do impressive things. But starting around 2005, thanks to the computing progress of Moore's law, we actually started to have that much computing power, and researchers in a few universities around the world started to see success in using neural networks for a wide variety of different kinds of tasks. I and a few others at Google heard about some of these successes, and we decided to start a project to train very large neural networks. One system that we trained, we trained with 10 million randomly selected frames from YouTube videos. The system developed the capability to recognize all kinds of different objects. And it being YouTube, of course, it developed the ability to recognize cats. YouTube is full of cats. (Laughter) But what made that so remarkable is that the system was never told what a cat was. So using just patterns in data, the system honed in on the concept of a cat all on its own. All of this occurred at the beginning of a decade-long string of successes, of using neural networks for a huge variety of tasks, at Google and elsewhere. Many of the things you use every day, things like better speech recognition for your phone, improved understanding of queries and documents for better search quality, better understanding of geographic information to improve maps, and so on. Around that time, we also got excited about how we could build hardware that was better tailored to the kinds of computations neural networks wanted to do. Neural network computations have two special properties. The first is they're very tolerant of reduced precision. Couple of significant digits, you don't need six or seven. And the second is that all the algorithms are generally composed of different sequences of matrix and vector operations. So if you can build a computer that is really good at low-precision matrix and vector operations but can't do much else, that's going to be great for neural-network computation, even though you can't use it for a lot of other things. And if you build such things, people will find amazing uses for them. This is the first one we built, TPU v1. ""TPU"" stands for Tensor Processing Unit. These have been used for many years behind every Google search, for translation, in the DeepMind AlphaGo matches, so Lee Sedol and Ke Jie maybe didn't realize, but they were competing against racks of TPU cards. And we've built a bunch of subsequent versions of TPUs that are even better and more exciting. But despite all these successes, I think we're still doing many things wrong, and I'll tell you about three key things we're doing wrong, and how we'll fix them. The first is that most neural networks today are trained to do one thing, and one thing only. You train it for a particular task that you might care deeply about, but it's a pretty heavyweight activity. You need to curate a data set, you need to decide what network architecture you'll use for this problem, you need to initialize the weights with random values, apply lots of computation to make adjustments to the weights. And at the end, if you’re lucky, you end up with a model that is really good at that task you care about. But if you do this over and over, you end up with thousands of separate models, each perhaps very capable, but separate for all the different tasks you care about. But think about how people learn. In the last year, many of us have picked up a bunch of new skills. I've been honing my gardening skills, experimenting with vertical hydroponic gardening. To do that, I didn't need to relearn everything I already knew about plants. I was able to know how to put a plant in a hole, how to pour water, that plants need sun, and leverage that in learning this new skill. Computers can work the same way, but they don’t today. If you train a neural network from scratch, it's effectively like forgetting your entire education every time you try to do something new. That’s crazy, right? So instead, I think we can and should be training multitask models that can do thousands or millions of different tasks. Each part of that model would specialize in different kinds of things. And then, if we have a model that can do a thousand things, and the thousand and first thing comes along, we can leverage the expertise we already have in the related kinds of things so that we can more quickly be able to do this new task, just like you, if you're confronted with some new problem, you quickly identify the 17 things you already know that are going to be helpful in solving that problem. Second problem is that most of our models today deal with only a single modality of data -- with images, or text or speech, but not all of these all at once. But think about how you go about the world. You're continuously using all your senses to learn from, react to, figure out what actions you want to take in the world. Makes a lot more sense to do that, and we can build models in the same way. We can build models that take in these different modalities of input data, text, images, speech, but then fuse them together, so that regardless of whether the model sees the word ""leopard,"" sees a video of a leopard or hears someone say the word ""leopard,"" the same response is triggered inside the model: the concept of a leopard can deal with different kinds of input data, even nonhuman inputs, like genetic sequences, 3D clouds of points, as well as images, text and video. The third problem is that today's models are dense. There's a single model, the model is fully activated for every task, for every example that we want to accomplish, whether that's a really simple or a really complicated thing. This, too, is unlike how our own brains work. Different parts of our brains are good at different things, and we're continuously calling upon the pieces of them that are relevant for the task at hand. For example, nervously watching a garbage truck back up towards your car, the part of your brain that thinks about Shakespearean sonnets is probably inactive. (Laughter) AI models can work the same way. Instead of a dense model, we can have one that is sparsely activated. So for particular different tasks, we call upon different parts of the model. During training, the model can also learn which parts are good at which things, to continuously identify what parts it wants to call upon in order to accomplish a new task. The advantage of this is we can have a very high-capacity model, but it's very efficient, because we're only calling upon the parts that we need for any given task. So fixing these three things, I think, will lead to a more powerful AI system: instead of thousands of separate models, train a handful of general-purpose models that can do thousands or millions of things. Instead of dealing with single modalities, deal with all modalities, and be able to fuse them together. And instead of dense models, use sparse, high-capacity models, where we call upon the relevant bits as we need them. We've been building a system that enables these kinds of approaches, and we’ve been calling the system “Pathways.” So the idea is this model will be able to do thousands or millions of different tasks, and then, we can incrementally add new tasks, and it can deal with all modalities at once, and then incrementally learn new tasks as needed and call upon the relevant bits of the model for different examples or tasks. And we're pretty excited about this, we think this is going to be a step forward in how we build AI systems. But I also wanted to touch on responsible AI. We clearly need to make sure that this vision of powerful AI systems benefits everyone. These kinds of models raise important new questions about how do we build them with fairness, interpretability, privacy and security, for all users in mind. For example, if we're going to train these models on thousands or millions of tasks, we'll need to be able to train them on large amounts of data. And we need to make sure that data is thoughtfully collected and is representative of different communities and situations all around the world. And data concerns are only one aspect of responsible AI. We have a lot of work to do here. So in 2018, Google published this set of AI principles by which we think about developing these kinds of technology. And these have helped guide us in how we do research in this space, how we use AI in our products. And I think it's a really helpful and important framing for how to think about these deep and complex questions about how we should be using AI in society. We continue to update these as we learn more. Many of these kinds of principles are active areas of research -- super important area. Moving from single-purpose systems that kind of recognize patterns in data to these kinds of general-purpose intelligent systems that have a deeper understanding of the world will really enable us to tackle some of the greatest problems humanity faces. For example, we’ll be able to diagnose more disease; we'll be able to engineer better medicines by infusing these models with knowledge of chemistry and physics; we'll be able to advance educational systems by providing more individualized tutoring to help people learn in new and better ways; we’ll be able to tackle really complicated issues, like climate change, and perhaps engineering of clean energy solutions. So really, all of these kinds of systems are going to be requiring the multidisciplinary expertise of people all over the world. So connecting AI with whatever field you are in, in order to make progress. So I've seen a lot of advances in computing, and how computing, over the past decades, has really helped millions of people better understand the world around them. And AI today has the potential to help billions of people. We truly live in exciting times. Thank you. (Applause) Chris Anderson: Thank you so much. I want to follow up on a couple things. This is what I heard. Most people's traditional picture of AI is that computers recognize a pattern of information, and with a bit of machine learning, they can get really good at that, better than humans. What you're saying is those patterns are no longer the atoms that AI is working with, that it's much richer-layered concepts that can include all manners of types of things that go to make up a leopard, for example. So what could that lead to? Give me an example of when that AI is working, what do you picture happening in the world in the next five or 10 years that excites you? Jeff Dean: I think the grand challenge in AI is how do you generalize from a set of tasks you already know how to do to new tasks, as easily and effortlessly as possible. And the current approach of training separate models for everything means you need lots of data about that particular problem, because you're effectively trying to learn everything about the world and that problem, from nothing. But if you can build these systems that already are infused with how to do thousands and millions of tasks, then you can effectively teach them to do a new thing with relatively few examples. So I think that's the real hope, that you could then have a system where you just give it five examples of something you care about, and it learns to do that new task. CA: You can do a form of self-supervised learning that is based on remarkably little seeding. JD: Yeah, as opposed to needing 10,000 or 100,000 examples to figure everything in the world out. CA: Aren't there kind of terrifying unintended consequences possible, from that? JD: I think it depends on how you apply these systems. It's very clear that AI can be a powerful system for good, or if you apply it in ways that are not so great, it can be a negative consequence. So I think that's why it's important to have a set of principles by which you look at potential uses of AI and really are careful and thoughtful about how you consider applications. CA: One of the things people worry most about is that, if AI is so good at learning from the world as it is, it's going to carry forward into the future aspects of the world as it is that actually aren't right, right now. And there's obviously been a huge controversy about that recently at Google. Some of those principles of AI development, you've been challenged that you're not actually holding to them. Not really interested to hear about comments on a specific case, but ... are you really committed? How do we know that you are committed to these principles? Is that just PR, or is that real, at the heart of your day-to-day? JD: No, that is absolutely real. Like, we have literally hundreds of people working on many of these related research issues, because many of those things are research topics in their own right. How do you take data from the real world, that is the world as it is, not as we would like it to be, and how do you then use that to train a machine-learning model and adapt the data bit of the scene or augment the data with additional data so that it can better reflect the values we want the system to have, not the values that it sees in the world? CA: But you work for Google, Google is funding the research. How do we know that the main values that this AI will build are for the world, and not, for example, to maximize the profitability of an ad model? When you know everything there is to know about human attention, you're going to know so much about the little wriggly, weird, dark parts of us. In your group, are there rules about how you hold off, church-state wall between a sort of commercial push, ""You must do it for this purpose,"" so that you can inspire your engineers and so forth, to do this for the world, for all of us. JD: Yeah, our research group does collaborate with a number of groups across Google, including the Ads group, the Search group, the Maps group, so we do have some collaboration, but also a lot of basic research that we publish openly. We've published more than 1,000 papers last year in different topics, including the ones you discussed, about fairness, interpretability of the machine-learning models, things that are super important, and we need to advance the state of the art in this in order to continue to make progress to make sure these models are developed safely and responsibly. CA: It feels like we're at a time when people are concerned about the power of the big tech companies, and it's almost, if there was ever a moment to really show the world that this is being done to make a better future, that is actually key to Google's future, as well as all of ours. JD: Indeed. CA: It's very good to hear you come and say that, Jeff. Thank you so much for coming here to TED. JD: Thank you. (Applause)",PT18M21S,2021-10-28T19:51:36Z
What moral decisions should driverless cars make?,"technology,transportation,innovation,AI,law,TEDx,driverless cars,ethics",Iyad Rahwan,"Should your driverless car kill you if it means saving five pedestrians? In this primer on the social dilemmas of driverless cars, Iyad Rahwan explores how the technology will challenge our morality and explains his work collecting data from real people on the ethical trade-offs we're willing (and not willing) to make.","Today I'm going to talk about technology and society. The Department of Transport estimated that last year 35,000 people died from traffic crashes in the US alone. Worldwide, 1.2 million people die every year in traffic accidents. If there was a way we could eliminate 90 percent of those accidents, would you support it? Of course you would. This is what driverless car technology promises to achieve by eliminating the main source of accidents -- human error. Now picture yourself in a driverless car in the year 2030, sitting back and watching this vintage TEDxCambridge video. (Laughter) All of a sudden, the car experiences mechanical failure and is unable to stop. If the car continues, it will crash into a bunch of pedestrians crossing the street, but the car may swerve, hitting one bystander, killing them to save the pedestrians. What should the car do, and who should decide? What if instead the car could swerve into a wall, crashing and killing you, the passenger, in order to save those pedestrians? This scenario is inspired by the trolley problem, which was invented by philosophers a few decades ago to think about ethics. Now, the way we think about this problem matters. We may for example not think about it at all. We may say this scenario is unrealistic, incredibly unlikely, or just silly. But I think this criticism misses the point because it takes the scenario too literally. Of course no accident is going to look like this; no accident has two or three options where everybody dies somehow. Instead, the car is going to calculate something like the probability of hitting a certain group of people, if you swerve one direction versus another direction, you might slightly increase the risk to passengers or other drivers versus pedestrians. It's going to be a more complex calculation, but it's still going to involve trade-offs, and trade-offs often require ethics. We might say then, ""Well, let's not worry about this. Let's wait until technology is fully ready and 100 percent safe."" Suppose that we can indeed eliminate 90 percent of those accidents, or even 99 percent in the next 10 years. What if eliminating the last one percent of accidents requires 50 more years of research? Should we not adopt the technology? That's 60 million people dead in car accidents if we maintain the current rate. So the point is, waiting for full safety is also a choice, and it also involves trade-offs. People online on social media have been coming up with all sorts of ways to not think about this problem. One person suggested the car should just swerve somehow in between the passengers -- (Laughter) and the bystander. Of course if that's what the car can do, that's what the car should do. We're interested in scenarios in which this is not possible. And my personal favorite was a suggestion by a blogger to have an eject button in the car that you press -- (Laughter) just before the car self-destructs. (Laughter) So if we acknowledge that cars will have to make trade-offs on the road, how do we think about those trade-offs, and how do we decide? Well, maybe we should run a survey to find out what society wants, because ultimately, regulations and the law are a reflection of societal values. So this is what we did. With my collaborators, Jean-François Bonnefon and Azim Shariff, we ran a survey in which we presented people with these types of scenarios. We gave them two options inspired by two philosophers: Jeremy Bentham and Immanuel Kant. Bentham says the car should follow utilitarian ethics: it should take the action that will minimize total harm -- even if that action will kill a bystander and even if that action will kill the passenger. Immanuel Kant says the car should follow duty-bound principles, like ""Thou shalt not kill."" So you should not take an action that explicitly harms a human being, and you should let the car take its course even if that's going to harm more people. What do you think? Bentham or Kant? Here's what we found. Most people sided with Bentham. So it seems that people want cars to be utilitarian, minimize total harm, and that's what we should all do. Problem solved. But there is a little catch. When we asked people whether they would purchase such cars, they said, ""Absolutely not."" (Laughter) They would like to buy cars that protect them at all costs, but they want everybody else to buy cars that minimize harm. (Laughter) We've seen this problem before. It's called a social dilemma. And to understand the social dilemma, we have to go a little bit back in history. In the 1800s, English economist William Forster Lloyd published a pamphlet which describes the following scenario. You have a group of farmers -- English farmers -- who are sharing a common land for their sheep to graze. Now, if each farmer brings a certain number of sheep -- let's say three sheep -- the land will be rejuvenated, the farmers are happy, the sheep are happy, everything is good. Now, if one farmer brings one extra sheep, that farmer will do slightly better, and no one else will be harmed. But if every farmer made that individually rational decision, the land will be overrun, and it will be depleted to the detriment of all the farmers, and of course, to the detriment of the sheep. We see this problem in many places: in the difficulty of managing overfishing, or in reducing carbon emissions to mitigate climate change. When it comes to the regulation of driverless cars, the common land now is basically public safety -- that's the common good -- and the farmers are the passengers or the car owners who are choosing to ride in those cars. And by making the individually rational choice of prioritizing their own safety, they may collectively be diminishing the common good, which is minimizing total harm. It's called the tragedy of the commons, traditionally, but I think in the case of driverless cars, the problem may be a little bit more insidious because there is not necessarily an individual human being making those decisions. So car manufacturers may simply program cars that will maximize safety for their clients, and those cars may learn automatically on their own that doing so requires slightly increasing risk for pedestrians. So to use the sheep metaphor, it's like we now have electric sheep that have a mind of their own. (Laughter) And they may go and graze even if the farmer doesn't know it. So this is what we may call the tragedy of the algorithmic commons, and if offers new types of challenges. Typically, traditionally, we solve these types of social dilemmas using regulation, so either governments or communities get together, and they decide collectively what kind of outcome they want and what sort of constraints on individual behavior they need to implement. And then using monitoring and enforcement, they can make sure that the public good is preserved. So why don't we just, as regulators, require that all cars minimize harm? After all, this is what people say they want. And more importantly, I can be sure that as an individual, if I buy a car that may sacrifice me in a very rare case, I'm not the only sucker doing that while everybody else enjoys unconditional protection. In our survey, we did ask people whether they would support regulation and here's what we found. First of all, people said no to regulation; and second, they said, ""Well if you regulate cars to do this and to minimize total harm, I will not buy those cars."" So ironically, by regulating cars to minimize harm, we may actually end up with more harm because people may not opt into the safer technology even if it's much safer than human drivers. I don't have the final answer to this riddle, but I think as a starting point, we need society to come together to decide what trade-offs we are comfortable with and to come up with ways in which we can enforce those trade-offs. As a starting point, my brilliant students, Edmond Awad and Sohan Dsouza, built the Moral Machine website, which generates random scenarios at you -- basically a bunch of random dilemmas in a sequence where you have to choose what the car should do in a given scenario. And we vary the ages and even the species of the different victims. So far we've collected over five million decisions by over one million people worldwide from the website. And this is helping us form an early picture of what trade-offs people are comfortable with and what matters to them -- even across cultures. But more importantly, doing this exercise is helping people recognize the difficulty of making those choices and that the regulators are tasked with impossible choices. And maybe this will help us as a society understand the kinds of trade-offs that will be implemented ultimately in regulation. And indeed, I was very happy to hear that the first set of regulations that came from the Department of Transport -- announced last week -- included a 15-point checklist for all carmakers to provide, and number 14 was ethical consideration -- how are you going to deal with that. We also have people reflect on their own decisions by giving them summaries of what they chose. I'll give you one example -- I'm just going to warn you that this is not your typical example, your typical user. This is the most sacrificed and the most saved character for this person. (Laughter) Some of you may agree with him, or her, we don't know. But this person also seems to slightly prefer passengers over pedestrians in their choices and is very happy to punish jaywalking. (Laughter) So let's wrap up. We started with the question -- let's call it the ethical dilemma -- of what the car should do in a specific scenario: swerve or stay? But then we realized that the problem was a different one. It was the problem of how to get society to agree on and enforce the trade-offs they're comfortable with. It's a social dilemma. In the 1940s, Isaac Asimov wrote his famous laws of robotics -- the three laws of robotics. A robot may not harm a human being, a robot may not disobey a human being, and a robot may not allow itself to come to harm -- in this order of importance. But after 40 years or so and after so many stories pushing these laws to the limit, Asimov introduced the zeroth law which takes precedence above all, and it's that a robot may not harm humanity as a whole. I don't know what this means in the context of driverless cars or any specific situation, and I don't know how we can implement it, but I think that by recognizing that the regulation of driverless cars is not only a technological problem but also a societal cooperation problem, I hope that we can at least begin to ask the right questions. Thank you. (Applause)",PT13M26S,2017-08-22T19:40:47Z
3 principles for creating safer AI,"science,technology,computers,innovation,robots,future,communication,decision-making,AI,humanity,data,algorithm,code,machine learning,ethics",Stuart Russell,"How can we harness the power of superintelligent AI while also preventing the catastrophe of robotic takeover? As we move closer toward creating all-knowing machines, AI pioneer Stuart Russell is working on something a bit different: robots with uncertainty. Hear his vision for human-compatible AI that can solve problems using common sense, altruism and other human values.","This is Lee Sedol. Lee Sedol is one of the world's greatest Go players, and he's having what my friends in Silicon Valley call a ""Holy Cow"" moment -- (Laughter) a moment where we realize that AI is actually progressing a lot faster than we expected. So humans have lost on the Go board. What about the real world? Well, the real world is much bigger, much more complicated than the Go board. It's a lot less visible, but it's still a decision problem. And if we think about some of the technologies that are coming down the pike ... Noriko [Arai] mentioned that reading is not yet happening in machines, at least with understanding. But that will happen, and when that happens, very soon afterwards, machines will have read everything that the human race has ever written. And that will enable machines, along with the ability to look further ahead than humans can, as we've already seen in Go, if they also have access to more information, they'll be able to make better decisions in the real world than we can. So is that a good thing? Well, I hope so. Our entire civilization, everything that we value, is based on our intelligence. And if we had access to a lot more intelligence, then there's really no limit to what the human race can do. And I think this could be, as some people have described it, the biggest event in human history. So why are people saying things like this, that AI might spell the end of the human race? Is this a new thing? Is it just Elon Musk and Bill Gates and Stephen Hawking? Actually, no. This idea has been around for a while. Here's a quotation: ""Even if we could keep the machines in a subservient position, for instance, by turning off the power at strategic moments"" -- and I'll come back to that ""turning off the power"" idea later on -- ""we should, as a species, feel greatly humbled."" So who said this? This is Alan Turing in 1951. Alan Turing, as you know, is the father of computer science and in many ways, the father of AI as well. So if we think about this problem, the problem of creating something more intelligent than your own species, we might call this ""the gorilla problem,"" because gorillas' ancestors did this a few million years ago, and now we can ask the gorillas: Was this a good idea? So here they are having a meeting to discuss whether it was a good idea, and after a little while, they conclude, no, this was a terrible idea. Our species is in dire straits. In fact, you can see the existential sadness in their eyes. (Laughter) So this queasy feeling that making something smarter than your own species is maybe not a good idea -- what can we do about that? Well, really nothing, except stop doing AI, and because of all the benefits that I mentioned and because I'm an AI researcher, I'm not having that. I actually want to be able to keep doing AI. So we actually need to nail down the problem a bit more. What exactly is the problem? Why is better AI possibly a catastrophe? So here's another quotation: ""We had better be quite sure that the purpose put into the machine is the purpose which we really desire."" This was said by Norbert Wiener in 1960, shortly after he watched one of the very early learning systems learn to play checkers better than its creator. But this could equally have been said by King Midas. King Midas said, ""I want everything I touch to turn to gold,"" and he got exactly what he asked for. That was the purpose that he put into the machine, so to speak, and then his food and his drink and his relatives turned to gold and he died in misery and starvation. So we'll call this ""the King Midas problem"" of stating an objective which is not, in fact, truly aligned with what we want. In modern terms, we call this ""the value alignment problem."" Putting in the wrong objective is not the only part of the problem. There's another part. If you put an objective into a machine, even something as simple as, ""Fetch the coffee,"" the machine says to itself, ""Well, how might I fail to fetch the coffee? Someone might switch me off. OK, I have to take steps to prevent that. I will disable my 'off' switch. I will do anything to defend myself against interference with this objective that I have been given."" So this single-minded pursuit in a very defensive mode of an objective that is, in fact, not aligned with the true objectives of the human race -- that's the problem that we face. And in fact, that's the high-value takeaway from this talk. If you want to remember one thing, it's that you can't fetch the coffee if you're dead. (Laughter) It's very simple. Just remember that. Repeat it to yourself three times a day. (Laughter) And in fact, this is exactly the plot of ""2001: [A Space Odyssey]"" HAL has an objective, a mission, which is not aligned with the objectives of the humans, and that leads to this conflict. Now fortunately, HAL is not superintelligent. He's pretty smart, but eventually Dave outwits him and manages to switch him off. But we might not be so lucky. So what are we going to do? I'm trying to redefine AI to get away from this classical notion of machines that intelligently pursue objectives. There are three principles involved. The first one is a principle of altruism, if you like, that the robot's only objective is to maximize the realization of human objectives, of human values. And by values here I don't mean touchy-feely, goody-goody values. I just mean whatever it is that the human would prefer their life to be like. And so this actually violates Asimov's law that the robot has to protect its own existence. It has no interest in preserving its existence whatsoever. The second law is a law of humility, if you like. And this turns out to be really important to make robots safe. It says that the robot does not know what those human values are, so it has to maximize them, but it doesn't know what they are. And that avoids this problem of single-minded pursuit of an objective. This uncertainty turns out to be crucial. Now, in order to be useful to us, it has to have some idea of what we want. It obtains that information primarily by observation of human choices, so our own choices reveal information about what it is that we prefer our lives to be like. So those are the three principles. Let's see how that applies to this question of: ""Can you switch the machine off?"" as Turing suggested. So here's a PR2 robot. This is one that we have in our lab, and it has a big red ""off"" switch right on the back. The question is: Is it going to let you switch it off? If we do it the classical way, we give it the objective of, ""Fetch the coffee, I must fetch the coffee, I can't fetch the coffee if I'm dead,"" so obviously the PR2 has been listening to my talk, and so it says, therefore, ""I must disable my 'off' switch, and probably taser all the other people in Starbucks who might interfere with me."" (Laughter) So this seems to be inevitable, right? This kind of failure mode seems to be inevitable, and it follows from having a concrete, definite objective. So what happens if the machine is uncertain about the objective? Well, it reasons in a different way. It says, ""OK, the human might switch me off, but only if I'm doing something wrong. Well, I don't really know what wrong is, but I know that I don't want to do it."" So that's the first and second principles right there. ""So I should let the human switch me off."" And in fact you can calculate the incentive that the robot has to allow the human to switch it off, and it's directly tied to the degree of uncertainty about the underlying objective. And then when the machine is switched off, that third principle comes into play. It learns something about the objectives it should be pursuing, because it learns that what it did wasn't right. In fact, we can, with suitable use of Greek symbols, as mathematicians usually do, we can actually prove a theorem that says that such a robot is provably beneficial to the human. You are provably better off with a machine that's designed in this way than without it. So this is a very simple example, but this is the first step in what we're trying to do with human-compatible AI. Now, this third principle, I think is the one that you're probably scratching your head over. You're probably thinking, ""Well, you know, I behave badly. I don't want my robot to behave like me. I sneak down in the middle of the night and take stuff from the fridge. I do this and that."" There's all kinds of things you don't want the robot doing. But in fact, it doesn't quite work that way. Just because you behave badly doesn't mean the robot is going to copy your behavior. It's going to understand your motivations and maybe help you resist them, if appropriate. But it's still difficult. What we're trying to do, in fact, is to allow machines to predict for any person and for any possible life that they could live, and the lives of everybody else: Which would they prefer? And there are many, many difficulties involved in doing this; I don't expect that this is going to get solved very quickly. The real difficulties, in fact, are us. As I have already mentioned, we behave badly. In fact, some of us are downright nasty. Now the robot, as I said, doesn't have to copy the behavior. The robot does not have any objective of its own. It's purely altruistic. And it's not designed just to satisfy the desires of one person, the user, but in fact it has to respect the preferences of everybody. So it can deal with a certain amount of nastiness, and it can even understand that your nastiness, for example, you may take bribes as a passport official because you need to feed your family and send your kids to school. It can understand that; it doesn't mean it's going to steal. In fact, it'll just help you send your kids to school. We are also computationally limited. Lee Sedol is a brilliant Go player, but he still lost. So if we look at his actions, he took an action that lost the game. That doesn't mean he wanted to lose. So to understand his behavior, we actually have to invert through a model of human cognition that includes our computational limitations -- a very complicated model. But it's still something that we can work on understanding. Probably the most difficult part, from my point of view as an AI researcher, is the fact that there are lots of us, and so the machine has to somehow trade off, weigh up the preferences of many different people, and there are different ways to do that. Economists, sociologists, moral philosophers have understood that, and we are actively looking for collaboration. Let's have a look and see what happens when you get that wrong. So you can have a conversation, for example, with your intelligent personal assistant that might be available in a few years' time. Think of a Siri on steroids. So Siri says, ""Your wife called to remind you about dinner tonight."" And of course, you've forgotten. ""What? What dinner? What are you talking about?"" ""Uh, your 20th anniversary at 7pm."" ""I can't do that. I'm meeting with the secretary-general at 7:30. How could this have happened?"" ""Well, I did warn you, but you overrode my recommendation."" ""Well, what am I going to do? I can't just tell him I'm too busy."" ""Don't worry. I arranged for his plane to be delayed."" (Laughter) ""Some kind of computer malfunction."" (Laughter) ""Really? You can do that?"" ""He sends his profound apologies and looks forward to meeting you for lunch tomorrow."" (Laughter) So the values here -- there's a slight mistake going on. This is clearly following my wife's values which is ""Happy wife, happy life."" (Laughter) It could go the other way. You could come home after a hard day's work, and the computer says, ""Long day?"" ""Yes, I didn't even have time for lunch."" ""You must be very hungry."" ""Starving, yeah. Could you make some dinner?"" ""There's something I need to tell you."" (Laughter) ""There are humans in South Sudan who are in more urgent need than you."" (Laughter) ""So I'm leaving. Make your own dinner."" (Laughter) So we have to solve these problems, and I'm looking forward to working on them. There are reasons for optimism. One reason is, there is a massive amount of data. Because remember -- I said they're going to read everything the human race has ever written. Most of what we write about is human beings doing things and other people getting upset about it. So there's a massive amount of data to learn from. There's also a very strong economic incentive to get this right. So imagine your domestic robot's at home. You're late from work again and the robot has to feed the kids, and the kids are hungry and there's nothing in the fridge. And the robot sees the cat. (Laughter) And the robot hasn't quite learned the human value function properly, so it doesn't understand the sentimental value of the cat outweighs the nutritional value of the cat. (Laughter) So then what happens? Well, it happens like this: ""Deranged robot cooks kitty for family dinner."" That one incident would be the end of the domestic robot industry. So there's a huge incentive to get this right long before we reach superintelligent machines. So to summarize: I'm actually trying to change the definition of AI so that we have provably beneficial machines. And the principles are: machines that are altruistic, that want to achieve only our objectives, but that are uncertain about what those objectives are, and will watch all of us to learn more about what it is that we really want. And hopefully in the process, we will learn to be better people. Thank you very much. (Applause) Chris Anderson: So interesting, Stuart. We're going to stand here a bit because I think they're setting up for our next speaker. A couple of questions. So the idea of programming in ignorance seems intuitively really powerful. As you get to superintelligence, what's going to stop a robot reading literature and discovering this idea that knowledge is actually better than ignorance and still just shifting its own goals and rewriting that programming? Stuart Russell: Yes, so we want it to learn more, as I said, about our objectives. It'll only become more certain as it becomes more correct, so the evidence is there and it's going to be designed to interpret it correctly. It will understand, for example, that books are very biased in the evidence they contain. They only talk about kings and princes and elite white male people doing stuff. So it's a complicated problem, but as it learns more about our objectives it will become more and more useful to us. CA: And you couldn't just boil it down to one law, you know, hardwired in: ""if any human ever tries to switch me off, I comply. I comply."" SR: Absolutely not. That would be a terrible idea. So imagine that you have a self-driving car and you want to send your five-year-old off to preschool. Do you want your five-year-old to be able to switch off the car while it's driving along? Probably not. So it needs to understand how rational and sensible the person is. The more rational the person, the more willing you are to be switched off. If the person is completely random or even malicious, then you're less willing to be switched off. CA: All right. Stuart, can I just say, I really, really hope you figure this out for us. Thank you so much for that talk. That was amazing. SR: Thank you. (Applause)",PT17M25S,2017-05-15T14:29:46Z
Machine intelligence makes human morals more important,"science,technology,computers,software,robots,potential,decision-making,AI,humanity,society,data,algorithm,machine learning,ethics,online privacy",Zeynep Tufekci,"Machine intelligence is here, and we're already using it to make subjective decisions. But the complex way AI grows and improves makes it hard to understand and even harder to control. In this cautionary talk, techno-sociologist Zeynep Tufekci explains how intelligent machines can fail in ways that don't fit human error patterns -- and in ways we won't expect or be prepared for. ""We cannot outsource our responsibilities to machines,"" she says. ""We must hold on ever tighter to human values and human ethics.""","So, I started my first job as a computer programmer in my very first year of college -- basically, as a teenager. Soon after I started working, writing software in a company, a manager who worked at the company came down to where I was, and he whispered to me, ""Can he tell if I'm lying?"" There was nobody else in the room. ""Can who tell if you're lying? And why are we whispering?"" The manager pointed at the computer in the room. ""Can he tell if I'm lying?"" Well, that manager was having an affair with the receptionist. (Laughter) And I was still a teenager. So I whisper-shouted back to him, ""Yes, the computer can tell if you're lying."" (Laughter) Well, I laughed, but actually, the laugh's on me. Nowadays, there are computational systems that can suss out emotional states and even lying from processing human faces. Advertisers and even governments are very interested. I had become a computer programmer because I was one of those kids crazy about math and science. But somewhere along the line I'd learned about nuclear weapons, and I'd gotten really concerned with the ethics of science. I was troubled. However, because of family circumstances, I also needed to start working as soon as possible. So I thought to myself, hey, let me pick a technical field where I can get a job easily and where I don't have to deal with any troublesome questions of ethics. So I picked computers. (Laughter) Well, ha, ha, ha! All the laughs are on me. Nowadays, computer scientists are building platforms that control what a billion people see every day. They're developing cars that could decide who to run over. They're even building machines, weapons, that might kill human beings in war. It's ethics all the way down. Machine intelligence is here. We're now using computation to make all sort of decisions, but also new kinds of decisions. We're asking questions to computation that have no single right answers, that are subjective and open-ended and value-laden. We're asking questions like, ""Who should the company hire?"" ""Which update from which friend should you be shown?"" ""Which convict is more likely to reoffend?"" ""Which news item or movie should be recommended to people?"" Look, yes, we've been using computers for a while, but this is different. This is a historical twist, because we cannot anchor computation for such subjective decisions the way we can anchor computation for flying airplanes, building bridges, going to the moon. Are airplanes safer? Did the bridge sway and fall? There, we have agreed-upon, fairly clear benchmarks, and we have laws of nature to guide us. We have no such anchors and benchmarks for decisions in messy human affairs. To make things more complicated, our software is getting more powerful, but it's also getting less transparent and more complex. Recently, in the past decade, complex algorithms have made great strides. They can recognize human faces. They can decipher handwriting. They can detect credit card fraud and block spam and they can translate between languages. They can detect tumors in medical imaging. They can beat humans in chess and Go. Much of this progress comes from a method called ""machine learning."" Machine learning is different than traditional programming, where you give the computer detailed, exact, painstaking instructions. It's more like you take the system and you feed it lots of data, including unstructured data, like the kind we generate in our digital lives. And the system learns by churning through this data. And also, crucially, these systems don't operate under a single-answer logic. They don't produce a simple answer; it's more probabilistic: ""This one is probably more like what you're looking for."" Now, the upside is: this method is really powerful. The head of Google's AI systems called it, ""the unreasonable effectiveness of data."" The downside is, we don't really understand what the system learned. In fact, that's its power. This is less like giving instructions to a computer; it's more like training a puppy-machine-creature we don't really understand or control. So this is our problem. It's a problem when this artificial intelligence system gets things wrong. It's also a problem when it gets things right, because we don't even know which is which when it's a subjective problem. We don't know what this thing is thinking. So, consider a hiring algorithm -- a system used to hire people, using machine-learning systems. Such a system would have been trained on previous employees' data and instructed to find and hire people like the existing high performers in the company. Sounds good. I once attended a conference that brought together human resources managers and executives, high-level people, using such systems in hiring. They were super excited. They thought that this would make hiring more objective, less biased, and give women and minorities a better shot against biased human managers. And look -- human hiring is biased. I know. I mean, in one of my early jobs as a programmer, my immediate manager would sometimes come down to where I was really early in the morning or really late in the afternoon, and she'd say, ""Zeynep, let's go to lunch!"" I'd be puzzled by the weird timing. It's 4pm. Lunch? I was broke, so free lunch. I always went. I later realized what was happening. My immediate managers had not confessed to their higher-ups that the programmer they hired for a serious job was a teen girl who wore jeans and sneakers to work. I was doing a good job, I just looked wrong and was the wrong age and gender. So hiring in a gender- and race-blind way certainly sounds good to me. But with these systems, it is more complicated, and here's why: Currently, computational systems can infer all sorts of things about you from your digital crumbs, even if you have not disclosed those things. They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy. Remember -- for things you haven't even disclosed. This is inference. I have a friend who developed such computational systems to predict the likelihood of clinical or postpartum depression from social media data. The results are impressive. Her system can predict the likelihood of depression months before the onset of any symptoms -- months before. No symptoms, there's prediction. She hopes it will be used for early intervention. Great! But now put this in the context of hiring. So at this human resources managers conference, I approached a high-level manager in a very large company, and I said to her, ""Look, what if, unbeknownst to you, your system is weeding out people with high future likelihood of depression? They're not depressed now, just maybe in the future, more likely. What if it's weeding out women more likely to be pregnant in the next year or two but aren't pregnant now? What if it's hiring aggressive people because that's your workplace culture?"" You can't tell this by looking at gender breakdowns. Those may be balanced. And since this is machine learning, not traditional coding, there is no variable there labeled ""higher risk of depression,"" ""higher risk of pregnancy,"" ""aggressive guy scale."" Not only do you not know what your system is selecting on, you don't even know where to begin to look. It's a black box. It has predictive power, but you don't understand it. ""What safeguards,"" I asked, ""do you have to make sure that your black box isn't doing something shady?"" She looked at me as if I had just stepped on 10 puppy tails. (Laughter) She stared at me and she said, ""I don't want to hear another word about this."" And she turned around and walked away. Mind you -- she wasn't rude. It was clearly: what I don't know isn't my problem, go away, death stare. (Laughter) Look, such a system may even be less biased than human managers in some ways. And it could make monetary sense. But it could also lead to a steady but stealthy shutting out of the job market of people with higher risk of depression. Is this the kind of society we want to build, without even knowing we've done this, because we turned decision-making to machines we don't totally understand? Another problem is this: these systems are often trained on data generated by our actions, human imprints. Well, they could just be reflecting our biases, and these systems could be picking up on our biases and amplifying them and showing them back to us, while we're telling ourselves, ""We're just doing objective, neutral computation."" Researchers found that on Google, women are less likely than men to be shown job ads for high-paying jobs. And searching for African-American names is more likely to bring up ads suggesting criminal history, even when there is none. Such hidden biases and black-box algorithms that researchers uncover sometimes but sometimes we don't know, can have life-altering consequences. In Wisconsin, a defendant was sentenced to six years in prison for evading the police. You may not know this, but algorithms are increasingly used in parole and sentencing decisions. He wanted to know: How is this score calculated? It's a commercial black box. The company refused to have its algorithm be challenged in open court. But ProPublica, an investigative nonprofit, audited that very algorithm with what public data they could find, and found that its outcomes were biased and its predictive power was dismal, barely better than chance, and it was wrongly labeling black defendants as future criminals at twice the rate of white defendants. So, consider this case: This woman was late picking up her godsister from a school in Broward County, Florida, running down the street with a friend of hers. They spotted an unlocked kid's bike and a scooter on a porch and foolishly jumped on it. As they were speeding off, a woman came out and said, ""Hey! That's my kid's bike!"" They dropped it, they walked away, but they were arrested. She was wrong, she was foolish, but she was also just 18. She had a couple of juvenile misdemeanors. Meanwhile, that man had been arrested for shoplifting in Home Depot -- 85 dollars' worth of stuff, a similar petty crime. But he had two prior armed robbery convictions. But the algorithm scored her as high risk, and not him. Two years later, ProPublica found that she had not reoffended. It was just hard to get a job for her with her record. He, on the other hand, did reoffend and is now serving an eight-year prison term for a later crime. Clearly, we need to audit our black boxes and not have them have this kind of unchecked power. (Applause) Audits are great and important, but they don't solve all our problems. Take Facebook's powerful news feed algorithm -- you know, the one that ranks everything and decides what to show you from all the friends and pages you follow. Should you be shown another baby picture? (Laughter) A sullen note from an acquaintance? An important but difficult news item? There's no right answer. Facebook optimizes for engagement on the site: likes, shares, comments. In August of 2014, protests broke out in Ferguson, Missouri, after the killing of an African-American teenager by a white police officer, under murky circumstances. The news of the protests was all over my algorithmically unfiltered Twitter feed, but nowhere on my Facebook. Was it my Facebook friends? I disabled Facebook's algorithm, which is hard because Facebook keeps wanting to make you come under the algorithm's control, and saw that my friends were talking about it. It's just that the algorithm wasn't showing it to me. I researched this and found this was a widespread problem. The story of Ferguson wasn't algorithm-friendly. It's not ""likable."" Who's going to click on ""like?"" It's not even easy to comment on. Without likes and comments, the algorithm was likely showing it to even fewer people, so we didn't get to see this. Instead, that week, Facebook's algorithm highlighted this, which is the ALS Ice Bucket Challenge. Worthy cause; dump ice water, donate to charity, fine. But it was super algorithm-friendly. The machine made this decision for us. A very important but difficult conversation might have been smothered, had Facebook been the only channel. Now, finally, these systems can also be wrong in ways that don't resemble human systems. Do you guys remember Watson, IBM's machine-intelligence system that wiped the floor with human contestants on Jeopardy? It was a great player. But then, for Final Jeopardy, Watson was asked this question: ""Its largest airport is named for a World War II hero, its second-largest for a World War II battle."" (Hums Final Jeopardy music) Chicago. The two humans got it right. Watson, on the other hand, answered ""Toronto"" -- for a US city category! The impressive system also made an error that a human would never make, a second-grader wouldn't make. Our machine intelligence can fail in ways that don't fit error patterns of humans, in ways we won't expect and be prepared for. It'd be lousy not to get a job one is qualified for, but it would triple suck if it was because of stack overflow in some subroutine. (Laughter) In May of 2010, a flash crash on Wall Street fueled by a feedback loop in Wall Street's ""sell"" algorithm wiped a trillion dollars of value in 36 minutes. I don't even want to think what ""error"" means in the context of lethal autonomous weapons. So yes, humans have always made biases. Decision makers and gatekeepers, in courts, in news, in war ... they make mistakes; but that's exactly my point. We cannot escape these difficult questions. We cannot outsource our responsibilities to machines. (Applause) Artificial intelligence does not give us a ""Get out of ethics free"" card. Data scientist Fred Benenson calls this math-washing. We need the opposite. We need to cultivate algorithm suspicion, scrutiny and investigation. We need to make sure we have algorithmic accountability, auditing and meaningful transparency. We need to accept that bringing math and computation to messy, value-laden human affairs does not bring objectivity; rather, the complexity of human affairs invades the algorithms. Yes, we can and we should use computation to help us make better decisions. But we have to own up to our moral responsibility to judgment, and use algorithms within that framework, not as a means to abdicate and outsource our responsibilities to one another as human to human. Machine intelligence is here. That means we must hold on ever tighter to human values and human ethics. Thank you. (Applause)",PT17M32S,2016-10-19T15:18:26Z
The ethical dilemma of self-driving cars,"technology,design,invention,transportation,education,innovation,AI,TED-Ed,machine learning,animation,driverless cars,ethics",Patrick Lin,"Self-driving cars are already cruising the streets today. And while these cars will ultimately be safer and cleaner than their manual counterparts, they can't completely avoid accidents altogether. How should the car be programmed if it encounters an unavoidable accident? Patrick Lin navigates the murky ethics of self-driving cars. ","This is a thought experiment. Let's say at some point in the not so distant future, you're barreling down the highway in your self-driving car, and you find yourself boxed in on all sides by other cars. Suddenly, a large, heavy object falls off the truck in front of you. Your car can't stop in time to avoid the collision, so it needs to make a decision: go straight and hit the object, swerve left into an SUV, or swerve right into a motorcycle. Should it prioritize your safety by hitting the motorcycle, minimize danger to others by not swerving, even if it means hitting the large object and sacrificing your life, or take the middle ground by hitting the SUV, which has a high passenger safety rating? So what should the self-driving car do? If we were driving that boxed in car in manual mode, whichever way we'd react would be understood as just that, a reaction, not a deliberate decision. It would be an instinctual panicked move with no forethought or malice. But if a programmer were to instruct the car to make the same move, given conditions it may  sense in the future, well, that looks more like premeditated homicide. Now, to be fair, self-driving cars are predicted  to dramatically reduce traffic accidents and fatalities by removing human error  from the driving equation. Plus, there may be all sorts  of other benefits: eased road congestion, decreased harmful emissions, and minimized unproductive and stressful driving time. But accidents can and will still happen, and when they do, their outcomes may be determined months or years in advance by programmers or policy makers. And they'll have  some difficult decisions to make. It's tempting to offer up general  decision-making principles, like minimize harm, but even that quickly leads  to morally murky decisions. For example, let's say we have the same initial set up, but now there's a motorcyclist  wearing a helmet to your left and another one without  a helmet to your right. Which one should  your robot car crash into? If you say the biker with the helmet because she's more likely to survive, then aren't you penalizing  the responsible motorist? If, instead, you save the biker  without the helmet because he's acting irresponsibly, then you've gone way beyond the initial design principle about minimizing harm, and the robot car is now  meting out street justice. The ethical considerations  get more complicated here. In both of our scenarios, the underlying design is functioning as a targeting algorithm of sorts. In other words, it's systematically favoring  or discriminating against a certain type  of object to crash into. And the owners of the target vehicles will suffer the negative consequences of this algorithm through no fault of their own. Our new technologies are opening up many other novel ethical dilemmas. For instance, if you had to  choose between a car that would always save as many lives as possible in an accident, or one that would save you at any cost, which would you buy? What happens if the cars start analyzing and factoring in the passengers of the cars and the particulars of their lives? Could it be the case  that a random decision is still better than a predetermined one designed to minimize harm? And who should be making  all of these decisions anyhow? Programmers? Companies? Governments? Reality may not play out exactly like our thought experiments, but that's not the point. They're designed to isolate  and stress test our intuitions on ethics, just like science experiments do for the physical world. Spotting these moral hairpin turns now will help us maneuver the unfamiliar road of technology ethics, and allow us to cruise confidently and conscientiously into our brave new future.",PT04M00S,2017-09-07T19:07:44Z
What if buildings created energy instead of consuming it?,"climate change,technology,design,innovation,future,pollution,Countdown",Ksenia Petrichenko,"Buildings are bad news for the climate -- but they don't have to be. While our structures are currently responsible for a third of global energy consumption and emissions, a future where they create more energy than they consume is possible. Energy policy analyst Ksenia Petrichenko has a three-tiered strategy for thinking differently about buildings, transforming them from passive users to active players in the energy system and bringing us closer to our climate targets.","Buildings are not only what they seem. They aren't just inert structures to live and work in. They're connected to the energy system. And this makes buildings an essential element of the efforts to decarbonize our world. Today, I want to talk about changing our thinking around the role of buildings in the energy system and how they can transform to bring us closer to our climate targets. But first, I want to take you to where I grew up, in this building in a mid-sized city in Russia. To me, it looked [like] just a great concrete box, like many others. There are indeed numerous buildings like this across Europe and Central Asia, inherited from Soviet times. They were built quickly to house as many people as possible, without much consideration for their design, comfort and energy usage. This is true of many, many buildings across the world. Nondescript housing and office boxes, built fast and cheap, and all connected to the energy system. I live in Paris now, and though buildings there look much nicer, from the perspective of energy, issues are similar. High energy consumption, reliance on fossil fuels and high energy costs, driven even higher by the current energy crisis. In Paris, I work with International Energy Agency, and we advise governments across the world on various energy issues. And for that, we look at data a lot. And our data shows that at the global scale, buildings are responsible for about one third of total energy consumption and energy- and process-related carbon dioxide emissions. In Europe, for example, 90 percent of buildings that exist today will still be standing and in use by 2050. We are also building more new buildings. Our data shows that the global floor area is expected to increase by 75 percent by mid-century. This is equivalent to adding an area the size of Paris every week for the next 30 years. If this happens with energy consumption patterns of today, this alone will make it very difficult to achieve our climate targets. Fortunately, we do have solutions available to reduce direct emissions from buildings by more than 95 percent by 2050. And for this, we need three things: efficiency, electrification and decarbonization. Energy efficiency must come first. And in buildings, it should be improved in two ways. First, through improving energy efficiency of the building envelope with better materials, design solutions, insulation of walls, roofs, basements, energy-efficient windows. Second, through improving energy efficiency of all appliances and equipment used in building -- for space heating and cooling, water heating, lighting, cooking, working and entertainment. However, energy efficiency, as they say, takes a village. Many forces and interests interact during construction and renovation of a building. The developers, the owners, the regulators, the architects, the suppliers of technologies, the market itself exerting price pressure. All these need to be aligned. And it all starts with the governments mandating minimum energy efficiency requirements through their regulation, putting the right incentives in place and providing clear information tools. Building energy codes, standards and labeling for appliances and equipment, as well as buildings, are among the most effective instruments at our disposal -- if they're enforced well and together with other supporting mechanisms. Energy efficiency in buildings also depends on us as consumers, the choices we make, buildings we decide to live in, appliances and devices we buy and the way we use them at home or our workplaces. So what is the result of improving energy efficiency? Significant energy savings, reduced CO2 emissions, lower energy bills and improved comfort, productivity and health of people living in them. The second thing we need is a massive shift towards electricity. And of course I mean clean, decarbonized electricity produced from solar, wind and other low-carbon sources. If we look at Europe, for instance, more than 40 percent of households rely on natural gas for space heating and cooking. To complicate matters, 40 percent of all gas consumed in Europe last year was imported from Russia. So there is a climate imperative, but also a strong geopolitical and energy security imperative to electrify and decarbonize our buildings and make them as efficient as possible. This doesn't only mean replacing energy supply from a gas pipeline with energy supply from an electric line. And here I reach back to my opening point. We need to look at buildings differently. Not only as great concrete boxes, as I used to do, but as potential active players in the energy system. So how can we make buildings active? Through installing efficient and low-carbon technologies such as solar panels, heat pumps, energy storage, making use of smart and digital tools. All technologies we already have, and in many cases their costs are decreasing. Let me introduce a word we don’t often hear: “prosumer.” Think of a building not only as a consumer of energy, but as prosumer, both consuming and producing. Most buildings can produce at least part of the energy they need. In many cases, they can produce more. Such buildings already exist in many parts of the world. In Switzerland, for example, there is even a single family house that produces 800 percent of the energy it needs. But still, we have a long way to go to make them a common practice. Besides challenges with improving energy efficiency and installing low-carbon technologies, another obstacle is that our electricity networks, or grids, as we call them, were designed to operate with large centralized sources of generation, like power plants. And their design hasn't changed much over the past century. So increasing decentralized or distributed, smaller-scale generation, especially from variable renewable sources like solar and wind, can really put pressure on existing electricity grids and challenge their stability. So we also need to rethink the way our entire electricity system is designed. Smart grids can communicate to a building through software algorithms and smart controls, and, for example, send a signal to a connected device, such as your heating system, to temporarily stop or reduce energy use, or, if possible, shift it to the time when electricity is more available, cleaner or cheaper. This can really improve the robustness of the electricity network and help to avoid, for example, rolling blackouts, which are becoming more common, while consumers can get paid for it. If you have an electric car and you keep it plugged in for a prolonged period of time, a smart charger can help to find the best time to charge it, depending on the information from the grid. This can help save you money and support the electricity system. It gets even more interesting. Electric cars are essentially batteries on wheels, so they can also be used as energy storage and dispatch electricity to the home or to the grid at the time of need. A building itself can be used to store energy for some time, but you need to make it energy-efficient first. Potentially you can also get paid for making your energy storage available for electricity system to store renewable electricity when there is too much of it, for example, when there is a lot of sunshine or wind, but demand is not high enough to use it all. It's almost like renting your spare bedroom on Airbnb, just for hosting electricity. Now imagine a future with many, many buildings, solar panels, electric cars, energy storage, other distributed devices, drawing electricity from the grid and supplying electricity back to it. Obviously we would need a way to manage all this data efficiently and in real time, make sure that devices interact well with each other and are able to communicate to the grid and optimize electricity demand and supply for the entire system. Virtual power plants, digital and intelligent platforms can do just that. They can aggregate electricity consumption and production from many distributed resources, make use of local storage and manage complex interactions. Virtual power plants can also facilitate peer-to-peer renewable electricity trading. Imagine that electricity that your building produced, for example, from solar panels, but you don't need at the moment, you can sell directly to another building that needs it via an online platform. A project like this has been running already for several years in one of the districts in Bangkok and is showing benefits to the consumers, the grid and the planet. There are other pilot projects like this in different countries, but for them to scale, our policy frameworks and electricity market design need to undergo substantial changes. Our buildings need to become more efficient and grids need to evolve into smarter managers of distributed energy resources. It's a dramatic shift. It's like going from all-television world with a single broadcaster to a new, connected world where everyone can generate and share their own content. Remember that building I grew up in? You might be surprised, but during winter, it would often get too hot inside the apartment, even if it was -30 outside. It was because of highly inefficient central heating system, which we couldn't control or do anything about. So we did this. Yes. We had to open the windows to get some fresh air and more comfort. Now I want to take you to my current flat in Paris, where I have tiny but very smart boxes attached to my electric heaters, which can automatically turn them on and off, maintaining comfortable temperatures, lowering my energy bills and providing flexibility to the grid. And I can control all that on my smartphone. The technologies -- that we have. Now we need the policies, the investments, the will -- and a new way to look at buildings not just passive users of energy, but as active players in the energy system. Buildings that can consume and produce energy efficiently, interact with a smart grid and respond to its signals, providing flexibility and bringing us closer to our climate targets. Thank you. (Applause)",PT13M32S,2023-02-14T15:51:33Z
A robot that eats pollution,"climate change,environment,science,technology,design,engineering,invention,innovation,robots,animals,nature,future,pollution,insects,water,microbiology,chemistry,TEDx,ocean,fossil fuels",Jonathan Rossiter,"Meet the ""Row-bot,"" a robot that cleans up pollution and generates the electricity needed to power itself by swallowing dirty water. Roboticist Jonathan Rossiter explains how this special swimming machine, which uses a microbial fuel cell to neutralize algal blooms and oil slicks, could be a precursor to biodegradable, autonomous pollution-fighting robots.","Hi, I'm an engineer and I make robots. Now, of course you all know what a robot is, right? If you don't, you'd probably go to Google, and you'd ask Google what a robot is. So let's do that. We'll go to Google and this is what we get. Now, you can see here there are lots of different types of robots, but they're predominantly humanoid in structure. And they look pretty conventional because they've got plastic, they've got metal, they've got motors and gears and so on. Some of them look quite friendly, and you could go up and you could hug them. Some of them not so friendly, they look like they're straight out of ""Terminator,"" in fact they may well be straight out of ""Terminator."" You can do lots of really cool things with these robots -- you can do really exciting stuff. But I'd like to look at different kinds of robots -- I want to make different kinds of robots. And I take inspiration from the things that don't look like us, but look like these. So these are natural biological organisms and they do some really cool things that we can't, and current robots can't either. They do all sorts of great things like moving around on the floor; they go into our gardens and they eat our crops; they climb trees; they go in water, they come out of water; they trap insects and digest them. So they do really interesting things. They live, they breathe, they die, they eat things from the environment. Our current robots don't really do that. Now, wouldn't it be great if you could use some of those characteristics in future robots so that you could solve some really interesting problems? I'm going to look at a couple of problems now in the environment where we can use the skills and the technologies derived from these animals and from the plants, and we can use them to solve those problems. Let's have a look at two environmental problems. They're both of our making -- this is man interacting with the environment and doing some rather unpleasant things. The first one is to do with the pressure of population. Such is the pressure of population around the world that agriculture and farming is required to produce more and more crops. Now, to do that, farmers put more and more chemicals onto the land. They put on fertilizers, nitrates, pesticides -- all sorts of things that encourage the growth of the crops, but there are some negative impacts. One of the negative impacts is if you put lots of fertilizer on the land, not all of it goes into the crops. Lots of it stays in the soil, and then when it rains, these chemicals go into the water table. And in the water table, then they go into streams, into lakes, into rivers and into the sea. Now, if you put all of these chemicals, these nitrates, into those kinds of environments, there are organisms in those environments that will be affected by that -- algae, for example. Algae loves nitrates, it loves fertilizer, so it will take in all these chemicals, and if the conditions are right, it will mass produce. It will produce masses and masses of new algae. That's called a bloom. The trouble is that when algae reproduces like this, it starves the water of oxygen. As soon as you do that, the other organisms in the water can't survive. So, what do we do? We try to produce a robot that will eat the algae, consume it and make it safe. So that's the first problem. The second problem is also of our making, and it's to do with oil pollution. Now, oil comes out of the engines that we use, the boats that we use. Sometimes tankers flush their oil tanks into the sea, so oil is released into the sea that way. Wouldn't it be nice if we could treat that in some way using robots that could eat the pollution the oil fields have produced? So that's what we do. We make robots that will eat pollution. To actually make the robot, we take inspiration from two organisms. On the right there you see the basking shark. The basking shark is a massive shark. It's noncarnivorous, so you can swim with it, as you can see. And the basking shark opens its mouth, and it swims through the water, collecting plankton. As it does that, it digests the food, and then it uses that energy in its body to keep moving. So, could we make a robot like that -- like the basking shark that chugs through the water and eats up pollution? Well, let's see if we can do that. But also, we take the inspiration from other organisms. I've got a picture here of a water boatman, and the water boatman is really cute. When it's swimming in the water, it uses its paddle-like legs to push itself forward. So we take those two organisms and we combine them together to make a new kind of robot. In fact, because we're using the water boatman as inspiration, and our robot sits on top of the water, and it rows, we call it the ""Row-bot."" So a Row-bot is a robot that rows. OK. So what does it look like? Here's some pictures of the Row-bot, and you'll see, it doesn't look anything like the robots we saw right at the beginning. Google is wrong; robots don't look like that, they look like this. So I've got the Row-bot here. I'll just hold it up for you. It gives you a sense of the scale, and it doesn't look anything like the others. OK, so it's made out of plastic, and we'll have a look now at the components that make up the Row-bot -- what makes it really special. The Row-bot is made up of three parts, and those three parts are really like the parts of any organism. It's got a brain, it's got a body and it's got a stomach. It needs the stomach to create the energy. Any Row-bot will have those three components, and any organism will have those three components, so let's go through them one at a time. It has a body, and its body is made out of plastic, and it sits on top of the water. And it's got flippers on the side here -- paddles that help it move, just like the water boatman. It's got a plastic body, but it's got a soft rubber mouth here, and a mouth here -- it's got two mouths. Why does it have two mouths? One is to let the food go in and the other is to let the food go out. So you can see really it's got a mouth and a derriere, or a -- (Laughter) something where the stuff comes out, which is just like a real organism. So it's starting to look like that basking shark. So that's the body. The second component might be the stomach. We need to get the energy into the robot and we need to treat the pollution, so the pollution goes in, and it will do something. It's got a cell in the middle here called a microbial fuel cell. I'll put this down, and I'll lift up the fuel cell. Here. So instead of having batteries, instead of having a conventional power system, it's got one of these. This is its stomach. And it really is a stomach because you can put energy in this side in the form of pollution, and it creates electricity. So what is it? It's called a microbial fuel cell. It's a little bit like a chemical fuel cell, which you might have come across in school, or you might've seen in the news. Chemical fuel cells take hydrogen and oxygen, and they can combine them together and you get electricity. That's well-established technology; it was in the Apollo space missions. That's from 40, 50 years ago. This is slightly newer. This is a microbial fuel cell. It's the same principle: it's got oxygen on one side, but instead of having hydrogen on the other, it's got some soup, and inside that soup there are living microbes. Now, if you take some organic material -- could be some waste products, some food, maybe a bit of your sandwich -- you put it in there, the microbes will eat that food, and they will turn it into electricity. Not only that, but if you select the right kind of microbes, you can use the microbial fuel cell to treat some of the pollution. If you choose the right microbes, the microbes will eat the algae. If you use other kinds of microbes, they will eat petroleum spirits and crude oil. So you can see how this stomach could be used to not only treat the pollution but also to generate electricity from the pollution. So the robot will move through the environment, taking food into its stomach, digest the food, create electricity, use that electricity to move through the environment and keep doing this. OK, so let's see what happens when we run the Row-bot -- when it does some rowing. Here we've got a couple of videos, the first thing you'll see -- hopefully you can see here is the mouth open. The front mouth and the bottom mouth open, and it will stay opened enough, then the robot will start to row forward. It moves through the water so that food goes in as the waste products go out. Once it's moved enough, it stops and then it closes the mouth -- slowly closes the mouths -- and then it will sit there, and it will digest the food. Of course these microbial fuel cells, they contain microbes. What you really want is lots of energy coming out of those microbes as quickly as possible. But we can't force the microbes and they generate a small amount of electricity per second. They generate milliwatts, or microwatts. Let's put that into context. Your mobile phone for example, one of these modern ones, if you use it, it takes about one watt. So that's a thousand or a million times as much energy that that uses compared to the microbial fuel cell. How can we cope with that? Well, when the Row-bot has done its digestion, when it's taken the food in, it will sit there and it will wait until it has consumed all that food. That could take some hours, it could take some days. A typical cycle for the Row-bot looks like this: you open your mouth, you move, you close your mouth and you sit there for a while waiting. Once you digest your food, then you can go about doing the same thing again. But you know what, that looks like a real organism, doesn't it? It looks like the kind of thing we do. Saturday night, we go out, open our mouths, fill our stomachs, sit in front of the telly and digest. When we've had enough, we do the same thing again. OK, if we're lucky with this cycle, at the end of the cycle we'll have enough energy left over for us to be able to do something else. We could send a message, for example. We could send a message saying, ""This is how much pollution I've eaten recently,"" or, ""This is the kind of stuff that I've encountered,"" or, ""This is where I am."" That ability to send a message saying, ""This is where I am,"" is really, really important. If you think about the oil slicks that we saw before, or those massive algal blooms, what you really want to do is put your Row-bot out there, and it eats up all of those pollutions, and then you have to go collect them. Why? Because these Row-bots at the moment, this Row-bot I've got here, it contains motors, it contains wires, it contains components which themselves are not biodegradable. Current Row-bots contain things like toxic batteries. You can't leave those in the environment, so you need to track them, and then when they've finished their job of work, you need to collect them. That limits the number of Row-bots you can use. If, on the other hand, you have robot a little bit like a biological organism, when it comes to the end of its life, it dies and it degrades to nothing. So wouldn't it be nice if these robots, instead of being like this, made out of plastic, were made out of other materials, which when you throw them out there, they biodegrade to nothing? That changes the way in which we use robots. Instead of putting 10 or 100 out into the environment, having to track them, and then when they die, collect them, you could put a thousand, a million, a billion robots into the environment. Just spread them around. You know that at the end of their lives, they're going to degrade to nothing. You don't need to worry about them. So that changes the way in which you think about robots and the way you deploy them. Then the question is: Can you do this? Well, yes, we have shown that you can do this. You can make robots which are biodegradable. What's really interesting is you can use household materials to make these biodegradable robots. I'll show you some; you might be surprised. You can make a robot out of jelly. Instead of having a motor, which we have at the moment, you can make things called artificial muscles. Artificial muscles are smart materials, you apply electricity to them, and they contract, or they bend or they twist. They look like real muscles. So instead of having a motor, you have these artificial muscles. And you can make artificial muscles out of jelly. If you take some jelly and some salts, and do a bit of jiggery-pokery, you can make an artificial muscle. We've also shown you can make the microbial fuel cell's stomach out of paper. So you could make the whole robot out of biodegradable materials. You throw them out there, and they degrade to nothing. Well, this is really, really exciting. It's going to totally change the way in which we think about robots, but also it allows you to be really creative in the way in which you think about what you can do with these robots. I'll give you an example. If you can use jelly to make a robot -- now, we eat jelly, right? So, why not make something like this? A robot gummy bear. Here, I've got some I prepared earlier. There we go. I've got a packet -- and I've got a lemon-flavored one. I'll take this gummy bear -- he's not robotic, OK? We have to pretend. And what you do with one of these is you put it in your mouth -- the lemon's quite nice. Try not to chew it too much, it's a robot, it may not like it. And then you swallow it. And then it goes into your stomach. And when it's inside your stomach, it moves, it thinks, it twists, it bends, it does something. It could go further down into your intestines, find out whether you've got some ulcer or cancer, maybe do an injection, something like that. You know that once it's done its job of work, it could be consumed by your stomach, or if you don't want that, it could go straight through you, into the toilet, and be degraded safely in the environment. So this changes the way, again, in which we think about robots. So, we started off looking at robots that would eat pollution, and then we're looking at robots which we can eat. I hope this gives you some idea of the kinds of things we can do with future robots. Thank you very much for your attention. (Applause)",PT14M00S,2017-02-22T16:17:33Z
